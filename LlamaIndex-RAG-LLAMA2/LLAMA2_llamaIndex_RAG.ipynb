{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3c3b5c53fd684c61ba39f556fc76e2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e5114fd505a446093b29cc2da487dbb",
              "IPY_MODEL_223779e15daf43468f2565f6a437bc86",
              "IPY_MODEL_8c5cd83fa1dd4b37a9493f5a79291000"
            ],
            "layout": "IPY_MODEL_cea6cff19a934e538e7f01e3d112f48b"
          }
        },
        "2e5114fd505a446093b29cc2da487dbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7b51152cffd4fc38cb7466f86629637",
            "placeholder": "​",
            "style": "IPY_MODEL_0d0683d01aa54dc29aadb8be590b4e3c",
            "value": "config.json: 100%"
          }
        },
        "223779e15daf43468f2565f6a437bc86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a69ab68c759d4bccaf90b332bcfa6326",
            "max": 614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fea4691323b94a99b42825ce407752ee",
            "value": 614
          }
        },
        "8c5cd83fa1dd4b37a9493f5a79291000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2907e1e62f04fd7b1484b0ddaecabf0",
            "placeholder": "​",
            "style": "IPY_MODEL_498dae17dfe54703be3207e124270e92",
            "value": " 614/614 [00:00&lt;00:00, 32.4kB/s]"
          }
        },
        "cea6cff19a934e538e7f01e3d112f48b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7b51152cffd4fc38cb7466f86629637": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d0683d01aa54dc29aadb8be590b4e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a69ab68c759d4bccaf90b332bcfa6326": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fea4691323b94a99b42825ce407752ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2907e1e62f04fd7b1484b0ddaecabf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "498dae17dfe54703be3207e124270e92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20af554668a3480a8bc06cd48f6b2d88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9536ebc1d9de42d69a7ac80357d29bf0",
              "IPY_MODEL_f86e5e0fc2854796aea2df2c91cb4b72",
              "IPY_MODEL_14a2dbdecbf8477085c949ab39048b8e"
            ],
            "layout": "IPY_MODEL_4c530a8971a74173be6c6f4f0fa43ded"
          }
        },
        "9536ebc1d9de42d69a7ac80357d29bf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de99ecd582de4d9a9694598d240da5fe",
            "placeholder": "​",
            "style": "IPY_MODEL_3f5c53c088ce4590ac2fe054610c878a",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "f86e5e0fc2854796aea2df2c91cb4b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fafb889a238b42888616f774137dea51",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40a9a877ccb74136b9e0edf3dc1fb2df",
            "value": 26788
          }
        },
        "14a2dbdecbf8477085c949ab39048b8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d591b7e891b741c2abe4b22fde6f2cdd",
            "placeholder": "​",
            "style": "IPY_MODEL_f99d6d14f3aa48329fdbebf73e35a461",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 1.45MB/s]"
          }
        },
        "4c530a8971a74173be6c6f4f0fa43ded": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de99ecd582de4d9a9694598d240da5fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f5c53c088ce4590ac2fe054610c878a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fafb889a238b42888616f774137dea51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40a9a877ccb74136b9e0edf3dc1fb2df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d591b7e891b741c2abe4b22fde6f2cdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f99d6d14f3aa48329fdbebf73e35a461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "651bdbc4f878490ebf05c5986c813806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f74a04c006343a1b336fa528c19b620",
              "IPY_MODEL_cf1becc4277f4da88b09947decb6d71d",
              "IPY_MODEL_0fb0807a9ced4074bc45e6f75db5f344"
            ],
            "layout": "IPY_MODEL_eb3ab1dc89634c3da2d06bbc4b7eda0b"
          }
        },
        "0f74a04c006343a1b336fa528c19b620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adfc14d368c24222945d58d076e42887",
            "placeholder": "​",
            "style": "IPY_MODEL_bbe1f006992d472aaa264b653d725d29",
            "value": "Downloading shards: 100%"
          }
        },
        "cf1becc4277f4da88b09947decb6d71d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b271e03fa114f188d2bc61589b85f56",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fee11ae0f3b4e93a787375943980700",
            "value": 2
          }
        },
        "0fb0807a9ced4074bc45e6f75db5f344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24a0996f264a48d9b551c9e4b356eacd",
            "placeholder": "​",
            "style": "IPY_MODEL_0e74702609254ef39fec984532330eb4",
            "value": " 2/2 [01:17&lt;00:00, 35.39s/it]"
          }
        },
        "eb3ab1dc89634c3da2d06bbc4b7eda0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adfc14d368c24222945d58d076e42887": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbe1f006992d472aaa264b653d725d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b271e03fa114f188d2bc61589b85f56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fee11ae0f3b4e93a787375943980700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24a0996f264a48d9b551c9e4b356eacd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e74702609254ef39fec984532330eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77aa9d4bc5df48b7bb10f685cc04400a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05367981e76f4e158fca2bbb8319a653",
              "IPY_MODEL_87676f1ca62e4e3396fb5a8132ab8feb",
              "IPY_MODEL_a8c3147f905246eaa4cae73ed9528022"
            ],
            "layout": "IPY_MODEL_88e916a250f344b8a474daf43d96a0c1"
          }
        },
        "05367981e76f4e158fca2bbb8319a653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46a7517d95b7488d80e3ff53bd672578",
            "placeholder": "​",
            "style": "IPY_MODEL_60712292e87c40d5a5b6be701ab139ff",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "87676f1ca62e4e3396fb5a8132ab8feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34167f00e9e54ea993a3950509c79c2c",
            "max": 9976576152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6a80339be2c46f998b568482e76387a",
            "value": 9976576152
          }
        },
        "a8c3147f905246eaa4cae73ed9528022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aad022034dce43e499aafb51dbda0384",
            "placeholder": "​",
            "style": "IPY_MODEL_ed83d296d49947c886e15723f73aa593",
            "value": " 9.98G/9.98G [00:56&lt;00:00, 119MB/s]"
          }
        },
        "88e916a250f344b8a474daf43d96a0c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46a7517d95b7488d80e3ff53bd672578": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60712292e87c40d5a5b6be701ab139ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34167f00e9e54ea993a3950509c79c2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6a80339be2c46f998b568482e76387a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aad022034dce43e499aafb51dbda0384": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed83d296d49947c886e15723f73aa593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "522020c17cf64ca2bae443b6236452e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56ebe9d06ee64b88a0927cd55770a6cb",
              "IPY_MODEL_1785b300424c4920860357aa387606f3",
              "IPY_MODEL_b880a14989e04a77afa062930514ebcc"
            ],
            "layout": "IPY_MODEL_2d5f9629667a486380422bedb8f2407a"
          }
        },
        "56ebe9d06ee64b88a0927cd55770a6cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e66d4b0e18940519cd2582271e1cf75",
            "placeholder": "​",
            "style": "IPY_MODEL_ceda604f5e064b84887e8c33171f7a70",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "1785b300424c4920860357aa387606f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_090c2e48fb774f3786487fcf258e7cba",
            "max": 3500296424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37a1d632e3fb4e6d99370755c377f593",
            "value": 3500296424
          }
        },
        "b880a14989e04a77afa062930514ebcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5b2a75e229944e08ebc23da96365056",
            "placeholder": "​",
            "style": "IPY_MODEL_5ba61d84d9254fdda0f8601a1c6b69ce",
            "value": " 3.50G/3.50G [00:19&lt;00:00, 225MB/s]"
          }
        },
        "2d5f9629667a486380422bedb8f2407a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e66d4b0e18940519cd2582271e1cf75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceda604f5e064b84887e8c33171f7a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "090c2e48fb774f3786487fcf258e7cba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37a1d632e3fb4e6d99370755c377f593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5b2a75e229944e08ebc23da96365056": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ba61d84d9254fdda0f8601a1c6b69ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9eb36cf18504493fa01a468feedb5ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96c14035bf6041cda1072aaa535664e3",
              "IPY_MODEL_94f4411599db4c11b3aafd26d07e3756",
              "IPY_MODEL_f75507a0d2574fa4a0831e080ffbe65f"
            ],
            "layout": "IPY_MODEL_8f42dd5e603e4b18939720702d2492e9"
          }
        },
        "96c14035bf6041cda1072aaa535664e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b472be8a949749bc9b08661d2524a193",
            "placeholder": "​",
            "style": "IPY_MODEL_9a2013182c5442efb94a5da0b504d376",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "94f4411599db4c11b3aafd26d07e3756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c0bbe4953764083b63b7fe7d92f3586",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e30f39ef4924e17ac730609179145de",
            "value": 2
          }
        },
        "f75507a0d2574fa4a0831e080ffbe65f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41d6aa85cfaf48d4b85e9d4be5a9d07e",
            "placeholder": "​",
            "style": "IPY_MODEL_1946fb2bdeab4ac7a39367b4bf3a275f",
            "value": " 2/2 [00:59&lt;00:00, 27.13s/it]"
          }
        },
        "8f42dd5e603e4b18939720702d2492e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b472be8a949749bc9b08661d2524a193": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a2013182c5442efb94a5da0b504d376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c0bbe4953764083b63b7fe7d92f3586": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e30f39ef4924e17ac730609179145de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41d6aa85cfaf48d4b85e9d4be5a9d07e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1946fb2bdeab4ac7a39367b4bf3a275f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81fdfcc5067347118d3ea9089a1b4ea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_773d7c33d876490abdc247c6163f7b09",
              "IPY_MODEL_962c10e150f44a3684d3810a139e3dd3",
              "IPY_MODEL_94a2d0e0cae04bae90d1a35ab4634109"
            ],
            "layout": "IPY_MODEL_4cedf8db6b104dd987db6c640abef5be"
          }
        },
        "773d7c33d876490abdc247c6163f7b09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2334cedebe94440804c4c220958f8e8",
            "placeholder": "​",
            "style": "IPY_MODEL_0f24cfb7d6824b1db9bf52460f354d89",
            "value": "generation_config.json: 100%"
          }
        },
        "962c10e150f44a3684d3810a139e3dd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0dd3382add44fb8ae0f89f2c3c07c7a",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e47287fb8b948799fb0b5f904059171",
            "value": 188
          }
        },
        "94a2d0e0cae04bae90d1a35ab4634109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cbcac6f7d2e454c8575b9724a1f4b38",
            "placeholder": "​",
            "style": "IPY_MODEL_99ca3b77d04c480ea9ec460e8a8be0dc",
            "value": " 188/188 [00:00&lt;00:00, 12.6kB/s]"
          }
        },
        "4cedf8db6b104dd987db6c640abef5be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2334cedebe94440804c4c220958f8e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f24cfb7d6824b1db9bf52460f354d89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0dd3382add44fb8ae0f89f2c3c07c7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e47287fb8b948799fb0b5f904059171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3cbcac6f7d2e454c8575b9724a1f4b38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99ca3b77d04c480ea9ec460e8a8be0dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "126980d5a9c446bcbc567a8e43fc0463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a120c98d991a42838bb4718932b51042",
              "IPY_MODEL_5c440524344e407da1e1ec9d49ae9075",
              "IPY_MODEL_428df9f4679444f2a5abd474e892ff61"
            ],
            "layout": "IPY_MODEL_a9ef976919db4388b35e00f4e4fde3f6"
          }
        },
        "a120c98d991a42838bb4718932b51042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8292728186f1448bb99952ea196cee06",
            "placeholder": "​",
            "style": "IPY_MODEL_5afd015bc8d847ca94b604a254ce3937",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "5c440524344e407da1e1ec9d49ae9075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a78e3c991da4494b9fc7809c3e7038a",
            "max": 1618,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8dff4d125e44b728343f779ab02de58",
            "value": 1618
          }
        },
        "428df9f4679444f2a5abd474e892ff61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_474b7d0483f94d309f1546fc08b7555a",
            "placeholder": "​",
            "style": "IPY_MODEL_c984c002a1834330ae01f834125ef21f",
            "value": " 1.62k/1.62k [00:00&lt;00:00, 41.5kB/s]"
          }
        },
        "a9ef976919db4388b35e00f4e4fde3f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8292728186f1448bb99952ea196cee06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5afd015bc8d847ca94b604a254ce3937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a78e3c991da4494b9fc7809c3e7038a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8dff4d125e44b728343f779ab02de58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "474b7d0483f94d309f1546fc08b7555a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c984c002a1834330ae01f834125ef21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "878ac444553640728bf2a96260da035b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac470c3113ca4b1fb1b44c71ded862db",
              "IPY_MODEL_d32b4766f9654c8781f4d102f44ed99f",
              "IPY_MODEL_3cdf01bb525449d3b667d3d5525d10f5"
            ],
            "layout": "IPY_MODEL_8b1141fa13a045bd8a4083a2a1b41f40"
          }
        },
        "ac470c3113ca4b1fb1b44c71ded862db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51e1a78890284f0a8e68c4bdd0603adb",
            "placeholder": "​",
            "style": "IPY_MODEL_50590f9910e9437788d4d54b48706d7b",
            "value": "tokenizer.model: 100%"
          }
        },
        "d32b4766f9654c8781f4d102f44ed99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6e8c292cd9249c8b70da63af3c17863",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29ccd06e517546178029bb8dea8d29fa",
            "value": 499723
          }
        },
        "3cdf01bb525449d3b667d3d5525d10f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b7ede89b07e4b0dbfd54c9dd4b4495c",
            "placeholder": "​",
            "style": "IPY_MODEL_44ed3168d08a41a78231dde5b5acbaca",
            "value": " 500k/500k [00:00&lt;00:00, 6.17MB/s]"
          }
        },
        "8b1141fa13a045bd8a4083a2a1b41f40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51e1a78890284f0a8e68c4bdd0603adb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50590f9910e9437788d4d54b48706d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6e8c292cd9249c8b70da63af3c17863": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29ccd06e517546178029bb8dea8d29fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b7ede89b07e4b0dbfd54c9dd4b4495c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44ed3168d08a41a78231dde5b5acbaca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98b2200e9cdd4dbfac47448e55a2bd8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39bf446473014366a299a96decd59191",
              "IPY_MODEL_8b380853b3624d1bb17fb416a2d981e8",
              "IPY_MODEL_d2299f4ee9674df295d130087040ff59"
            ],
            "layout": "IPY_MODEL_9873099406e14fdbb8e904ba2e1e92ec"
          }
        },
        "39bf446473014366a299a96decd59191": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c119d83591bb46e99a0c7f879818945a",
            "placeholder": "​",
            "style": "IPY_MODEL_0df6592e593a49b48fd826353063d7bd",
            "value": "tokenizer.json: 100%"
          }
        },
        "8b380853b3624d1bb17fb416a2d981e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_367ade8833354a6e9affb798fda4b736",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a878f9e24de94f94b12d9226d2802e7e",
            "value": 1842767
          }
        },
        "d2299f4ee9674df295d130087040ff59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0251700590794b1d81973c651da71825",
            "placeholder": "​",
            "style": "IPY_MODEL_e3050ee556f54a0a953659890d1669a8",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 2.24MB/s]"
          }
        },
        "9873099406e14fdbb8e904ba2e1e92ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c119d83591bb46e99a0c7f879818945a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0df6592e593a49b48fd826353063d7bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "367ade8833354a6e9affb798fda4b736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a878f9e24de94f94b12d9226d2802e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0251700590794b1d81973c651da71825": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3050ee556f54a0a953659890d1669a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab35fef595364edfa1c5c76176a6fbaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_933f8e848c1d44848ef6288edbf23d38",
              "IPY_MODEL_cf7c9d9789d244b992d8b1bff525b856",
              "IPY_MODEL_03949fd9ffdd4031aa4ad1ab297d6585"
            ],
            "layout": "IPY_MODEL_a0bffe0ba0454d669fcbb5bfa061a017"
          }
        },
        "933f8e848c1d44848ef6288edbf23d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_498dd232222b4b7797232ab2e51b7561",
            "placeholder": "​",
            "style": "IPY_MODEL_2528ddca263f42bbad4759b458723ca4",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "cf7c9d9789d244b992d8b1bff525b856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82df6d5ea1be41e2b86814b060bc7f69",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93663b6ef22a4e4583d6b71ae762fc56",
            "value": 414
          }
        },
        "03949fd9ffdd4031aa4ad1ab297d6585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42b08480705541809f069e6ae72ecb30",
            "placeholder": "​",
            "style": "IPY_MODEL_5f186060b8b040dca354007f4cabb871",
            "value": " 414/414 [00:00&lt;00:00, 30.9kB/s]"
          }
        },
        "a0bffe0ba0454d669fcbb5bfa061a017": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "498dd232222b4b7797232ab2e51b7561": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2528ddca263f42bbad4759b458723ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82df6d5ea1be41e2b86814b060bc7f69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93663b6ef22a4e4583d6b71ae762fc56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42b08480705541809f069e6ae72ecb30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f186060b8b040dca354007f4cabb871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbc1e60edd994aad8a6019f294024e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d65d9fcc5ebf4754883ed25abc78dab7",
              "IPY_MODEL_4f03dd2ec904443ab851476721d1e26a",
              "IPY_MODEL_0a016ee9ea2b4959991a58e7583af32c"
            ],
            "layout": "IPY_MODEL_85b4e96b05ed4d7c90926dc5257a828c"
          }
        },
        "d65d9fcc5ebf4754883ed25abc78dab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4abd701554d548e7b2087b221254afd8",
            "placeholder": "​",
            "style": "IPY_MODEL_5c9b1af13a8b47169a96a1851938930e",
            "value": "modules.json: 100%"
          }
        },
        "4f03dd2ec904443ab851476721d1e26a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffbc2650c8ba4f439d3f35509b923c2e",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a9a473d4ad64d959b8559723b85a711",
            "value": 349
          }
        },
        "0a016ee9ea2b4959991a58e7583af32c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee21df104a18470bad7f339557b3e9d7",
            "placeholder": "​",
            "style": "IPY_MODEL_57170e6c3dd34d27b270078dd089cef4",
            "value": " 349/349 [00:00&lt;00:00, 19.1kB/s]"
          }
        },
        "85b4e96b05ed4d7c90926dc5257a828c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4abd701554d548e7b2087b221254afd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c9b1af13a8b47169a96a1851938930e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffbc2650c8ba4f439d3f35509b923c2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a9a473d4ad64d959b8559723b85a711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee21df104a18470bad7f339557b3e9d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57170e6c3dd34d27b270078dd089cef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eed4ed16576d487cb419a5718a05255d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6212d43637840cc83ce80eee6848b5d",
              "IPY_MODEL_44afc373b8434829b85a575573044bb4",
              "IPY_MODEL_d87cc676dac14f0fbf44add109130018"
            ],
            "layout": "IPY_MODEL_74cdd77837f94a9db0e0b612f58c1833"
          }
        },
        "e6212d43637840cc83ce80eee6848b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a63bd15c5684fc5b465b9d373089302",
            "placeholder": "​",
            "style": "IPY_MODEL_6823dd03dcec4f209d2589bcc1650e1d",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "44afc373b8434829b85a575573044bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae9f45af81ef4d249053b9c6021f8e19",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f47cbde305c0482b934ac865f1633767",
            "value": 116
          }
        },
        "d87cc676dac14f0fbf44add109130018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58ef2327baa24107a0d31ee6cdb8574b",
            "placeholder": "​",
            "style": "IPY_MODEL_1ddee2905a3845349742a9bbe2bc04b7",
            "value": " 116/116 [00:00&lt;00:00, 8.84kB/s]"
          }
        },
        "74cdd77837f94a9db0e0b612f58c1833": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a63bd15c5684fc5b465b9d373089302": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6823dd03dcec4f209d2589bcc1650e1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae9f45af81ef4d249053b9c6021f8e19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f47cbde305c0482b934ac865f1633767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58ef2327baa24107a0d31ee6cdb8574b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ddee2905a3845349742a9bbe2bc04b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33f24c56b0c243739d9e196b7395a954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcdd9c45956e425eb6d9e79ae269e0e4",
              "IPY_MODEL_022411744c4a4fba9f93d7ab31d65ffb",
              "IPY_MODEL_89c1f882654f447290bd3222d8c25505"
            ],
            "layout": "IPY_MODEL_29a0170dd53a465eab477e70a6f1afd3"
          }
        },
        "bcdd9c45956e425eb6d9e79ae269e0e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6f55b7a70bd46b886c0747d6a1ba89e",
            "placeholder": "​",
            "style": "IPY_MODEL_8c78209650864062b0ca39a4926784da",
            "value": "README.md: 100%"
          }
        },
        "022411744c4a4fba9f93d7ab31d65ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e7f01c0d21b4460a6582262b83bfe42",
            "max": 10571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fddfc9e3949246548dfa488578145227",
            "value": 10571
          }
        },
        "89c1f882654f447290bd3222d8c25505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3df895f8dadb40cd96b34363d80397c1",
            "placeholder": "​",
            "style": "IPY_MODEL_8a6d3693dc3c45a28f7e75aff47fde7a",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 783kB/s]"
          }
        },
        "29a0170dd53a465eab477e70a6f1afd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6f55b7a70bd46b886c0747d6a1ba89e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c78209650864062b0ca39a4926784da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e7f01c0d21b4460a6582262b83bfe42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fddfc9e3949246548dfa488578145227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3df895f8dadb40cd96b34363d80397c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a6d3693dc3c45a28f7e75aff47fde7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7116ca05e2c74d5f8d8e1fa51d709297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d69e4b87b8f40afa06d62ee54ab4f84",
              "IPY_MODEL_8b966f6f546a4ff2b0a92413933f9e5f",
              "IPY_MODEL_ee6ce8fbf7234291a40925a621223d65"
            ],
            "layout": "IPY_MODEL_c5215600719641f08a1cd484d7c83747"
          }
        },
        "2d69e4b87b8f40afa06d62ee54ab4f84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3afb911f67147dfbaa30aa4b6c8fef3",
            "placeholder": "​",
            "style": "IPY_MODEL_7bdbd1fde16e4b4aaa2004e3f7dfc298",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "8b966f6f546a4ff2b0a92413933f9e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_243c8f59bf5b4e00b05b961bcdabf85a",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b74322955e5d41a6b00012e52762c35d",
            "value": 53
          }
        },
        "ee6ce8fbf7234291a40925a621223d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3300d909f96a43b7a3c1034fdd8ca629",
            "placeholder": "​",
            "style": "IPY_MODEL_d53e15c8e6d74051aca61366c70ba0c6",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.19kB/s]"
          }
        },
        "c5215600719641f08a1cd484d7c83747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3afb911f67147dfbaa30aa4b6c8fef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bdbd1fde16e4b4aaa2004e3f7dfc298": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "243c8f59bf5b4e00b05b961bcdabf85a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b74322955e5d41a6b00012e52762c35d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3300d909f96a43b7a3c1034fdd8ca629": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d53e15c8e6d74051aca61366c70ba0c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "367d5a81981a4724ae9b4f0edb6b448b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_684e17dfce6a4aeeb797ad1fc252d678",
              "IPY_MODEL_0f81d9cf83164595ab0bb31b5d49f299",
              "IPY_MODEL_134a7f3475e644a5b9716d8394b54dc4"
            ],
            "layout": "IPY_MODEL_810c01ada3264379a83fadfcc9725e8b"
          }
        },
        "684e17dfce6a4aeeb797ad1fc252d678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb247cb6ffaa4428afde0749e0b73f18",
            "placeholder": "​",
            "style": "IPY_MODEL_8dcc66664fdb43d8b4355811dcda1feb",
            "value": "config.json: 100%"
          }
        },
        "0f81d9cf83164595ab0bb31b5d49f299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06382ae2a006456faec98da5d063a0db",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_209a1c5836d3460790eb0d69ae2cb85f",
            "value": 571
          }
        },
        "134a7f3475e644a5b9716d8394b54dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9736c323f9742988f620055935cf2ef",
            "placeholder": "​",
            "style": "IPY_MODEL_7a40808f89e44e05a12ee1587561ad3c",
            "value": " 571/571 [00:00&lt;00:00, 22.2kB/s]"
          }
        },
        "810c01ada3264379a83fadfcc9725e8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb247cb6ffaa4428afde0749e0b73f18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dcc66664fdb43d8b4355811dcda1feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06382ae2a006456faec98da5d063a0db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "209a1c5836d3460790eb0d69ae2cb85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9736c323f9742988f620055935cf2ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a40808f89e44e05a12ee1587561ad3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cba51e764084f4a96032ddd08058396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0be7a6031feb470085aba84d35c2ee5d",
              "IPY_MODEL_64736854a6e84479ae6ac2d9849050fe",
              "IPY_MODEL_a1ae709277834fe1ace32fcc4f96834a"
            ],
            "layout": "IPY_MODEL_82a89f5a61e74bf1a63f7511d26117ab"
          }
        },
        "0be7a6031feb470085aba84d35c2ee5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e14fee1129d84d15a280688fdff41501",
            "placeholder": "​",
            "style": "IPY_MODEL_95250e29e1fc4863bd85dcb4bcb85d8b",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "64736854a6e84479ae6ac2d9849050fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f32edf1c8c940d0824e8126a17aec54",
            "max": 438011953,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc069484b53a45f09d1d3a97ddfa7810",
            "value": 438011953
          }
        },
        "a1ae709277834fe1ace32fcc4f96834a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2911be61532142d587b3025bde5dfb9a",
            "placeholder": "​",
            "style": "IPY_MODEL_4534bcb912964da9acc1b00d38bc6d77",
            "value": " 438M/438M [00:01&lt;00:00, 240MB/s]"
          }
        },
        "82a89f5a61e74bf1a63f7511d26117ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e14fee1129d84d15a280688fdff41501": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95250e29e1fc4863bd85dcb4bcb85d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f32edf1c8c940d0824e8126a17aec54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc069484b53a45f09d1d3a97ddfa7810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2911be61532142d587b3025bde5dfb9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4534bcb912964da9acc1b00d38bc6d77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "546b6697800142f5a9151879bd5b703d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d46151eedf148dca968809cbd5aa299",
              "IPY_MODEL_c52d5dc10d134f6c9c1e710862c8e5aa",
              "IPY_MODEL_73446268d96548ccb65691f5330969bd"
            ],
            "layout": "IPY_MODEL_97084c7bb59344c69e82e107e99ac7a1"
          }
        },
        "5d46151eedf148dca968809cbd5aa299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4a47e2dbfac43b0abf6c0628eda3ee6",
            "placeholder": "​",
            "style": "IPY_MODEL_418be8ae9efc44cfb8de7f941cf7db56",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c52d5dc10d134f6c9c1e710862c8e5aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d935b661da848a793b8330af533f4d9",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b36c85d0322486199c45d447a96077d",
            "value": 363
          }
        },
        "73446268d96548ccb65691f5330969bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04e4c70015ba4f04953cfa311a138096",
            "placeholder": "​",
            "style": "IPY_MODEL_267bb6610d0c43cda77f2e40d8cd8545",
            "value": " 363/363 [00:00&lt;00:00, 20.8kB/s]"
          }
        },
        "97084c7bb59344c69e82e107e99ac7a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4a47e2dbfac43b0abf6c0628eda3ee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "418be8ae9efc44cfb8de7f941cf7db56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d935b661da848a793b8330af533f4d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b36c85d0322486199c45d447a96077d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04e4c70015ba4f04953cfa311a138096": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "267bb6610d0c43cda77f2e40d8cd8545": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9b1ac80d41d4f63afe61ef30249647f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3473893423c64c21ad54ff03fc969fc2",
              "IPY_MODEL_9b5bca86543a46deb554f26722793fc1",
              "IPY_MODEL_2a4c9e9f318e4f9f82dd19d4eafc010a"
            ],
            "layout": "IPY_MODEL_2a0d7ea1a4c14db0919dfd0f53f66dc9"
          }
        },
        "3473893423c64c21ad54ff03fc969fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49213edb77954249b15ee40190fa6723",
            "placeholder": "​",
            "style": "IPY_MODEL_cc6e4b1d82ef4aaba1423b66f6965785",
            "value": "vocab.txt: 100%"
          }
        },
        "9b5bca86543a46deb554f26722793fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87b6b972d9d046f7b2f93797b105bf5f",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68339754933d4b71ad3cc0ac2e9b2ddb",
            "value": 231536
          }
        },
        "2a4c9e9f318e4f9f82dd19d4eafc010a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4546172525ef42539b70d8684179a478",
            "placeholder": "​",
            "style": "IPY_MODEL_e7dfb984f58549fbafd9c8a1ea4f6b4c",
            "value": " 232k/232k [00:00&lt;00:00, 3.69MB/s]"
          }
        },
        "2a0d7ea1a4c14db0919dfd0f53f66dc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49213edb77954249b15ee40190fa6723": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc6e4b1d82ef4aaba1423b66f6965785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87b6b972d9d046f7b2f93797b105bf5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68339754933d4b71ad3cc0ac2e9b2ddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4546172525ef42539b70d8684179a478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7dfb984f58549fbafd9c8a1ea4f6b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d1a8cd6f42544bd8aff098c81d64bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9aaefc63547943c391f68a6539cfba77",
              "IPY_MODEL_3dd70d93bfbc4cf094bc7a260e60bfb6",
              "IPY_MODEL_d5b7e8d1047b435d8c270a4aa9f6e0b3"
            ],
            "layout": "IPY_MODEL_7af3d17acae24858b3063f849a7c5a06"
          }
        },
        "9aaefc63547943c391f68a6539cfba77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b936cc1c7134f5d9b7b5cf19d0ea572",
            "placeholder": "​",
            "style": "IPY_MODEL_1fe94a8f5c25493185f5fb2ace6495d3",
            "value": "tokenizer.json: 100%"
          }
        },
        "3dd70d93bfbc4cf094bc7a260e60bfb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_347066369e7047f1b846a1fede3efab6",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ed29fb9e37c4c0a93483bcdf50b8ad7",
            "value": 466021
          }
        },
        "d5b7e8d1047b435d8c270a4aa9f6e0b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c06f69c888e941cbaa3c86ebc2b1ec68",
            "placeholder": "​",
            "style": "IPY_MODEL_2efa754cbc934207aaf1d96c6c4a47ac",
            "value": " 466k/466k [00:00&lt;00:00, 2.50MB/s]"
          }
        },
        "7af3d17acae24858b3063f849a7c5a06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b936cc1c7134f5d9b7b5cf19d0ea572": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fe94a8f5c25493185f5fb2ace6495d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "347066369e7047f1b846a1fede3efab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ed29fb9e37c4c0a93483bcdf50b8ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c06f69c888e941cbaa3c86ebc2b1ec68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2efa754cbc934207aaf1d96c6c4a47ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b398f2f636e3439f9def3537c2780981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00770e0d7ebe4f4db64444e221167f88",
              "IPY_MODEL_2df65f850826419cb0f64a9840e18011",
              "IPY_MODEL_d362b59bdc8e4a909d1185d2fb144466"
            ],
            "layout": "IPY_MODEL_8314d73125d94911be32c2c1860d58b2"
          }
        },
        "00770e0d7ebe4f4db64444e221167f88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d090856ee2a41209e390b8429afee67",
            "placeholder": "​",
            "style": "IPY_MODEL_e161172162694385bb982a7f2ba46aae",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "2df65f850826419cb0f64a9840e18011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cff4dd3349747d4bd05575264fcded2",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_89482cf4ec8341a1bc05c66c456d8837",
            "value": 239
          }
        },
        "d362b59bdc8e4a909d1185d2fb144466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1208affd68144a29f356cb3260d94d5",
            "placeholder": "​",
            "style": "IPY_MODEL_8f661c97d2f9430a89b3082753097b83",
            "value": " 239/239 [00:00&lt;00:00, 15.5kB/s]"
          }
        },
        "8314d73125d94911be32c2c1860d58b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d090856ee2a41209e390b8429afee67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e161172162694385bb982a7f2ba46aae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cff4dd3349747d4bd05575264fcded2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89482cf4ec8341a1bc05c66c456d8837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1208affd68144a29f356cb3260d94d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f661c97d2f9430a89b3082753097b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ceef2476a4a24c2ca0b5d376219e5ac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba4c43903b5140e4a77330fd6f7ebafd",
              "IPY_MODEL_bb34908a81e54353b20781310a6e979b",
              "IPY_MODEL_da7db5631eaa4315b98f6342aa21b54a"
            ],
            "layout": "IPY_MODEL_f2ab5483a955444d8c2079f4627a051d"
          }
        },
        "ba4c43903b5140e4a77330fd6f7ebafd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a96bde1074a43f498f3e92c79d1c10a",
            "placeholder": "​",
            "style": "IPY_MODEL_7961c2df5ab54d45be2372d7e0f7df9e",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "bb34908a81e54353b20781310a6e979b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6ffa4923e0448b39ccc69019b21b5d1",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1abbac4c2fc41aca3980a4320d75d3c",
            "value": 190
          }
        },
        "da7db5631eaa4315b98f6342aa21b54a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0753ec466f14f6b8694343643c71d6a",
            "placeholder": "​",
            "style": "IPY_MODEL_cbd3f29428a440e3ae53be40f8f747ac",
            "value": " 190/190 [00:00&lt;00:00, 14.2kB/s]"
          }
        },
        "f2ab5483a955444d8c2079f4627a051d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a96bde1074a43f498f3e92c79d1c10a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7961c2df5ab54d45be2372d7e0f7df9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6ffa4923e0448b39ccc69019b21b5d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1abbac4c2fc41aca3980a4320d75d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0753ec466f14f6b8694343643c71d6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbd3f29428a440e3ae53be40f8f747ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hegtv45hhDGF",
        "outputId": "210508fe-a81d-4cee-a95b-fecb293ba538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.0.1-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_rhFGqrhKlU",
        "outputId": "c41adff9-f42f-46cf-c480-3e6185c88243"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV1xalsphlOx",
        "outputId": "dc3b09ff-96e4-4b58-ea8b-74e58acc1e8d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/132.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: sentence_transformers\n",
            "Successfully installed sentence_transformers-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dorDaxR9hw3W",
        "outputId": "7d5b0216-0497-42ad-f04a-52c96c9359fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_index\n",
            "  Downloading llama_index-0.9.40-py3-none-any.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.9.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.6.3)\n",
            "Collecting deprecated>=1.2.9.3 (from llama_index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama_index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2023.6.0)\n",
            "Collecting httpx (from llama_index)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.23.5)\n",
            "Collecting openai>=1.1.0 (from llama_index)\n",
            "  Downloading openai-1.10.0-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama_index)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (4.5.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.9.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (4.0.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama_index) (1.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (4.66.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama_index) (1.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index) (1.10.14)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index) (1.3.0)\n",
            "Collecting typing-extensions>=4.5.0 (from llama_index)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx->llama_index)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index) (3.6)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama_index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama_index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama_index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama_index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama_index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama_index) (3.20.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index) (2023.3.post1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama_index) (1.2.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama_index) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama_index) (1.16.0)\n",
            "Installing collected packages: dirtyjson, typing-extensions, h11, deprecated, tiktoken, httpcore, httpx, openai, llama_index\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 llama_index-0.9.40 openai-1.10.0 tiktoken-0.5.2 typing-extensions-4.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from llama_index.prompts.prompts import SimpleInputPrompt"
      ],
      "metadata": {
        "id": "-7Sfj4FVh208"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = SimpleDirectoryReader('/content/docs').load_data()"
      ],
      "metadata": {
        "id": "fq9H0ZQeiTnP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp2l2EL5idp1",
        "outputId": "991ff0ba-eb5d-406d-a02a-ebd321ee6a8f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id_='09c0656c-5048-47bf-b7e1-5396173320cf', embedding=None, metadata={'page_label': '1', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='DATA WAREHOUSING\\nFUNDAMENTALSData Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b162f5ae-658b-4ab2-94e6-ea45d1e53462', embedding=None, metadata={'page_label': '2', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='DATA WAREHOUSING\\nFUNDAMENTALS\\nA Comprehensive Guide for\\nIT Professionals\\nPAULRAJ PONNIAH\\nA Wiley-Interscience Publication\\nJOHN WILEY & SONS, INC.New York / Chichester / Weinheim / Brisbane / Singapore / Toronto\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ef68ee65-ec8f-4c41-96f0-6da53f50a26a', embedding=None, metadata={'page_label': '3', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Designations used by companies to distinguish their products are often claimed as trademarks.  In all instances\\nwhere John Wiley & Sons, Inc., is aware of a claim, the product names appear in initial capital or ALL CAPITALLETTERS.  Readers, however, should contact the appropriate companies for more complete information regardingtrademarks and registration.\\nCopyright © 2001 by John Wiley & Sons, Inc. All rights reserved.\\nNo part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electro nic\\nor mechanical, including uploading, downloading, printing, decompiling, recording or otherwise, except as permitted underSections 107 or 108 of the 1976 United States Copyright Act, without the prior written permission of the Publisher. Requests tothe Publisher for permission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 605 Third Avenue,New York, NY  10158-0012, (212) 850-6011, fax (212) 850-6008, E-Mail: PERMREQ @ WILEY.COM.\\nThis publication is designed to provide accurate and authoritative information in regard to the \\nsubject matter covered. It is sold with the understanding that the publisher is not engaged in rendering professional services. If professional advice or other expert assistance is required, theservices of a competent professional person should be sought.\\nISBN 0-471-22162-7This title is also available in print as ISBN 0-471-41254-6.For more information about Wiley products, visit our web site at www.Wiley.com. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='da98cffc-21ac-4372-ac39-da75a1f39fe7', embedding=None, metadata={'page_label': '4', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='To\\nVimala, my loving wife\\nand to\\nJoseph, David, and Shobi,\\nmy dear children', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='57b16e37-5dcf-4a73-828a-1f214f342d86', embedding=None, metadata={'page_label': '5', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CONTENTS\\nForeword xxi\\nPreface xxiii\\nPart 1 OVERVIEW AND CONCEPTS\\n1 The Compelling Need for Data Warehousing 1\\n1Chapter Objectives 1\\n1Escalating Need for Strategic Information 2\\n1 The Information Crisis 3\\n1 Technology Trends 4\\n1 Opportunities and Risks 5\\n1Failures of Past Decision-Support Systems 7\\n1 History of Decision-Support Systems 8\\n1 Inability to Provide Information 9\\n1Operational Versus Decision-Support Systems 9\\n1 Making the Wheels of Business Turn 10\\n1 Watching the Wheels of Business Turn 10\\n1 Different Scope, Different Purposes 10\\n1Data Warehousing—The Only Viable Solution 12\\n1 A New Type of System Environment 12\\n1 Processing Requirements in the New Environment 12\\n1 Business Intelligence at the Data Warehouse 12\\n1Data Warehouse Defined 13\\n1 A Simple Concept for Information Delivery 14\\nvii', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='47433e87-7b39-4eed-8a41-6a3c9a31a812', embedding=None, metadata={'page_label': '6', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1 An Environment, Not a Product 14\\n1 A Blend of Many Technologies 14\\n1Chapter Summary 15\\n1Review Questions 16\\n1Exercises 16\\n2 Data Warehouse: The Building Blocks 19\\n1Chapter Objectives 19\\n1Defining Features 20\\n1 Subject-Oriented Data 20\\n1 Integrated Data 21\\n1 Time-V ariant Data 22\\n1 Nonvolatile Data 23\\n1 Data Granularity 23\\n1Data Warehouses and Data Marts 24\\n1 How are They Different? 25 1\\n1 Top-Down V ersus Bottom-Up Approach 26\\n1 A Practical Approach 27\\n1Overview of the Components 28\\n1 Source Data Component 28\\n1 Data Staging Component 31\\n1 Data Storage Component 33\\n1 Information Delivery Component 34\\n1 Metadata Component 35\\n1 Management and Control Component 35\\n1Metadata in the Data Warehouse 35\\n1 Types of Metadata 36\\n1 Special Significance 36\\n1Chapter Summary 36\\n1Review Questions 37\\n1Exercises 37\\n3 Trends in Data Warehousing 39\\n1Chapter Objectives 39\\n1Continued Growth in Data Warehousing 40\\n1 Data Warehousing is Becoming Mainstream 40\\n1 Data Warehouse Expansion 41\\n1 V endor Solutions and Products 42\\n1Significant Trends 43\\n1 Multiple Data Types 44\\n1 Data Visualization 46\\n1 Parallel Processing 48viii CONTENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1e88a8a3-f65a-4911-86bc-51d3f0da3504', embedding=None, metadata={'page_label': '7', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1 Query Tools 49\\n1 Browser Tools 50\\n1 Data Fusion 50\\n1 Multidimensional Analysis 51\\n1 Agent Technology 51\\n1 Syndicated Data 52\\n1 Data Warehousing and ERP 52\\n1 Data Warehousing and KM 53\\n1 Data Warehousing and CRM 54\\n1 Active Data Warehousing 56\\n1Emergence of Standards 56\\n1 Metadata 57\\n1 OLAP 57\\n1Web-Enabled Data Warehouse 58\\n1 The Warehouse to the Web 59\\n1 The Web to the Warehouse 59\\n1 The Web-Enabled Configuration 60\\n1Chapter Summary 61\\n1Review Questions 61\\n1Exercises 62\\nPart 2 PLANNING AND REQUIREMENTS\\n4 Planning and Project Management 63\\n1Chapter Objectives 63\\n1Planning Y our Data Warehouse 64\\n1 Key Issues 64\\n1 Business Requirements, Not Technology 66\\n1 Top Management Support 67\\n1 Justifying Your Data Warehouse 67\\n1 The Overall Plan 68\\n1The Data Warehouse Project 69\\n1 How is it Different? 70\\n1 Assessment of Readiness 71\\n1 The Life-Cycle Approach 71\\n1 The Development Phases 73\\n1The Project Team 74\\n1 Organizing the Project Team 75\\n1 Roles and Responsibilities 75\\n1 Skills and Experience Levels 77\\n1 User Participation 78\\n1Project Management Considerations 80\\n1 Guiding Principles 81CONTENTS ix', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='895e2f0a-48b1-447a-8523-efd51bc6ca4a', embedding=None, metadata={'page_label': '8', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1 Warning Signs 82\\n1 Success Factors 82\\n1 Anatomy of a Successful Project 83\\n1 Adopt a Practical Approach 84\\n1Chapter Summary 86\\n1Review Questions 86\\n1Exercises 87\\n5 Defining the Business Requirements 89\\n1Chapter Objectives 89\\n1Dimensional Analysis 90\\n1 Usage of Information Unpredictable 90\\n1 Dimensional Nature of Business Data 90\\n1 Examples of Business Dimensions 92\\n1Information Packages—A New Concept 93\\n1 Requirements Not Fully Determinate 93\\n1 Business Dimensions 95\\n1 Dimension Hierarchies/Categories 95\\n1 Key Business Metrics or Facts 96\\n1Requirements Gathering Methods 97\\n1 Interview Techniques 99\\n1 Adapting the JAD Methodology 102\\n1 Review of Existing Documentation 103\\n1Requirements Definition: Scope and Content 104\\n1 Data Sources 105\\n1 Data Transformation 105\\n1 Data Storage 105\\n1 Information Delivery 105\\n1 Information Package Diagrams 106\\n1 Requirements Definition Document Outline 106\\n1Chapter Summary 106\\n1Review Questions 107\\n1Exercises 107\\n6 Requirements as the Driving Force for Data Warehousing 109\\n1Chapter Objectives 109\\n1Data Design 110\\n1 Structure for Business Dimensions 112\\n1 Structure for Key Measurements 112\\n1 Levels of Detail 113\\n1The Architectural Plan 113\\n1 Composition of the Components 114x CONTENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1fe22721-4bba-404f-a000-ed7b49aff731', embedding=None, metadata={'page_label': '9', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1 Special Considerations 115\\n1 Tools and Products 118\\n1Data Storage Specifications 119\\n1 DBMS Selection 120\\n1 Storage Sizing 120\\n1Information Delivery Strategy 121\\n1 Queries and Reports 122\\n1 Types of Analysis 123\\n1 Information Distribution 123 1\\n1 Decision Support Applications 123\\n1 Growth and Expansion 123\\n1Chapter Summary 124\\n1Review Questions 124\\n1Exercises 125\\nPart 3 ARCHITECTURE AND INFRASTRUCTURE\\n7 The Architectural Components 127\\n1Chapter Objectives 127\\n1Understanding Data Warehouse Architecture 127\\n1 Architecture: Definitions 127\\n1 Architecture in Three Major Areas 128\\n1Distinguishing Characteristics 129\\n1 Different Objectives and Scope 130\\n1 Data Content 130\\n1 Complex Analysis and Quick Response 131\\n1 Flexible and Dynamic 131\\n1 Metadata-driven 132\\n1Architectural Framework 132\\n1 Architecture Supporting Flow of Data 132\\n1 The Management and Control Module 133\\n1Technical Architecture 134\\n1 Data Acquisition 135\\n1 Data Storage 138\\n1 Information Delivery 140\\n1Chapter Summary 142\\n1Review Questions 142\\n1Exercises 143\\n8 Infrastructure as the Foundation for Data Warehousing 145\\n1Chapter Objectives 145\\n1Infrastructure Supporting Architecture 145CONTENTS xi', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='66b1281c-eab7-4faf-a703-6cbc2d4811c8', embedding=None, metadata={'page_label': '10', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1 Operational Infrastructure 147\\n1 Physical Infrastructure 147\\n1Hardware and Operating Systems 148\\n1 Platform Options 150\\n1 Server Hardware 158\\n1Database Software 164\\n1 Parallel Processing Options 164\\n1 Selection of the DBMS 166\\n1Collection of Tools 167\\n1 Architecture First, Then Tools 168\\n1 Data Modeling 169\\n1 Data Extraction  169\\n1 Data Transformation 169\\n1 Data Loading 169\\n1 Data Quality 169\\n1 Queries and Reports 170\\n1 Online Analytical Processing (OLAP) 170\\n1 Alert Systems 170\\n1 Middleware and Connectivity 170\\n1 Data Warehouse Management 170\\n1Chapter Summary 170\\n1Review Questions 171\\n1Exercises 171\\n9 The Significant Role of Metadata 173\\n1Chapter Objectives 173\\n1Why Metadata is Important 173\\n1 A Critical Need in the Data Warehouse 175\\n1 Why Metadata is Vital for End-Users 177\\n1 Why Metadata is Essential for IT 179\\n1 Automation of Warehousing Tasks 181\\n1 Establishing the Context of Information 183\\n1Metadata Types by Functional Areas 183\\n1 Data Acquisition 184\\n1 Data Storage 186\\n1 Information Delivery 186\\n1Business Metadata 187\\n1 Content Overview 188\\n1 Examples of Business Metadata 188\\n1 Content Highlights 189\\n1 Who Benefits? 190\\n1Technical Metadata 190xii CONTENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b3aba82e-06a7-4dc8-8fc0-83d7d0fad054', embedding=None, metadata={'page_label': '11', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12 Content Overview 190\\n12 Examples of Technical Metadata 191\\n12 Content Highlights 192\\n12 Who Benefits? 192\\n12 How to Provide Metadata 193\\n12 Metadata Requirements 193\\n12 Sources of Metadata 194\\n12 Challenges for Metadata Management 196\\n12 Metadata Repository 196\\n12 Metadata Integration and Standards 198\\n12 Implementation Options 199\\n12 Chapter Summary 200\\n12 Review Questions 201\\n12 Exercises 201\\nPart 4 DATA DESIGN AND DATA PREPARATION\\n10 Principles of Dimensional Modeling 203\\n11 Chapter Objectives 203\\n11 From Requirements to Data Design 203\\n12 Design Decisions 204\\n12 Dimensional Modeling Basics 204\\n12 E-R Modeling V ersus Dimensional Modeling 209\\n12 Use of CASE Tools 209\\n11 The STAR Schema 210\\n12 Review of a Simple STAR Schema 210\\n12 Inside a Dimension Table 212\\n12 Inside the Fact Table 214\\n12 The Factless Fact Table 216\\n12 Data Granularity 217\\n11 STAR Schema Keys 218\\n12 Primary Keys 218\\n12 Surrogate Keys 219\\n12 Foreign Keys 219\\n11 Advantages of the STAR Schema 220\\n12 Easy for Users to Understand 220\\n12 Optimizes Navigation 221\\n12 Most Suitable for Query Processing 222\\n12 STARjoin and STARindex 223\\n11 Chapter Summary 223\\n11 Review Questions 224\\n11 Exercises 224CONTENTS xiii', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='901672ad-3612-45bd-8e18-7050edc3973d', embedding=None, metadata={'page_label': '12', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11 Dimensional Modeling: Advanced Topics 225\\n11 Chapter Objectives 225\\n11 Updates to the Dimension Tables 226\\n12 Slowly Changing Dimensions 226\\n12 Type 1 Changes: Correction of Errors 227\\n12 Type 2 Changes: Preservation of History 228\\n12 Type 3 Changes: Tentative Soft Revisions 230\\n11 Miscellaneous Dimensions 231\\n12 Large Dimensions 231\\n12 Rapidly Changing Dimensions 233\\n12 Junk Dimensions 235\\n11 The Snowflake Schema 235\\n12 Options to Normalize 235\\n12 Advantages and Disadvantages  238\\n12 When to Snowflake 238\\n11 Aggregate Fact Tables 239\\n12 Fact Table Sizes 241\\n12 Need for Aggregates 242\\n12 Aggregating Fact Tables 243\\n12 Aggregation Options 247\\n11 Families of STARS 249\\n12 Snapshot and Transaction Tables 250\\n12 Core and Custom Tables 251\\n12 Supporting Enterprise V alue Chain or V alue Circle 251\\n12 Conforming Dimensions 253\\n12 Standardizing Facts 254\\n12 Summary of Family of STARS 254\\n11 Chapter Summary 255\\n11 Review Questions 255\\n11 Exercises 256\\n12 Data Extraction, Transformation, and Loading 257\\n11 Chapter Objectives 257\\n11 ETL Overview 258\\n12 Most Important and Most Challenging 259\\n12 Time-consuming and Arduous 260\\n12 ETL Requirements and Steps 260\\n12 Key Factors 261\\n11 Data Extraction 262\\n12 Source Identification 263\\n12 Data Extraction Techniques 263\\n12 Evaluation of the Techniques 270xiv CONTENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d6505336-74aa-4da2-a55d-3a40773eb673', embedding=None, metadata={'page_label': '13', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11 Data Transformation 271\\n12 Data Transformation: Basic Tasks 272\\n12 Major Transformation Types  273\\n12 Data Integration and Consolidation 275\\n12 Transformation for Dimension Attributes 277\\n12 How to Implement Transformation 277\\n11 Data Loading 279\\n12 Applying Data: Techniques and Processes 280\\n12 Data Refresh V ersus Update 282\\n12 Procedure for Dimension Tables 283\\n12 Fact Tables: History and Incremental Loads 284\\n12 ETL Summary 285\\n12 ETL Tool Options 285\\n12 Reemphasizing ETL Metadata 286\\n12 ETL Summary and Approach 287\\n11 Chapter Summary 288\\n11 Review Questions 288\\n11 Exercises 289\\n13 Data Quality: A Key to Success 291\\n11 Chapter Objectives 291\\n11 Why is Data Quality Critical? 292\\n12 What is Data Quality? 292\\n12 Benefits of Improved Data Quality 295\\n12 Types of Data Quality Problems 296\\n11 Data Quality Challenges 299\\n12 Sources of Data Pollution 299\\n12 V alidation of Names and Addresses 301\\n12 Costs of Poor Data Quality 302\\n11 Data Quality Tools 303\\n12 Categories of Data Cleansing Tools 303\\n12 Error Discovery Features  303\\n12 Data Correction Features 303\\n12 The DBMS for Quality Control 304\\n11 Data Quality Initiative 304\\n12 Data Cleansing Decisions 305\\n12 Who Should be Responsible? 307\\n12 The Purification Process 309\\n12 Practical Tips on Data Quality 311\\n11 Chapter Summary 311\\n11 Review Questions 312\\n11 Exercises 312CONTENTS xv', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='46870774-5417-4fb4-ad2e-c8e217ff2cd4', embedding=None, metadata={'page_label': '14', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Part 5 INFORMATION ACCESS AND DELIVERY\\n14 Matching Information to the Classes of Users 315\\n11 Chapter Objectives 315\\n11 Information from the Data Warehouse 316\\n12 Data Warehouse V ersus Operational Systems 316\\n12 Information Potential 318\\n12 User-Information Interface 321\\n12 Industry Applications 323\\n11 Who Will Use the Information? 323\\n12 Classes of Users 323\\n12 What They Need 326\\n12 How to Provide Information 329\\n11 Information Delivery 329\\n12 Queries 331\\n12 Reports 332\\n12 Analysis 333\\n12 Applications 334\\n11 Information Delivery Tools 335\\n12 The Desktop Environment 335\\n12 Methodology for Tool Selection 335\\n12 Tool Selection Criteria 338\\n12 Information Delivery Framework 340\\n11 Chapter Summary 341\\n11 Review Questions 341\\n11 Exercises 341\\n15 OLAP in the Data Warehouse 343\\n11 Chapter Objectives 343\\n11 Demand for Online Analytical Processing 344\\n12 Need for Multidimensional Analysis 344\\n12 Fast Access and Powerful Calculations 345\\n12 Limitations of Other Analysis Methods 347\\n12 OLAP is the Answer 349\\n12 OLAP Definitions and Rules 349\\n12 OLAP Characteristics 352\\n11 Major Features and Functions 353\\n12 General Features 353\\n12 Dimensional Analysis 353\\n12 What are Hypercubes? 357\\n12 Drill-Down and Roll-Up 360\\n12 Slice-and-Dice or Rotation 362xvi CONTENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2b1ecece-c8a6-42a8-b7a4-2fdf64568f65', embedding=None, metadata={'page_label': '15', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12 Uses and Benefits 363\\n11 OLAP Models 363\\n12 Overview of V ariations 364\\n12 The MOLAP Model 365\\n12 The ROLAP Model 366\\n12 ROLAP V ersus MOLAP 367\\n11 OLAP Implementation Considerations 368\\n12 Data Design and Preparation 368\\n12 Administration and Performance 370\\n12 OLAP Platforms 372\\n12 OLAP Tools and Products 373\\n12 Implementation Steps 374\\n11 Chapter Summary 374\\n11 Review Questions 374\\n11 Exercises 375\\n16 Data Warehousing and the Web 377\\n11 Chapter Objectives 377\\n11 Web-Enabled Data Warehouse 378\\n12 Why the Web? 378\\n12 Convergence of Technologies 380\\n12 Adapting the Data Warehouse for the Web 381\\n12 The Web as a Data Source 382\\n11 Web-Based Information Delivery 383\\n12 Expanded Usage 383\\n12 New Information Strategies 385\\n12 Browser Technology for the Data Warehouse 387\\n12 Security Issues 389\\n11 OLAP and the Web 389\\n12 Enterprise OLAP 389\\n12 Web-OLAP Approaches 390\\n12 OLAP Engine Design 390\\n11 Building a Web-Enabled Data Warehouse 391\\n12 Nature of the Data Webhouse 391\\n12 Implementation Considerations 393\\n12 Putting the Pieces Together 394\\n12 Web Processing Model 394\\n11 Chapter Summary 396\\n11 Review Questions 396\\n11 Exercises 396CONTENTS xvii', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e3885ad2-c65f-4723-8ac8-aac1dbf5b26d', embedding=None, metadata={'page_label': '16', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17 Data Mining Basics 399\\n11 Chapter Objectives 399\\n11 What is Data Mining? 400\\n12 Data Mining Defined 401\\n12 The Knowledge Discovery Process 402\\n12 OLAP V ersus Data Mining 404\\n12 Data Mining and the Data Warehouse 406\\n11 Major Data Mining Techniques 408\\n12 Cluster Detection 409\\n12 Decision Trees 411\\n12 Memory-Based Reasoning 413\\n12 Link Analysis 415\\n12 Neural Networks 417\\n12 Genetic Algorithms 418\\n12 Moving into Data Mining 419\\n11 Data Mining Applications 422\\n12 Benefits of Data Mining 423\\n12 Applications in Retail Industry 424\\n12 Applications in Telecommunications Industry 425\\n12 Applications in Banking and Finance 426\\n11 Chapter Summary 426\\n11 Review Questions 426\\n11 Exercises 427\\nPart 6 IMPLEMENTATION AND MAINTENANCE\\n18 The Physical Design Process 429\\n11 Chapter Objectives 429\\n11 Physical Design Steps 430\\n12 Develop Standards 430\\n12 Create Aggregates Plan 431\\n12 Determine the Data Partitioning Scheme 431\\n12 Establish Clustering Options 432\\n12 Prepare an Indexing Strategy 432\\n12 Assign Storage Structures 432\\n12 Complete Physical Model 433\\n11 Physical Design Considerations 433\\n12 Physical Design Objectives 433\\n12 From Logical Model to Physical Model 434\\n12 Physical Model Components 435\\n12 Significance of Standards 436\\n11 Physical Storage 438xviii CONTENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4113f5c6-dcec-4c82-89b0-f23dac2e20e6', embedding=None, metadata={'page_label': '17', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12 Storage Area Data Structures 439\\n12 Optimizing Storage 440\\n12 Using RAID Technology 442\\n12 Estimating Storage Sizes 442\\n11 Indexing the Data Warehouse 443\\n12 Indexing Overview 443\\n12 B-Tree Index 445\\n12 Bitmapped Index 446\\n12 Clustered Indexes 448\\n12 Indexing the Fact Table 448\\n12 Indexing the Dimension Tables 449\\n11 Performance Enhancement Techniques 449\\n12 Data Partitioning 449\\n12 Data Clustering 450\\n12 Parallel Processing 450\\n12 Summary Levels 451\\n12 Referential Integrity Checks 451\\n12 Initialization Parameters 451\\n12 Data Arrays 452\\n11 Chapter Summary 452\\n11 Review Questions 452\\n11 Exercises 453\\n19 Data Warehouse Deployment 455\\n11 Chapter Objectives 455\\n11 Major Deployment Activities 456\\n12 Complete User Acceptance 456\\n12 Perform Initial Loads 457\\n12 Get User Desktops Ready 458\\n12 Complete Initial User Training 459\\n12 Institute Initial User Support 460\\n12 Deploy in Stages 460\\n11 Considerations for a Pilot 462\\n12 When Is a Pilot Data Mart Useful? 462\\n12 Types of Pilot Projects 463\\n12 Choosing the Pilot 465\\n12 Expanding and Integrating the Pilot 466\\n11 Security 467\\n12 Security Policy 467\\n12 Managing User Privileges 468\\n12 Password Considerations 469\\n12 Security Tools 469CONTENTS xix', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a3a2fa7c-8fe8-4cb9-b038-6896ae5938fa', embedding=None, metadata={'page_label': '18', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11 Backup and Recovery 470\\n12 Why Back Up the Data Warehouse? 470\\n12 Backup Strategy 471\\n12 Setting Up a Practical Schedule 472\\n12 Recovery 472\\n11 Chapter Summary 473\\n11 Review Questions 474\\n11 Exercises 474\\n20 Growth and Maintenance 477\\n11 Chapter Objectives 477\\n11 Monitoring the Data Warehouse 478\\n12 Collection of Statistics 478\\n12 Using Statistics for Growth Planning 480\\n12 Using Statistics for Fine-Tuning 480\\n12 Publishing Trends for Users 481\\n11 User Training and Support 481\\n12 User Training Content 482\\n12 Preparing the Training Program 482\\n12 Delivering the Training Program 484\\n12 User Support 485\\n11 Managing the Data Warehouse 487\\n12 Platform Upgrades 487\\n12 Managing Data Growth 488\\n12 Storage Management 488\\n12 ETL Management 489\\n12 Data Model Revisions 489\\n12 Information Delivery Enhancements 489\\n12 Ongoing Fine-Tuning 490\\n11 Chapter Summary 490\\n11 Review Questions 491\\n11 Exercises 491\\nAppendix A. Project Life Cycle Steps and Checklists 493\\nAppendix B. Critical Factors for Success 497Appendix C. Guidelines for Evaluating Vendor Solutions 499References 501Glossary 503Index 511xx CONTENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1ad21942-c816-479a-a6da-261ef951ff9d', embedding=None, metadata={'page_label': '19', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='FOREWORD\\nI am delighted to share my thoughts with information technology professionals about my\\nfaculty colleague Paulraj Ponniah’ s textbook Data Warehousing Fundamentals. In the\\nspring of 1998, Raritan Valley Community College decided to offer a course on datawarehousing. This was mainly through the initiative of Dr. Ponniah, who had been teach-ing our database design and development course for several years. It was very difficult tofind a good textbook for a college course on data warehousing. We had to settle for a bookthat was not quite suitable. In order to make the course effective, Paul had to supplementthe book with his own data warehousing seminar materials. Our students, primarily ITprofessionals from local industries, received the course very well. Now this magnificenttextbook on data warehousing comes to you through the foresight and diligent work of Dr.Ponniah, along with the insightful support of the publishers, John Wiley and Sons. \\nThis book has numerous features that make it a winner:\\n/L50539The order of topics is very logical.\\n/L50539The choice of topics is quite appropriate for a comprehensive introductory book.\\nThe coverage of topics is also very well balanced. \\n/L50539The subject matter is logically structured, with chapters covering essential compo-\\nnents of the data warehousing field. The sequence of topics is well planned to pro-vide a seamless transition from design to implementation.\\n/L50539Within each chapter, the continuity of topics is excellent.\\n/L50539None of the topics included in the textbook is superfluous to the basic objectives.\\n/L50539The material included is technically correct and up-to-date. The figures appropriate-\\nly enhance and amplify the topics. \\n/L50539Ample review questions and exercises can be found at the end of each chapter. This\\nis something lacking in most books on data warehousing. These review questionsand exercises are pedagogically sound. They are designed to test the knowledge, notthe ignorance, of the reader.\\nxxi', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eaa86f6d-668c-478c-8ca5-30076a26e9c1', embedding=None, metadata={'page_label': '20', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Dr. Ponniah’ s writing style is clear and concise. Because of the simplicity and com-\\npleteness of this book, I believe it will find a definite market niche, particularly amongcollege students, not-so-technically savvy IT people, and data warehousing mavens.\\nIn spite of a plethora of books on data warehousing by luminaries such as Kimball, In-\\nmon, Barquin, and Singh, this book fulfills a special purpose, and information technologyprofessionals will definitely benefit from reading it. In addition, the book should be wellreceived by college professors for use by students in their data warehousing courses. Toput it succinctly, this book fills a void in the midst of plenty. \\nIn summary, Dr. Ponniah has produced a winner for both students and experienced IT\\nprofessionals. As someone who has been in IT education for many years, I certainly rec-ommend this book to college professors and seminar leaders for their data warehousingcourses.\\nP\\nRATAP P.  R EDDY , Ph.D.\\nProfessor and Chair of CIS Department\\nRaritan Valley Community CollegeNorth Branch, New Jerseyxxii FOREWORD', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5210af7c-5af7-43e3-b78b-033afd31a377', embedding=None, metadata={'page_label': '21', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='PREFACE\\nTHIS BOOK IS FOR YOU\\nAre you an information technology professional watching, with great interest, the massive\\nunfolding of the data warehouse movement? Are you contemplating a move into this newarea of opportunity? Are you a systems analyst, programmer, data analyst, database ad-ministrator, project leader, or software engineer eager to grasp the fundamentals of datawarehousing? Do you wonder how many different books you may have to read to learn thebasics? Are you lost in the maze of the literature and products on the subject? Do youwish for a single publication on data warehousing, clearly and specifically designed for ITprofessionals? Do you need a textbook that helps you learn the fundamentals in sufficientdepth—not more, not less? If you answered “yes” to any of the above, this book is writtenspecially for you.\\nThis is the onedefinitive book on data warehousing clearly intended for IT profession-\\nals. The organization and presentation of the book are specially tuned for IT professionals.This book does not presume to target anyone and everyone remotely interested in the sub-ject for some reason or another, but is written to address the specific needs of IT profes-sionals like you. It does not tend to emphasize certain aspects and neglect other criticalones. The book takes you over the entire landscape of data warehousing. \\nHow can this book be exactly suitable for IT professionals? As a veteran IT profession-\\nal with wide and intensive industry experience, as a successful database and data ware-housing consultant for many years, and as one who teaches data warehousing fundamen-tals in the college classroom and in public seminars, I have come to appreciate the preciseneeds of IT professionals, and in every chapter I have incorporated these requirements ofthe IT community.\\nxxiii', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='de458e21-2c19-4f0e-89f6-5b544e47a409', embedding=None, metadata={'page_label': '22', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='THE SCENARIO \\nWhy are companies rushing into data warehousing? Why is there a tremendous surge in\\ninterest? Data warehousing is no longer a purely novel idea just for research and experi-mentation. It has become a mainstream phenomenon. True, the data warehouse is not inevery doctor’ s office yet, but neither is it confined to only high-end businesses. More thanhalf of all U.S. companies and a large percentage of worldwide businesses have made acommitment to data warehousing. \\nIn every industry across the board, from retail chain stores to financial institutions,\\nfrom manufacturing enterprises to government departments, and from airline companiesto utility businesses, data warehousing is revolutionizing the way people perform businessanalysis and make strategic decisions. Every company that has a data warehouse is realiz-ing the enormous benefits translated into positive results at the bottom line. These compa-nies, now incorporating Web-based technologies, are enhancing the potential for greaterand easier delivery of vital information. \\nOver the past five years, hundreds of vendors have flooded the market with numerous\\ndata warehousing products. Vendor solutions and products run the gamut of data ware-housing—data modeling, data acquisition, data quality, data analysis, metadata, and soon. The market is already large and continues to grow.\\nCHANGED ROLE OF IT \\nIn this scenario, information technology departments of all progressive companies per-\\nceive a radical change in their roles. IT is no longer required to create every report andpresent every screen for providing information to the end-users. IT is now charged withthe building of information delivery systems and letting the end-users themselves retrieveinformation in innovative ways for analysis and decision making. Data warehousing isproving to be just that type of successful information delivery system. \\nIT professionals responsible for building data warehouses need to revise their mindsets\\nabout building applications. They have to understand that a data warehouse is not a one-size-fits-all proposition; they must get a clear understanding of the extraction of data fromsource systems, data transformations, data staging, data warehouse architecture, infra-structure, and the various methods of information delivery. \\nIn short, IT professionals, like you, must get a strong grip on the fundamentals of data\\nwarehousing.\\nWHAT THIS BOOK CAN DO FOR YOU\\nThe book is comprehensive and detailed. Y ou will be able to study every significant topic\\nin planning, requirements, architecture, infrastructure, design, data preparation, informa-tion delivery, deployment, and maintenance. It is specially designed for IT professionals;you will be able to follow the presentation easily because it is built upon the foundation ofyour background as an IT professional, your knowledge, and the technical terminology fa-miliar to you. It is organized logically, beginning with an overview of concepts, movingon to planning and requirements, then to architecture and infrastructure, on to data design,then to information delivery, and concluding with deployment and maintenance. This pro-xxiv PREFACE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='62b25981-a432-477a-8207-755e6f63bee9', embedding=None, metadata={'page_label': '23', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='gression is typical of what you are most familiar with in your experience and day-to-day\\nwork.\\nThe book provides an interactive learning experience. It is not a one-way lecture. Y ou\\nparticipate through the review questions and exercises at the end of each chapter. For eachchapter, the objectives set the theme and the summary provides a list of the topics cov-ered. Y ou can relate each concept and technique to the data warehousing industry andmarketplace. Y ou will notice a substantial number of industry examples. Although intend-ed as a first course on fundamentals, this book provides sufficient coverage of each topicso that you can comfortably proceed to the next step of specialization for specific roles ina data warehouse project.\\nFeaturing all the significant topics in appropriate measure, this book is eminently suit-\\nable as a textbook for serious self-study, a college course, or a seminar on the essentials. Itprovides an opportunity for you to become a data warehouse expert. \\nI acknowledge my indebtedness to the authors listed in the reference section at the end\\nof the book. Their insights and observations have helped me cover adequately the topics. Imust also express my appreciation to my students and professional colleagues. Our inter-actions have enabled me to shape this textbook according to the needs of IT professionals. \\nP\\nAULRAJ PONNIAH , Ph.D. \\nEdison, New Jersey\\nJune 2001PREFACE xxv', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0bce5bc7-b03f-4589-80f3-09ddc3da72de', embedding=None, metadata={'page_label': '24', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='DATA WAREHOUSING\\nFUNDAMENTALS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e0e8828f-a932-4f90-84da-30db157976af', embedding=None, metadata={'page_label': '25', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 1\\nTHE COMPELLING NEED \\nFOR DATA WAREHOUSING\\nCHAPTER OBJECTIVES\\n/L50539Understand the desperate need for strategic information\\n/L50539Recognize the information crisis at every enterprise\\n/L50539Distinguish between operational and informational systems\\n/L50539Learn why all past attempts to provide strategic information failed\\n/L50539Clearly see why data warehousing is the viable solution\\nAs an information technology professional, you have worked on computer applications\\nas an analyst, programmer, designer, developer, database administrator, or project manag-er. Y ou have been involved in the design, implementation, and maintenance of systemsthat support day-to-day business operations. Depending on the industries you haveworked in, you must have been involved in applications such as order processing, generalledger, inventory, in-patient billing, checking accounts, insurance claims, and so on. \\nThese applications are important systems that run businesses. They process orders,\\nmaintain inventory, keep the accounting books, service the clients, receive payments, andprocess claims. Without these computer systems, no modern business can survive. Com-panies started building and using these systems in the 1960s and have become completelydependent on them. As an enterprise grows larger, hundreds of computer applications areneeded to support the various business processes. These applications are effective in whatthey are designed to do. They gather, store, and process all the data needed to successfullyperform the daily operations. They provide online information and produce a variety ofreports to monitor and run the business.\\nIn the 1990s, as businesses grew more complex, corporations spread globally, and\\ncompetition became fiercer, business executives became desperate for information to staycompetitive and improve the bottom line. The operational computer systems did provideinformation to run the day-to-day operations, but what the executives needed were differ-ent kinds of information that could be readily used to make strategic decisions. They\\n1Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6e6c32c3-0227-48ef-a96b-90cefc410f55', embedding=None, metadata={'page_label': '26', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='wanted to know where to build the next warehouse, which product lines to expand, and\\nwhich markets they should strengthen. The operational systems, important as they were,could not provide strategic information. Businesses, therefore, were compelled to turn tonew ways of getting strategic information.\\nData warehousing is a new paradigm specifically intended to provide vital strategic in-\\nformation. In the 1990s, organizations began to achieve competitive advantage by build-ing data warehouse systems. Figure 1-1 shows a sample of strategic areas where datawarehousing is already producing results in different industries.\\nWe will now briefly examine a crucial question: why do enterprises really need data\\nwarehouses? This discussion is important because unless we grasp the significance of thiscritical need, our study of data warehousing will lack motivation. So, please pay close at-tention. \\nESCALATING NEED FOR STRATEGIC INFORMATION\\nWhile we discuss the clamor by enterprises for strategic information, we need to look at\\nthe prevailing information crisis that is holding them back as well as the technology trendsof the past few years that are working in our favor, enabling us to provide strategic infor-mation. Our discussion of the need for strategic information will not be complete unlesswe study the opportunities provided by strategic information and the risks facing a com-pany without such information. \\nWho needs strategic information in an enterprise? What exactly do we mean by strate-\\ngic information? The executives and managers who are responsible for keeping the enter-prise competitive need information to make proper decisions. They need information toformulate the business strategies, establish goals, set objectives, and monitor results. \\nHere are some examples of business objectives:\\n/L50539Retain the present customer base\\n/L50539Increase the customer base by 15% over the next 5 years2 THE COMPELLING NEED FOR DATA WAREHOUSING\\n/G75Retail\\n/G75Customer Loyalty\\n/G75Market Planning\\n/G75Financial\\n/G75Risk Management\\n/G75Fraud Detection\\n/G75Airlines\\n/G75Route Profitability\\n/G75Yield Management/G75Manufacturing\\n/G75Cost Reduction\\n/G75Logistics Management\\n/G75Utilities\\n/G75Asset Management\\n/G75Resource Management\\n/G75Government\\n/G75Manpower Planning\\n/G75Cost ControlOrganizations achieve competitive advantage:\\nFigure 1-1 Organizations’ use of data warehousing.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='38edb07f-6497-4d5c-81a5-ccdf321f7cc2', embedding=None, metadata={'page_label': '27', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Gain market share by 10% in the next 3 years\\n/L50539Improve product quality levels in the top five product groups\\n/L50539Enhance customer service level in shipments\\n/L50539Bring three new products to market in 2 years\\n/L50539Increase sales by 15% in the North East Division\\nFor making decisions about these objectives, executives and managers need informa-\\ntion for the following purposes: to get in-depth knowledge of their company’ s operations;learn about the key business factors and how these affect one another; monitor how thebusiness factors change over time; and compare their company’ s performance relative tothe competition and to industry benchmarks. Executives and managers need to focus theirattention on customers’ needs and preferences, emerging technologies, sales and market-ing results, and quality levels of products and services. The types of information neededto make decisions in the formulation and execution of business strategies and objectivesare broad-based and encompass the entire organization. We may combine all these typesof essential information into one group and call it strategic information. \\nStrategic information is not for running the day-to-day operations of the business. It is\\nnot intended to produce an invoice, make a shipment, settle a claim, or post a withdrawalfrom a bank account. Strategic information is far more important for the continued healthand survival of the corporation. Critical business decisions depend on the availability ofproper strategic information in an enterprise. Figure 1-2 lists the desired characteristics ofstrategic information.\\nThe Information Crisis\\nY ou may be working in the Information Technology Department of a large conglomerate\\nor you may be part of a medium-sized company. Whatever the size of your company mayESCALATING NEED FOR STRATEGIC INFORMATION 3\\nINTEGRATED\\nDATA INTEGRITY\\nACCESSIBLE\\nCREDIBLETIMELYMust have a single, enterprise-wide view.\\nInformation must be accurate and must \\nconform to business rules.\\nEasily accessible with intuitive access \\npaths, and responsive for analysis.\\nEvery business factor must have one and \\nonly one value.\\nInformation must be available within the \\nstipulated time frame.\\nFigure 1-2 Characteristics of strategic information.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6d7996f6-d51d-443d-a39e-8b700317b4cc', embedding=None, metadata={'page_label': '28', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='be, think of all the various computer applications in your company. Think of all the data-\\nbases and the quantities of data that support the operations of your company. How manyyears’ worth of customer data is saved and available? How many years’ worth of financialdata is kept in storage? Ten years? Fifteen years? Where is all this data? On one platform?In legacy systems? In client/server applications?\\nWe are faced with two startling facts: (1) organizations have lots of data; (2) informa-\\ntion technology resources and systems are not effective at turning all that data into usefulstrategic information. Over the past two decades, companies have accumulated tons andtons of data about their operations. Mountains of data exist. Information is said to doubleevery 18 months. \\nIf we have such huge quantities of data in our organizations, why can’t our executives\\nand managers use this data for making strategic decisions? Lots and lots of informationexists. Why then do we talk about an information crisis? Most companies are faced withan information crisis not because of lack of sufficient data, but because the available datais not readily usable for strategic decision making. These large quantities of data are veryuseful and good for running the business operations, but hardly amenable for use in mak-ing decisions about business strategies and objectives. \\nWhy is this so? First, the data of an enterprise is spread across many types of incom-\\npatible structures and systems. Y our order processing system might have been developed20 years ago and is running on an old mainframe. Some of the data may still be on VSAMfiles. Y our later credit assignment and verification system might be on a client/server plat-form and the data for this application might be in relational tables. The data in a corpora-tion resides in various disparate systems, multiple platforms, and diverse structures. Themore technology your company has used in the past, the more disparate the data of yourcompany will be. But, for proper decision making on overall corporate strategies and ob-jectives, we need information integrated from all systems.\\nData needed for strategic decision making must be in a format suitable for analyzing\\ntrends. Executives and managers need to look at trends over time and steer their compa-nies in the proper direction. The tons of available operational data cannot be readily usedto spot trends. Operational data is event-driven. Y ou get snapshots of transactions thathappen at specific times. Y ou have data about units of sale of a single product in a specif-ic order on a given date to a certain customer. In the operational systems, you do not read-ily have the trends of a single product over the period of a month, a quarter, or a year. \\nFor strategic decision making, executives and managers must be able to review data\\nfrom different business viewpoints. For example, they must be able to review sales quanti-ties by product, salesperson, district, region, and customer groups. Can you think of oper-ational data being readily available for such analysis? Operational data is not directly suit-able for review from different viewpoints. \\nTechnology Trends\\nThose of us who have worked in the information technology field for two or three decades\\nhave witnessed the breathtaking changes that have taken place. First, the name of the com-puter department in an enterprise went from “data processing” to “management informa-tion systems,” then to “information systems,” and more recently to “information technolo-gy.” The entire spectrum of computing has undergone tremendous changes. The computingfocus itself has changed over the years. Old practices could not meet new needs. Screensand preformatted reports are no longer adequate to meet user requirements.4 THE COMPELLING NEED FOR DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7b7e3f1c-a1e9-4782-bc19-f2a2ff4e68ac', embedding=None, metadata={'page_label': '29', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Over the years, the price of MIPs is continuing to decline, digital storage is costing less\\nand less, and network bandwidth is increasing as its price decreases. Specifically, we haveseen explosive changes in these critical areas:\\n/L50539Computing technology\\n/L50539Human/machine interface\\n/L50539Processing options\\nFigure 1-3 illustrates these waves of explosive growth. \\nWhat is our current position in the technology revolution? Hardware economics and\\nminiaturization allow a workstation on every desk and provide increasing power at reduc-ing costs. New software provides easy-to-use systems. Open systems architecture createscooperation and enables usage of multivendor software. Improved connectivity, network-ing, and the Internet open up interaction with an enormous number of systems and data-bases. \\nAll of these improvements in technology are meritorious. These have made computing\\nfaster, cheaper, and widely available. But what is their relevance to the escalating need forstrategic information? Let us understand how the current state of the technology is con-ducive to providing strategic information.\\nProviding strategic information requires collection of large volumes of corporate data\\nand storing it in suitable formats. Technology advances in data storage and reduction instorage costs readily accommodate data storage needs for strategic decision-support sys-tems. Analysts, executives, and managers use strategic information interactively to ana-lyze and spot business trends. The user will ask a question and get the results, then ask an-other question, look at the results, and ask yet another question. This interactive processESCALATING NEED FOR STRATEGIC INFORMATION 5\\n1950 1960 1970 1980 1990 2000Computing Technology\\nProcessing OptionsHuman/Machine InterfaceMainframe Mini PCs/Networking Client/Server\\nPunch Card GUI Voice Video Display\\nBatchOnline Networked\\nFigure 1-3 Explosive growth of information technology.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f5efa925-538f-4577-8da1-252c1daf2d78', embedding=None, metadata={'page_label': '30', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='continues. Tremendous advances in interface software make such interactive analysis pos-\\nsible. Processing large volumes of data and providing interactive analysis requires extracomputing power. The explosive increase in computing power and its lower costs makeprovision of strategic information feasible. What we could not accomplish a few yearsearlier for providing strategic information is now possible with the current advanced stageof information technology.\\nOpportunities and Risks\\nWe have looked at the information crisis that exists in every enterprise and grasped that in\\nspite of lots of operational data in the enterprise, data suitable for strategic decision mak-ing is not available. Y et, the current state of the technology can make it possible to providestrategic information. While we are still discussing the escalating need for strategic infor-mation by companies, let us ask some basic questions. What are the opportunities avail-able to companies resulting from the possible use of strategic information? What are thethreats and risks resulting from the lack of strategic information available in companies?\\nHere are some examples of the opportunities made available to companies through the\\nuse of strategic information:\\n/L50539A business unit of a leading long-distance telephone carrier empowers its sales per-\\nsonnel to make better business decisions and thereby capture more business in ahighly competitive, multibillion-dollar market. A Web-accessible solution gathersinternal and external data to provide strategic information.\\n/L50539Availability of strategic information at one of the largest banks in the United States\\nwith assets in the $250 billion range allows users to make quick decisions to retaintheir valued customers. \\n/L50539In the case of a large health management organization, significant improvements in\\nhealth care programs are realized, resulting in a 22% decrease in emergency roomvisits, 29% decrease in hospital admissions for asthmatic children, potentially sight-saving screenings for hundreds of diabetics, improved vaccination rates, and morethan 100,000 performance reports created annually for physicians and pharmacists.\\n/L50539At one of the top five U.S. retailers, strategic information combined with Web-en-\\nabled analysis tools enables merchants to gain insights into their customer base,manage inventories more tightly, and keep the right products in front of the rightpeople at the right place at the right time.\\n/L50539A community-based pharmacy that competes on a national scale with more than\\n800 franchised pharmacies coast to coast gains in-depth understanding of what cus-tomers buy, resulting in reduced inventory levels, improved effectiveness of promo-tions and marketing campaigns, and improved profitability for the company. \\nOn the other hand, consider the following cases where risks and threats of failures ex-\\nisted before strategic information was made available for analysis and decision making:\\n/L50539With an average fleet of about 150,000 vehicles, a nationwide car rental company\\ncan easily get into the red at the bottom line if fleet management is not effective.The fleet is the biggest cost in that business. With intensified competition, the po-tential for failure is immense if the fleet is not managed effectively. Car idle time6 THE COMPELLING NEED FOR DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f94a632d-caa6-4345-837c-f9388bb37c93', embedding=None, metadata={'page_label': '31', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='must be kept to an absolute minimum. In attempting to accomplish this, failure to\\nhave the right class of car available in the right place at the right time, all washedand ready, can lead to serious loss of business.\\n/L50539For a world-leading supplier of systems and components to automobile and light\\ntruck equipment manufacturers, serious challenges faced included inconsistent datacomputations across nearly 100 plants, inability to benchmark quality metrics, andtime-consuming manual collection of data. Reports needed to support decisionmaking took weeks. It was never easy to get company-wide integrated information.\\n/L50539For a large utility company that provided electricity to about 25 million consumers\\nin five mid-Atlantic states in the United States, deregulation could result in a fewwinners and lots of losers. Remaining competitive and perhaps even surviving itselfdepended on centralizing strategic information from various sources, streamliningdata access, and facilitating analysis of the information by the business units.\\nFAILURES OF PAST DECISION-SUPPORT SYSTEMS\\nThe marketing department in your company has been concerned about the performance of\\nthe West Coast Region and the sales numbers from the monthly report this month aredrastically low. The marketing Vice President is agitated and wants to get some reportsfrom the IT department to analyze the performance over the past two years, product byproduct, and compared to monthly targets. He wants to make quick strategic decisions torectify the situation. The CIO wants your boss to deliver the reports as soon as possible.Y our boss runs to you and asks you to stop everything and work on the reports. There areno regular reports from any system to give the marketing department what they want. Y ouhave to gather the data from multiple applications and start from scratch. Does this soundfamiliar?\\nAt one time or another in your career in information technology, you must have been\\nexposed to situations like this. Sometimes, you may be able to get the information re-quired for such ad hoc reports from the databases or files of one application. Usually thisis not so. Y ou may have to go to several applications, perhaps running on different plat-forms in your company environment, to get the information. What happens next? Themarketing department likes the ad hoc reports you have produced. But now they wouldlike reports in a different form, containing more information that they did not think oforiginally. After the second round, they find that the contents of the reports are still not ex-actly what they wanted. They may also find inconsistencies among the data obtained fromdifferent applications.\\nThe fact is that for nearly two decades or more, IT departments have been attempting to\\nprovide information to key personnel in their companies for making strategic decisions.Sometimes an IT department could produce ad hoc reports from a single application. Inmost cases, the reports would need data from multiple systems, requiring the writing of ex-tract programs to create intermediary files that could be used to produce the ad hoc reports.\\nMost of these attempts by IT in the past ended in failure. The users could not clearly\\ndefine what they wanted in the first place. Once they saw the first set of reports, theywanted more data in different formats. The chain continued. This was mainly because ofthe very nature of the process of making strategic decisions. Information needed forstrategic decision making has to be available in an interactive manner. The user must beFAILURES OF PAST DECISION-SUPPORT SYSTEMS 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c2e31624-6150-4c50-a345-ad65d4e9d7fa', embedding=None, metadata={'page_label': '32', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='able to query online, get results, and query some more. The information must be in a for-\\nmat suitable for analysis. \\nIn order to appreciate the reasons for the failure of IT to provide strategic information\\nin the past, we need to consider how IT was attempting to do this all these years. Let us,therefore, quickly run through a brief history of decision support systems.\\nHistory of Decision-Support Systems\\nDepending on the size and nature of the business, most companies have gone through the\\nfollowing stages of attempts to provide strategic information for decision making. \\nAd Hoc Reports. This was the earliest stage. Users, especially from Marketing and\\nFinance, would send requests to IT for special reports. IT would write special programs,typically one for each request, and produce the ad hoc reports.\\nSpecial Extract Programs. This stage was an attempt by IT to anticipate somewhat\\nthe types of reports that would be requested from time to time. IT would write a suite ofprograms and run the programs periodically to extract data from the various applications.IT would create and keep the extract files to fulfill any requests for special reports. Forany reports that could not be run off the extracted files, IT would write individual specialprograms.\\nSmall Applications. In this stage, IT formalized the extract process. IT would create\\nsimple applications based on the extracted files. The users could stipulate the parametersfor each special report. The report printing programs would print the information based onuser-specific parameters. Some advanced applications would also allow users to view in-formation through online screens. \\nInformation Centers. In the early 1970s, some major corporations created what were\\ncalled information centers. The information center typically was a place where userscould go to request ad hoc reports or view special information on screens. These were pre-determined reports or screens. IT personnel were present at these information centers tohelp the users to obtain the desired information. \\nDecision-Support Systems. In this stage, companies began to build more sophisti-\\ncated systems intended to provide strategic information. Again, similar to the earlier at-tempts, these systems were supported by extracted files. The systems were menu-drivenand provided online information and also the ability to print special reports. Many of suchdecision-support systems were for marketing. \\nExecutive Information Systems. This was an attempt to bring strategic informa-\\ntion to the executive desktop. The main criteria were simplicity and ease of use. The sys-tem would display key information every day and provide ability to request simple,straightforward reports. However, only preprogrammed screens and reports were avail-able. After seeing the total countrywide sales, if the executive wanted to see the analysisby region, by product, or by another dimension, it was not possible unless such break-downs were already preprogrammed. This limitation caused frustration and executive in-formation systems did not last long in many companies.8 THE COMPELLING NEED FOR DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='baa065e2-4c28-4cc0-8895-e291b2b232cf', embedding=None, metadata={'page_label': '33', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Inability to Provide Information\\nEvery one of the past attempts at providing strategic information to decision makers was\\nunsatisfactory. Figure 1-4 depicts the inadequate attempts by IT to provide strategic infor-mation. As IT professionals, we are all familiar with the situation.\\nHere are some of the factors relating to the inability to provide strategic information:\\n/L50539IT receives too many ad hoc requests, resulting in a large overload. With limited re-\\nsources, IT is unable to respond to the numerous requests in a timely fashion.\\n/L50539Requests are not only too numerous, they also keep changing all the time. The users\\nneed more reports to expand and understand the earlier reports.\\n/L50539The users find that they get into the spiral of asking for more and more supplemen-\\ntary reports, so they sometimes adapt by asking for every possible combination,which only increases the IT load even further. \\n/L50539The users have to depend on IT to provide the information. They are not able to ac-\\ncess the information themselves interactively.\\n/L50539The information environment ideally suited for making strategic decision making\\nhas to be very flexible and conducive for analysis. IT has been unable to providesuch an environment. \\nOPERATIONAL VERSUS DECISION-SUPPORT SYSTEMS\\nWhat is a basic reason for the failure of all the previous attempts by IT to provide strategic\\ninformation? What has IT been doing all along? The fundamental reason for the inabilityto provide strategic information is that we have been trying all along to provide strategicinformation from the operational systems. These operational systems such as order pro-cessing, inventory control, claims processing, outpatient billing, and so on are not de-OPERATIONAL VERSUS DECISION-SUPPORT SYSTEMS 9\\nFigure 1-4 Inadequate attempts by IT to provide strategic information.THE FAMILIAR\\nMERRY-GO-ROUND\\n(4–6 weeks)User hopes\\nto find the\\nright\\nanswersUser needs\\ninformation\\nUser requests\\nreports from IT\\nIT places\\nrequest on\\nbacklog IT creates ad\\nhoc queriesIT sends\\nrequested\\nreports', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='735fd9a7-391d-46e2-83dc-968ab9de97dc', embedding=None, metadata={'page_label': '34', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='signed or intended to provide strategic information. If we need the ability to provide\\nstrategic information, we must get the information from altogether different types of sys-tems. Only specially designed decision support systems or informational systems can pro-vide strategic information. Let us understand why. \\nMaking the Wheels of Business Turn\\nOperational systems are online transaction processing (OLTP) systems. These are the sys-\\ntems that are used to run the day-to-day core business of the company. They are the so-called bread-and-butter systems. Operational systems make the wheels of business turn(see Figure 1-5). They support the basic business processes of the company. These sys-tems typically get the data into the database. Each transaction processes informationabout a single entity such as a single order, a single invoice, or a single customer.\\nWatching the Wheels of Business Turn\\nOn the other hand, specially designed and built decision-support systems are not meant to\\nrun the core business processes. They are used to watch how the business runs, and thenmake strategic decisions to improve the business (see Figure 1-6).\\nDecision-support systems are developed to get strategic information out of the data-\\nbase, as opposed to OLTP systems that are designed to put the data intothe database. De-\\ncision-support systems are developed to provide strategic information. \\nDifferent Scope, Different Purposes\\nTherefore, we find that in order to provide strategic information we need to build infor-\\nmational systems that are different from the operational systems we have been building torun the basic business. It will be worthless to continue to dip into the operational systemsfor strategic information as we have been doing in the past. As companies face fiercercompetition and businesses become more complex, continuing the past practices will onlylead to disaster. 10 THE COMPELLING NEED FOR DATA WAREHOUSING\\nFigure 1-5 Operational systems.Get the data in\\n/G4D/G61/G6B/G69/G6E/G67/G20/G74/G68/G65/G20/G77/G68/G65/G65/G6C/G73/G20/G6F/G66/G20/G62/G75/G73/G69/G6E/G65/G73/G73/G20/G74/G75/G72/G6E\\n/G75Take an order\\n/G75Process a claim\\n/G75Make a shipment\\n/G75Generate an invoice\\n/G75Receive cash\\n/G75Reserve an airline seat', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1c90bdb0-41bb-4f0f-b3c4-5b890a765025', embedding=None, metadata={'page_label': '35', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='We need to design and build informational systems\\n/L50539That serve different purposes\\n/L50539Whose scopes are different\\n/L50539Whose data content is different\\n/L50539Where the data usage patterns are different\\n/L50539Where the data access types are different\\nFigure 1-7 summarizes the differences between the traditional operational systems and\\nthe newer informational systems that need to be built.OPERATIONAL VERSUS DECISION-SUPPORT SYSTEMS 11\\nFigure 1-6 Decision-support systems.Get the information out\\n/G57/G61/G74/G63/G68/G69/G6E/G67/G20/G74/G68/G65/G20/G77/G68/G65/G65/G6C/G73/G20/G6F/G66/G20/G62/G75/G73/G69/G6E/G65/G73/G73/G20/G74/G75/G72/G6E\\n/G75Show me the top-selling products\\n/G75Show me the problem regions\\n/G75Tell me why (drill down)\\n/G75Let me see other data (drill across)\\n/G75Show the highest margins\\n/G75Alert me when a district sells below target\\nFigure 1-7 Operational and informational systems.How are they different?\\n.Current values                  \\nOptimized for \\ntransactions \\nHighRead, update, deletePredictable, repetitiveSub-secondsLarge numberArchived, derived, \\nsummarized\\nOptimized for complex \\nqueries\\nMedium to lowReadAd hoc, random, heuristicSeveral seconds to minutesRelatively small numberData Content\\nData StructureAccess Frequency\\nAccess TypeUsageResponse Time\\nUsersOPERATIONAL INFORMATIONAL', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9af280a8-e40e-44bf-92a4-200280ad0c88', embedding=None, metadata={'page_label': '36', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='DATA WAREHOUSING—THE ONLY VIABLE SOLUTION\\nAt this stage of our discussion, we now realize that we do need different types of decision-\\nsupport systems to provide strategic information. The type of information needed forstrategic decision making is different from that available from operational systems. Weneed a new type of system environment for the purpose of providing strategic informationfor analysis, discerning trends, and monitoring performance. \\nLet us examine the desirable features and processing requirements of this new type of\\nsystem environment. Let us also consider the advantages of this type of system environ-ment designed for strategic information.\\nA New Type of System Environment\\nThe desired features of the new type of system environment are:\\n/L50539Database designed for analytical tasks\\n/L50539Data from multiple applications\\n/L50539Easy to use and conducive to long interactive sessions by users\\n/L50539Read-intensive data usage\\n/L50539Direct interaction with the system by the users without IT assistance \\n/L50539Content updated periodically and stable\\n/L50539Content to include current and historical data\\n/L50539Ability for users to run queries and get results online\\n/L50539Ability for users to initiate reports\\nProcessing Requirements in the New Environment\\nMost of the processing in the new environment for strategic information will have to be\\nanalytical. There are four levels of analytical processing requirements:\\n1. Running of simple queries and reports against current and historical data\\n2. Ability to perform “what if ” analysis is many different ways3. Ability to query, step back, analyze, and then continue the process to any desired\\nlength\\n4. Spot historical trends and apply them for future results \\nBusiness Intelligence at the Data Warehouse\\nThis new system environment that users desperately need to obtain strategic information\\nhappens to be the new paradigm of data warehousing. Enterprises that are building datawarehouses are actually building this new system environment. This new environment iskept separate from the system environment supporting the day-to-day operations. The datawarehouse essentially holds the business intelligence for the enterprise to enable strategicdecision making. The data warehouse is the only viable solution. We have clearly seen thatsolutions based on the data extracted from operational systems are all totally unsatisfacto-ry. Figure 1-8 shows the nature of business intelligence at the data warehouse.12 THE COMPELLING NEED FOR DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1d91f4f1-a824-4a40-baab-e2df216d35cd', embedding=None, metadata={'page_label': '37', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='At a high level of interpretation, the data warehouse contains critical measurements of\\nthe business processes stored along business dimensions. For example, a data warehousemight contain units of sales, by product, day, customer group, sales district, sales region,and promotion. Here the business dimensions are product, day, customer group, sales dis-trict, sales region, and promotion. \\nFrom where does the data warehouse get its data? The data is derived from the opera-\\ntional systems that support the basic business processes of the organization. In betweenthe operational systems and the data warehouse, there is a data staging area. In this stag-ing area, the operational data is cleansed and transformed into a form suitable for place-ment in the data warehouse for easy retrieval.\\nDATA WAREHOUSE DEFINED\\nWe have reached the strong conclusion that data warehousing is the only viable solution\\nfor providing strategic information. We arrived at this conclusion based on the functionsof the new system environment called the data warehouse. So, let us try to come up with afunctional definition of the data warehouse. \\nThe data warehouse is an informational environment that\\n/L50539Provides an integrated and total view of the enterprise\\n/L50539Makes the enterprise’ s current and historical information easily available for deci-\\nsion making\\n/L50539Makes decision-support transactions possible without hindering operational sys-\\ntems\\n/L50539Renders the organization’ s information consistent\\n/L50539Presents a flexible and interactive source of strategic informationDATA WAREHOUSE DEFINED 13\\nOPERATIONAL\\nSYSTEMS\\nData TransformationBasic\\nbusiness\\nprocessesExtraction,\\ncleansing,\\naggregation\\nKey measurements,\\nbusiness dimensions\\nFigure 1-8 Business intelligence at the data warehouse.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b6ff8211-780c-4765-8f12-c07918d96be6', embedding=None, metadata={'page_label': '38', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Simple Concept for Information Delivery\\nIn the final analysis, data warehousing is a simple concept. It is born out of the need for\\nstrategic information and is the result of the search for a new way to provide such infor-mation. The methods of the last two decades using the operational computing environ-ment, were unsatisfactory. The new concept is not to generate fresh data, but to make useof the large volumes of existing data and to transform it into forms suitable for providingstrategic information. \\nThe data warehouse exists to answer questions users have about the business, the per-\\nformance of the various operations, the business trends, and about what can be done toimprove the business. The data warehouse exists to provide business users with direct ac-cess to data, to provide a single unified version of the performance indicators, to recordthe past accurately, and to provide the ability to view the data from many different per-spectives. In short, the data warehouse is there to support decisional processes. \\nData warehousing is really a simple concept: Take all the data you already have in the\\norganization, clean and transform it, and then provide useful strategic information. Whatcould be simpler than that? \\nAn Environment, Not a Product\\nA data warehouse is not a single software or hardware product you purchase to provide\\nstrategic information. It is, rather, a computing environment where users can find strategicinformation, an environment where users are put directly in touch with the data they needto make better decisions. It is a user-centric environment.\\nLet us summarize the characteristics of this new computing environment called the\\ndata warehouse: \\n/L50539An ideal environment for data analysis and decision support\\n/L50539Fluid, flexible, and interactive\\n/L50539100 percent user-driven\\n/L50539Very responsive and conducive to the ask–answer–ask–again pattern\\n/L50539Provides the ability to discover answers to complex, unpredictable questions \\nA Blend of Many Technologies\\nLet us reexamine the basic concept of data warehousing. The basic concept of data ware-\\nhousing is: \\n/L50539Take all the data from the operational systems\\n/L50539Where necessary, include relevant data from outside, such as industry benchmark\\nindicators\\n/L50539Integrate all the data from the various sources\\n/L50539Remove inconsistencies and transform the data\\n/L50539Store the data in formats suitable for easy access for decision making\\nAlthough a simple concept, it involves different functions: data extraction, the function\\nof loading the data, transforming the data, storing the data, and providing user interfaces.14 THE COMPELLING NEED FOR DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a2b5155d-5f27-4efc-951d-c5c0ce3ab44d', embedding=None, metadata={'page_label': '39', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Different technologies are, therefore, needed to support these functions. Figure 1-9 shows\\nhow data warehouse is a blend of many technologies needed for the various functions.\\nAlthough many technologies are in use, they all work together in a data warehouse.\\nThe end result is the creation of a new computing environment for the purpose of provid-ing the strategic information every enterprise needs desperately. There are several vendortools available in each of these technologies. Y ou do not have to build your data warehousefrom scratch. \\nCHAPTER SUMMARY\\n/L50539Companies are desperate for strategic information to counter fiercer competition,\\nextend market share, and improve profitability.\\n/L50539In spite of tons of data accumulated by enterprises over the past decades, every en-\\nterprise is caught in the middle of an information crisis. Information needed forstrategic decision making is not readily available.\\n/L50539All the past attempts by IT to provide strategic information have been failures. This\\nwas mainly because IT has been trying to provide strategic information from opera-tional systems. \\n/L50539Informational systems are different from the traditional operational systems. Opera-\\ntional systems are not designed for strategic information. \\n/L50539We need a new type of computing environment to provide strategic information.\\nThe data warehouse promises to be this new computing environment.CHAPTER SUMMARY 15\\nOPERATIONAL\\nSYSTEMS\\nDATA\\nWAREHOUSEData TransformationBasic\\nbusiness\\nprocessesExtraction,\\ncleansing,\\naggregationKey measurements,\\nbusiness dimensions\\nExecutives/Managers/\\nAnalysts\\nData ModelingData\\nAcquisition\\nData Quality\\nData\\nManagement -Metadata\\nManagementAnalysis\\nAdministration\\nDevelopment\\nToolsAApplications\\nStorage\\nManagementBLEND  OF  TECHNOLOGIES\\nFigure 1-9   The data warehouse: a blend of technologies.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0258a994-6194-4cac-8dd6-91f9fbcbe996', embedding=None, metadata={'page_label': '40', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Data warehousing is the viable solution. There is a compelling need for data ware-\\nhousing for every enterprise.\\nREVIEW QUESTIONS\\n1. What do we mean by strategic information? For a commercial bank, name five\\ntypes of strategic objectives. \\n2. Do you agree that a typical retail store collects huge volumes of data through its\\noperational systems? Name three types of transaction data likely to be collectedby a retail store in large volumes during its daily operations.\\n3. Examine the opportunities that can be provided by strategic information for a\\nmedical center. Can you list five such opportunities?\\n4. Why were all the past attempts by IT to provide strategic information failures? List\\nthree concrete reasons and explain.\\n5. Describe five differences between operational systems and informational systems.6. Why are operational systems not suitable for providing strategic information?\\nGive three specific reasons and explain.\\n7. Name six characteristics of the computing environment needed to provide strate-\\ngic information.\\n8. What types of processing take place in a data warehouse? Describe.9. A data warehouse in an environment, not a product. Discuss.\\n10. Data warehousing is the only viable means to resolve the information crisis and to\\nprovide strategic information. List four reasons to support this assertion and ex-plain them.\\nEXERCISES\\n1. Match the columns:\\n1. information crisis A. OLTP application\\n2. strategic information B. produce ad hoc reports3. operational systems C. explosive growth4. information center D. despite lots of data5. data warehouse E. data cleaned and transformed6. order processing F . users go to get information7. executive information system G. used for decision making8. data staging area H. environment, not product 9. extract programs I. for day-to-day operations \\n10. information technology J. simple, easy to use\\n2. The current trends in hardware/software technology make data warehousing feasi-\\nble. Explain via some examples how exactly technology trends do help. 16 THE COMPELLING NEED FOR DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='16939340-d1a5-4bc3-a5d7-17c33ae8f569', embedding=None, metadata={'page_label': '41', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. Y ou are the IT Director of a nationwide insurance company. Write a memo to the\\nExecutive Vice President explaining the types of opportunities that can be realizedwith readily available strategic information.\\n4. For an airlines company, how can strategic information increase the number of fre-\\nquent flyers? Discuss giving specific details.\\n5. Y ou are a Senior Analyst in the IT department of a company manufacturing auto-\\nmobile parts. The marketing VP is complaining about the poor response by IT inproviding strategic information. Draft a proposal to him explaining the reasons forthe problems and why a data warehouse would be the only viable solution. EXERCISES 17', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f3ea6b18-5e6f-442b-957e-6e617ecfa352', embedding=None, metadata={'page_label': '42', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 2\\nDATA WAREHOUSE: \\nTHE BUILDING BLOCKS\\nCHAPTER OBJECTIVES\\n/L50539Review formal definitions of a data warehouse\\n/L50539Discuss the defining features\\n/L50539Distinguish between data warehouses and data marts\\n/L50539Study each component or building block that makes up a data warehouse\\n/L50539Introduce metadata and highlight its significance\\nAs we have seen in the last chapter, the data warehouse is an information delivery system.\\nIn this system, you integrate and transform enterprise data into information suitable forstrategic decision making. Y ou take all the historic data from the various operational sys-tems, combine this internal data with any relevant data from outside sources, and pullthem together. Y ou resolve any conflicts in the way data resides in different systems andtransform the integrated data content into a format suitable for providing information tothe various classes of users. Finally, you implement the information delivery methods. \\nIn order to set up this information delivery system, you need different components or\\nbuilding blocks. These building blocks are arranged together in the most optimal way toserve the intended purpose. They are arranged in a suitable architecture. Before we getinto the individual components and their arrangement in the overall architecture, let usfirst look at some fundamental features of the data warehouse.\\nBill Inmon, considered to be the father of Data Warehousing provides the following de-\\nfinition: “A Data Warehouse is a subject oriented, integrated, nonvolatile, and time variantcollection of data in support of management’ s decisions.”\\nSean Kelly, another leading data warehousing practitioner defines the data warehouse\\nin the following way. The data in the data warehouse is:\\nSeparate\\nAvailable\\n19Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2eede961-829e-476e-b5de-662bccd2a095', embedding=None, metadata={'page_label': '43', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Integrated\\nTime stampedSubject orientedNonvolatileAccessible\\nDEFINING FEATURES\\nLet us examine some of the key defining features of the data warehouse based on these\\ndefinitions. What about the nature of the data in the data warehouse? How is this data dif-ferent from the data in any operational system? Why does it have to be different? How isthe data content in the data warehouse used? \\nSubject-Oriented Data\\nIn operational systems, we store data by individual applications. In the data sets for an or-\\nder processing application, we keep the data for that particular application. These data setsprovide the data for all the functions for entering orders, checking stock, verifying cus-tomer’ s credit, and assigning the order for shipment. But these data sets contain only thedata that is needed for those functions relating to this particular application. We will havesome data sets containing data about individual orders, customers, stock status, and de-tailed transactions, but all of these are structured around the processing of orders.\\nSimilarly, for a banking institution, data sets for a consumer loans application contain\\ndata for that particular application. Data sets for other distinct applications of checkingaccounts and savings accounts relate to those specific applications. Again, in an insurancecompany, different data sets support individual applications such as automobile insurance,life insurance, and workers’ compensation insurance.\\nIn every industry, data sets are organized around individual applications to support\\nthose particular operational systems. These individual data sets have to provide data forthe specific applications to perform the specific functions efficiently. Therefore, the datasets for each application need to be organized around that specific application.\\nIn striking contrast, in the data warehouse, data is stored by subjects, not by applica-\\ntions. If data is stored by business subjects, what are business subjects? Business subjectsdiffer from enterprise to enterprise. These are the subjects critical for the enterprise. For amanufacturing company, sales, shipments, and inventory are critical business subjects.For a retail store, sales at the check-out counter is a critical subject.\\nFigure 2-1 distinguishes between how data is stored in operational systems and in the\\ndata warehouse. In the operational systems shown, data for each application is organizedseparately by application: order processing, consumer loans, customer billing, accountsreceivable, claims processing, and savings accounts. For example, Claims is a critical\\nbusiness subject for an insurance company. Claims under automobile insurance policiesare processed in the Auto Insurance application. Claims data for automobile insurance isorganized in that application. Similarly, claims data for workers’ compensation insuranceis organized in the Workers’ Comp Insurance application. But in the data warehouse for aninsurance company, claims data are organized around the subject of claims and not by in-dividual applications of Auto Insurance and Workers’ Comp.20 DATA WAREHOUSE: THE BUILDING BLOCKS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='501e84bd-f446-4a86-870b-9a988b94119a', embedding=None, metadata={'page_label': '44', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='In a data warehouse, there is no application flavor. The data in a data warehouse cut\\nacross applications. \\nIntegrated Data\\nFor proper decision making, you need to pull together all the relevant data from the vari-\\nous applications. The data in the data warehouse comes from several operational systems.Source data are in different databases, files, and data segments. These are disparate appli-cations, so the operational platforms and operating systems could be different. The filelayouts, character code representations, and field naming conventions all could be differ-ent.\\nIn addition to data from internal operational systems, for many enterprises, data from\\noutside sources is likely to be very important. Companies such as Metro Mail, A. C.Nielsen, and IRI specialize in providing vital data on a regular basis. Y our data warehousemay need data from such sources. This is one more variation in the mix of source data fora data warehouse. \\nFigure 2-2 illustrates a simple process of data integration for a banking institution.\\nHere the data fed into the subject area of account in the data warehouse comes from three\\ndifferent operational applications. Even within just three applications, there could be sev-eral variations. Naming conventions could be different; attributes for data items could bedifferent. The account number in the Savings Account application could be eight byteslong, but only six bytes in the Checking Account application. \\nBefore the data from various disparate sources can be usefully stored in a data ware-\\nhouse, you have to remove the inconsistencies. Y ou have to standardize the various data el-ements and make sure of the meanings of data names in each source application. Beforemoving the data into the data warehouse, you have to go through a process of transforma-tion, consolidation, and integration of the source data.DEFINING FEATURES 21\\nIn the data warehouse, data is not stored by operational\\napplications, but by business subjects.\\nOrder\\nProcessing\\nConsumer\\nLoans\\nCustomer\\nBilling\\nAccounts\\nReceivable\\nClaims\\nProcessing\\nSavings\\nAccountsSales Product\\nClaims PolicyAccountCustomerOperational Applications Data Warehouse Subjects\\nFigure 2-1 The data warehouse is subject oriented.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='db5d04e8-837b-49b3-a169-2399b6e685ad', embedding=None, metadata={'page_label': '45', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Here are some of the items that would need standardization:\\n/L50539Naming conventions\\n/L50539Codes\\n/L50539Data attributes\\n/L50539Measurements \\nTime-Variant Data\\nFor an operational system, the stored data contains the current values. In an accounts re-\\nceivable system, the balance is the current outstanding balance in the customer’ s account.In an order entry system, the status of an order is the current status of the order. In a con-sumer loans application, the balance amount owed by the customer is the current amount.Of course, we store some past transactions in operational systems, but, essentially, opera-tional systems reflect current information because these systems support day-to-day cur-rent operations.\\nOn the other hand, the data in the data warehouse is meant for analysis and decision\\nmaking. If a user is looking at the buying pattern of a specific customer, the user needsdata not only about the current purchase, but on the past purchases as well. When a userwants to find out the reason for the drop in sales in the North East division, the user needsall the sales data for that division over a period extending back in time. When an analyst ina grocery chain wants to promote two or more products together, that analyst wants salesof the selected products over a number of past quarters. \\nA data warehouse, because of the very nature of its purpose, has to contain historical\\ndata, not just current values. Data is stored as snapshots over past and current periods.Every data structure in the data warehouse contains the time element. Y ou will find histor-22 DATA WAREHOUSE: THE BUILDING BLOCKS\\nData inconsistencies are removed; data from diverse operational \\napplications is integrated.\\nSavings \\nAccount\\nLoans \\nAccountChecking \\nAccount\\nDATA   FROM   APPLICATIONSDATA WAREHOUSE SUBJECTS\\nSubject     \\n= Account\\nFigure 2-2 The data warehouse is integrated.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fc23b8df-1634-4c7d-8a97-988b44ec7e4b', embedding=None, metadata={'page_label': '46', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ical snapshots of the operational data in the data warehouse. This aspect of the data ware-\\nhouse is quite significant for both the design and the implementation phases. \\nFor example, in a data warehouse containing units of sale, the quantity stored in each\\nfile record or table row relates to a specific time element. Depending on the level of thedetails in the data warehouse, the sales quantity in a record may relate to a specific date,week, month, or quarter. \\nThe time-variant nature of the data in a data warehouse\\n/L50539Allows for analysis of the past\\n/L50539Relates information to the present \\n/L50539Enables forecasts for the future\\nNonvolatile Data\\nData extracted from the various operational systems and pertinent data obtained from\\noutside sources are transformed, integrated, and stored in the data warehouse. The datain the data warehouse is not intended to run the day-to-day business. When you want toprocess the next order received from a customer, you do not look into the data ware-house to find the current stock status. The operational order entry application is meantfor that purpose. In the data warehouse, you keep the extracted stock status data as snap-shots over time. Y ou do not update the data warehouse every time you process a singleorder.\\nData from the operational systems are moved into the data warehouse at specific inter-\\nvals. Depending on the requirements of the business, these data movements take placetwice a day, once a day, once a week, or once in two weeks. In fact, in a typical data ware-house, data movements to different data sets may take place at different frequencies. Thechanges to the attributes of the products may be moved once a week. Any revisions to ge-ographical setup may be moved once a month. The units of sales may be moved once aday. Y ou plan and schedule the data movements or data loads based on the requirements ofyour users.\\nAs illustrated in Figure 2-3, every business transaction does not update the data in the\\ndata warehouse. The business transactions update the operational system databases in realtime. We add, change, or delete data from an operational system as each transaction hap-pens but do not usually update the data in the data warehouse. Y ou do not delete the datain the data warehouse in real time. Once the data is captured in the data warehouse, you donot run individual transactions to change the data there. Data updates are commonplace inan operational database; not so in a data warehouse. The data in a data warehouse is not asvolatile as the data in an operational database is. The data in a data warehouse is primarilyfor query and analysis. \\nData Granularity\\nIn an operational system, data is usually kept at the lowest level of detail. In a point-of-\\nsale system for a grocery store, the units of sale are captured and stored at the level ofunits of a product per transaction at the check-out counter. In an order entry system, thequantity ordered is captured and stored at the level of units of a product per order receivedfrom the customer. Whenever you need summary data, you add up the individual transac-DEFINING FEATURES 23', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='397e0a89-9aac-4ac2-a81a-ab1c05175f24', embedding=None, metadata={'page_label': '47', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='tions. If you are looking for units of a product ordered this month, you read all the orders\\nentered for the entire month for that product and add up. Y ou do not usually keep summa-ry data in an operational system. \\nWhen a user queries the data warehouse for analysis, he or she usually starts by look-\\ning at summary data. The user may start with total sale units of a product in an entire re-gion. Then the user may want to look at the breakdown by states in the region. The nextstep may be the examination of sale units by the next level of individual stores. Frequent-ly, the analysis begins at a high level and moves down to lower levels of detail.\\nIn a data warehouse, therefore, you find it efficient to keep data summarized at differ-\\nent levels. Depending on the query, you can then go to the particular level of detail andsatisfy the query. Data granularity in a data warehouse refers to the level of detail. Thelower the level of detail, the finer the data granularity. Of course, if you want to keep datain the lowest level of detail, you have to store a lot of data in the data warehouse. Y ou willhave to decide on the granularity levels based on the data types and the expected systemperformance for queries. Figure 2-4 shows examples of data granularity in a typical datawarehouse.\\nDATA WAREHOUSES AND DATA MARTS\\nIf you have been following the literature on data warehouses for the past few years, you\\nwould, no doubt, have come across the terms “data warehouse” and “data mart.” Manywho are new to this paradigm are confused about these terms. Some authors and vendorsuse the two terms synonymously. Some make distinctions that are not clear enough. Atthis point, it would be worthwhile for us to examine these two terms and take our position. \\nWriting in a leading trade magazine in 1998, Bill Inmon stated, “The single most im-\\nportant issue facing the IT manager this year is whether to build the data warehouse first24 DATA WAREHOUSE: THE BUILDING BLOCKS\\nUsually the data in the data warehouse is not updated or\\ndeleted.\\nOperational System Applications Decision Support  SystemsOLTP\\nDATABASESDATA\\nWAREHOUSE\\nRead Add / Change / Delete ReadLOADS\\nFigure 2-3 The data warehouse is nonvolatile.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='27e4ba9a-9869-4a54-a00d-08f9833dc288', embedding=None, metadata={'page_label': '48', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='or the data mart first.” This statement is true even today. Let us examine this statement and\\ntake a stand.\\nBefore deciding to build a data warehouse for your organization, you need to ask the\\nfollowing basic and fundamental questions and address the relevant issues:\\n/L50539Top-down or bottom-up approach?\\n/L50539Enterprise-wide or departmental?\\n/L50539Which first—data warehouse or data mart?\\n/L50539Build pilot or go with a full-fledged implementation?\\n/L50539Dependent or independent data marts? \\nThese are critical issues requiring careful examination and planning.\\nShould you look at the big picture of your organization, take a top-down approach, and\\nbuild a mammoth data warehouse? Or, should you adopt a bottom-up approach, look atthe individual local and departmental requirements, and build bite-size departmental datamarts?\\nShould you build a large data warehouse and then let that repository feed data into lo-\\ncal, departmental data marts? On the other hand, should you build individual local datamarts, and combine them to form your overall data warehouse? Should these local datamarts be independent of one another? Or, should they be dependent on the overall datawarehouse for data feed? Should you build a pilot data mart? These are crucial questions.\\nHow are They Different?\\nLet us take a close look at Figure 2-5. Here are the two different basic approaches: (1)\\noverall data warehouse feeding dependent data marts, and (2) several departmental or lo-DATA WAREHOUSES AND DATA MARTS 25\\nData granularity refers to the level of detail. Depending on the\\nrequirements, multiple levels of detail may be present. Many data \\nwarehouses have at least dual levels of granularity.Daily Detail\\nAccount\\nActivity DateAmountDeposit/WithdrawalMonthly Summary\\nAccount\\nMonthNumber of transactionsWithdrawalsDepositsBeginning BalanceEnding BalanceQuarterly Summary\\nAccount\\nMonthNumber of transactionsWithdrawalsDepositsBeginning BalanceEnding BalanceTHREE DATA LEVELS IN A BANKING DATA WAREHOUSE\\nFigure 2-4 Data granularity.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='283034d5-106f-4a10-9d9a-d2e40676b624', embedding=None, metadata={'page_label': '49', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='cal data marts combining into a data warehouse. In the first approach, you extract data\\nfrom the operational systems; you then transform, clean, integrate, and keep the data inthe data warehouse. So, which approach is best in your case, the top-down or the bottom-up approach? Let us examine these two approaches carefully.\\nTop-Down Versus Bottom-Up Approach\\nTop-Down Approach\\nThe advantages of this approach are:\\n/L50539A truly corporate effort, an enterprise view of data\\n/L50539Inherently architected—not a union of disparate data marts\\n/L50539Single, central storage of data about the content\\n/L50539Centralized rules and control\\n/L50539May see quick results if implemented with iterations\\nThe disadvantages are:\\n/L50539Takes longer to build even with an iterative method\\n/L50539High exposure/risk to failure\\n/L50539Needs high level of cross-functional skills\\n/L50539High outlay without proof of concept \\nThis is the big-picture approach in which you build the overall, big, enterprise-wide\\ndata warehouse. Here you do not have a collection of fragmented islands of information.The data warehouse is large and integrated. This approach, however, would take longer tobuild and has a high risk of failure. If you do not have experienced professionals on yourteam, this approach could be dangerous. Also, it will be difficult to sell this approach tosenior management and sponsors. They are not likely to see results soon enough.26 DATA WAREHOUSE: THE BUILDING BLOCKS\\nDATA WAREHOUSE\\n/G75Corporate/Enterprise-wide\\n/G75Union of all data marts\\n/G75Data received from staging area\\n/G75Queries on presentation resource\\n/G75Structure for corporate view of \\ndata\\n/G75Organized on E-R modelDATA MART\\n/G75Departmental\\n/G75A single business process\\n/G75Star-join (facts & dimensions) \\n/G75Technology optimal for data \\naccess and analysis\\n/G75Structure to suit the \\ndepartmental view of data\\nFigure 2-5 Data warehouse versus data mart.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='51c3cd96-9617-4af6-a4d1-2c601c7638d4', embedding=None, metadata={'page_label': '50', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Bottom-Up Approach\\nThe advantages of this approach are:\\n/L50539Faster and easier implementation of manageable pieces\\n/L50539Favorable return on investment and proof of concept\\n/L50539Less risk of failure\\n/L50539Inherently incremental; can schedule important data marts first \\n/L50539Allows project team to learn and grow\\nThe disadvantages are:\\n/L50539Each data mart has its own narrow view of data\\n/L50539Permeates redundant data in every data mart \\n/L50539Perpetuates inconsistent and irreconcilable data \\n/L50539Proliferates unmanageable interfaces \\nIn this bottom-up approach, you build your departmental data marts one by one. Y ou\\nwould set a priority scheme to determine which data marts you must build first. The mostsevere drawback of this approach is data fragmentation. Each independent data mart willbe blind to the overall requirements of the entire organization. \\nA Practical Approach\\nIn order to formulate an approach for your organization, you need to examine what exact-\\nly your organization wants. Is your organization looking for long-term results or fast datamarts for only a few subjects for now? Does your organization want quick, proof-of-con-cept, throw-away implementations? Or, do you want to look into some other practical ap-proach?\\nAlthough both the top-down and the bottom-up approaches each have their own advan-\\ntages and drawbacks, a compromise approach accommodating both views appears to bepractical. The chief proponent of this practical approach is Ralph Kimball, an eminent au-thor and data warehouse expert. The steps in this practical approach are as follows:\\n1. Plan and define requirements at the overall corporate level\\n2. Create a surrounding architecture for a complete warehouse3. Conform and standardize the data content4. Implement the data warehouse as a series of supermarts, one at a time\\nIn this practical approach, you go to the basics and determine what exactly your orga-\\nnization wants in the long term. The key to this approach is that you first plan at the enter-prise level. Y ou gather requirements at the overall level. Y ou establish the architecture forthe complete warehouse. Then you determine the data content for each supermart. Super-marts are carefully architected data marts. Y ou implement these supermarts, one at a time.Before implementation, you make sure that the data content among the various super-marts are conformed in terms of data types, field lengths, precision, and semantics. A cer-tain data element must mean the same thing in every supermart. This will avoid spread ofdisparate data across several data marts.DATA WAREHOUSES AND DATA MARTS 27', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f126a185-8274-4776-805b-4bc6099671e4', embedding=None, metadata={'page_label': '51', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A data mart, in this practical approach, is a logical subset of the complete data ware-\\nhouse, a sort of pie-wedge of the whole data warehouse. A data warehouse, therefore, is aconformed union of all data marts. Individual data marts are targeted to particular busi-ness groups in the enterprise, but the collection of all the data marts form an integratedwhole, called the enterprise data warehouse. \\nWhen we refer to data warehouses and data marts in our discussions here, we use the\\nmeanings as understood in this practical approach. For us, a data warehouse means a col-lection of the constituent data marts. \\nOVERVIEW OF THE COMPONENTS\\nWe have now reviewed the basic definitions and features of data warehouses and data marts\\nand completed a significant discussion of them. We have established our position on whatthe term data warehouse means to us. Now we are ready to examine its components.\\nWhen we build an operational system such as order entry, claims processing, or sav-\\nings account, we put together several components to make up the system. The front-endcomponent consists of the GUI (graphical user interface) to interface with the users fordata input. The data storage component includes the database management system, suchas Oracle, Informix, or Microsoft SQL Server. The display component is the set of screensand reports for the users. The data interfaces and the network software form the connec-tivity component. Depending on the information requirements and the framework of ourorganization, we arrange these components in the most optimum way. \\nArchitecture is the proper arrangement of the components. Y ou build a data warehouse\\nwith software and hardware components. To suit the requirements of your organizationyou arrange these building blocks in a certain way for maximum benefit. Y ou may want tolay special emphasis on one component; you may want to bolster up another componentwith extra tools and services. All of this depends on your circumstances.\\nFigure 2-6 shows the basic components of a typical warehouse. Y ou see the Source\\nData component shown on the left. The Data Staging component serves as the next build-\\ning block. In the middle, you see the Data Storage component that manages the data ware-\\nhouse data. This component not only stores and manages the data, it also keeps track ofthe data by means of the metadata repository. The Information Delivery component shown\\non the right consists of all the different ways of making the information from the datawarehouse available to the users. \\nWhether you build a data warehouse for a large manufacturing company on the For-\\ntune 500 list, a leading grocery chain with stores all over the country, or a global bankinginstitution, the basic components are the same. Each data warehouse is put together withthe same building blocks. The essential difference for each organization is in the waythese building blocks are arranged. The variation is in the manner in which some of theblocks are made stronger than others in the architecture. \\nWe will now take a closer look at each of the components. At this stage, we want to\\nknow what the components are and how each fits into the architecture. We also want to re-view specific issues relating to each particular component. \\nSource Data Component\\nSource data coming into the data warehouse may be grouped into four broad categories,\\nas discussed here.28 DATA WAREHOUSE: THE BUILDING BLOCKS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6afa3491-03dd-4007-af16-8ca27886a718', embedding=None, metadata={'page_label': '52', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Production Data. This category of data comes from the various operational systems of\\nthe enterprise. Based on the information requirements in the data warehouse, you choosesegments of data from the different operational systems. While dealing with this data, youcome across many variations in the data formats. Y ou also notice that the data resides ondifferent hardware platforms. Further, the data is supported by different database systemsand operating systems. This is data from many vertical applications.\\nIn operational systems, information queries are narrow. Y ou query an operational sys-\\ntem for information about specific instances of business objects. Y ou may want just thename and address of a single customer. Or, you may need the orders placed by a singlecustomer in a single week. Or, you may just need to look at a single invoice and the itemsbilled on that single invoice. In operational systems, you do not have broad queries. Y oudo not query the operational system in unexpected ways. The queries are all predictable.Again, you do not expect a particular query to run across different operational systems.What does all of this mean? Simply this: there is no conformance of data among the vari-ous operational systems of an enterprise. A term like an account may have different\\nmeanings in different systems.\\nThe significant and disturbing characteristic of production data is disparity. Y our great\\nchallenge is to standardize and transform the disparate data from the various productionsystems, convert the data, and integrate the pieces into useful data for storage in the datawarehouse. \\nInternal Data. In every organization, users keep their “private” spreadsheets, docu-\\nments, customer profiles, and sometimes even departmental databases. This is the internaldata, parts of which could be useful in a data warehouse.OVERVIEW OF THE COMPONENTS 29\\nFigure 2-6 Data warehouse: building blocks or components.Architecture is the proper arrangement of the components.\\nData Warehouse\\nDBMS\\nData MartsMulti -\\ndimensional\\nDBsExternal\\nReport/QueryOLAPData MiningSource Data\\nData StagingData StorageMetadataManagement & ControlInformation DeliveryArchived  Internal    Production', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bd7a1bb3-2657-42c2-8e95-706735c7db94', embedding=None, metadata={'page_label': '53', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='If your organization does business with the customers on a one-to-one basis and the\\ncontribution of each customer to the bottom line is significant, then detailed customerprofiles with ample demographics are important in a data warehouse. Profiles of individ-ual customers become very important for consideration. When your account representa-tives talk to their assigned customers or when your marketing department wants to makespecific offerings to individual customers, you need the details. Although much of thisdata may be extracted from production systems, a lot of it is held by individuals and de-partments in their private files.\\nY ou cannot ignore the internal data held in private files in your organization. It is a col-\\nlective judgment call on how much of the internal data should be included in the datawarehouse. The IT department must work with the user departments to gather the internaldata. \\nInternal data adds additional complexity to the process of transforming and integrating\\nthe data before it can be stored in the data warehouse. Y ou have to determine strategies forcollecting data from spreadsheets, find ways of taking data from textual documents, andtie into departmental databases to gather pertinent data from those sources. Again, youmay want to schedule the acquisition of internal data. Initially, you may want to limityourself to only some significant portions before going live with your first data mart. \\nArchived Data. Operational systems are primarily intended to run the current business.\\nIn every operational system, you periodically take the old data and store it in archivedfiles. The circumstances in your organization dictate how often and which portions of theoperational databases are archived for storage. Some data is archived after a year. Some-times data is left in the operational system databases for as long as five years.\\nMany different methods of archiving exist. There are staged archival methods. At the\\nfirst stage, recent data is archived to a separate archival database that may still be online.At the second stage, the older data is archived to flat files on disk storage. At the nextstage, the oldest data is archived to tape cartridges or microfilm and even kept off-site.\\nAs mentioned earlier, a data warehouse keeps historical snapshots of data. Y ou essen-\\ntially need historical data for analysis over time. For getting historical information, youlook into your archived data sets. Depending on your data warehouse requirements, youhave to include sufficient historical data. This type of data is useful for discerning patternsand analyzing trends.\\nExternal Data. Most executives depend on data from external sources for a high per-\\ncentage of the information they use. They use statistics relating to their industry producedby external agencies. They use market share data of competitors. They use standard valuesof financial indicators for their business to check on their performance. \\nFor example, the data warehouse of a car rental company contains data on the current\\nproduction schedules of the leading automobile manufacturers. This external data in thedata warehouse helps the car rental company plan for their fleet management.\\nThe purposes served by such external data sources cannot be fulfilled by the data avail-\\nable within your organization itself. The insights gleaned from your production data andyour archived data are somewhat limited. They give you a picture based on what you aredoing or have done in the past. In order to spot industry trends and compare performanceagainst other organizations, you need data from external sources.\\nUsually, data from outside sources do not conform to your formats. Y ou have to devise30 DATA WAREHOUSE: THE BUILDING BLOCKS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='db926f24-64fb-4df4-992b-3b3fe0ef91a4', embedding=None, metadata={'page_label': '54', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='conversions of data into your internal formats and data types. Y ou have to organize the\\ndata transmissions from the external sources. Some sources may provide information atregular, stipulated intervals. Others may give you the data on request. Y ou need to accom-modate the variations.\\nData Staging Component\\nAfter you have extracted data from various operational systems and from external\\nsources, you have to prepare the data for storing in the data warehouse. The extracted datacoming from several disparate sources needs to be changed, converted, and made ready ina format that is suitable to be stored for querying and analysis.\\nThree major functions need to be performed for getting the data ready. Y ou have to ex-\\ntract the data, transform the data, and then load the data into the data warehouse storage.These three major functions of extraction, transformation, and preparation for loadingtake place in a staging area. The data staging component consists of a workbench for thesefunctions. Data staging provides a place and an area with a set of functions to clean,change, combine, convert, deduplicate, and prepare source data for storage and use in thedata warehouse.\\nWhy do you need a separate place or component to perform the data preparation? Can\\nyou not move the data from the various sources into the data warehouse storage itself andthen prepare the data? When we implement an operational system, we are likely to pick updata from different sources, move the data into the new operational system database, andrun data conversions. Why can’t this method work for a data warehouse? The essential dif-ference here is this: in a data warehouse you pull in data from many source operationalsystems. Remember that data in a data warehouse is subject-oriented and cuts across op-erational applications. A separate staging area, therefore, is a necessity for preparing datafor the data warehouse.\\nNow that we have clarified the need for a separate data staging component, let us un-\\nderstand what happens in data staging. We will now briefly discuss the three major func-tions that take place in the staging area.\\nData Extraction. This function has to deal with numerous data sources. Y ou have to\\nemploy the appropriate technique for each data source. Source data may be from differ-ent source machines in diverse data formats. Part of the source data may be in relation-al database systems. Some data may be on other legacy network and hierarchical datamodels. Many data sources may still be in flat files. Y ou may want to include data fromspreadsheets and local departmental data sets. Data extraction may become quite com-plex.\\nTools are available on the market for data extraction. Y ou may want to consider using\\noutside tools suitable for certain data sources. For the other data sources, you may want todevelop in-house programs to do the data extraction. Purchasing outside tools may entailhigh initial costs. In-house programs, on the other hand, may mean ongoing costs for de-velopment and maintenance. \\nAfter you extract the data, where do you keep the data for further preparation? Y ou may\\nperform the extraction function in the legacy platform itself if that approach suits yourframework. More frequently, data warehouse implementation teams extract the sourceinto a separate physical environment from which moving the data into the data warehouseOVERVIEW OF THE COMPONENTS 31', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='61023203-504f-47ac-bb44-edee7a53fb91', embedding=None, metadata={'page_label': '55', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='would be easier. In the separate environment, you may extract the source data into a group\\nof flat files, or a data-staging relational database, or a combination of both.\\nData Transformation. In every system implementation, data conversion is an impor-\\ntant function. For example, when you implement an operational system such as a maga-zine subscription application, you have to initially populate your database with data fromthe prior system records. Y ou may be converting over from a manual system. Or, you maybe moving from a file-oriented system to a modern system supported with relational data-base tables. In either case, you will convert the data from the prior systems. So, what is sodifferent for a data warehouse? How is data transformation for a data warehouse more in-volved than for an operational system? \\nAgain, as you know, data for a data warehouse comes from many disparate sources. If\\ndata extraction for a data warehouse poses great challenges, data transformation presentseven greater challenges. Another factor in the data warehouse is that the data feed is notjust an initial load. Y ou will have to continue to pick up the ongoing changes from thesource systems. Any transformation tasks you set up for the initial load will be adapted forthe ongoing revisions as well.\\nY ou perform a number of individual tasks as part of data transformation. First, you\\nclean the data extracted from each source. Cleaning may just be correction of mis-spellings, or may include resolution of conflicts between state codes and zip codes in thesource data, or may deal with providing default values for missing data elements, or elim-ination of duplicates when you bring in the same data from multiple source systems. \\nStandardization of data elements forms a large part of data transformation. Y ou stan-\\ndardize the data types and field lengths for same data elements retrieved from the varioussources. Semantic standardization is another major task. Y ou resolve synonyms andhomonyms. When two or more terms from different source systems mean the same thing,you resolve the synonyms. When a single term means many different things in differentsource systems, you resolve the homonym.\\nData transformation involves many forms of combining pieces of data from the differ-\\nent sources. Y ou combine data from a single source record or related data elements frommany source records. On the other hand, data transformation also involves purging sourcedata that is not useful and separating out source records into new combinations. Sortingand merging of data takes place on a large scale in the data staging area. \\nIn many cases, the keys chosen for the operational systems are field values with built-\\nin meanings. For example, the product key value may be a combination of characters indi-cating the product category, the code of the warehouse where the product is stored, andsome code to show the production batch. Primary keys in the data warehouse cannot havebuilt-in meanings. We will discuss this further in Chapter 10. Data transformation also in-cludes the assignment of surrogate keys derived from the source system primary keys.\\nA grocery chain point-of-sale operational system keeps the unit sales and revenue\\namounts by individual transactions at the check-out counter at each store. But in the datawarehouse, it may not be necessary to keep the data at this detailed level. Y ou may want tosummarize the totals by product at each store for a given day and keep the summary totalsof the sale units and revenue in the data warehouse storage. In such cases, the data trans-formation function would include appropriate summarization.\\nWhen the data transformation function ends, you have a collection of integrated data\\nthat is cleaned, standardized, and summarized. Y ou now have data ready to load into eachdata set in your data warehouse. 32 DATA WAREHOUSE: THE BUILDING BLOCKS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='06c8d29c-69ae-48f3-8b86-63b5edc423a0', embedding=None, metadata={'page_label': '56', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Data Loading. Two distinct groups of tasks form the data loading function. When you\\ncomplete the design and construction of the data warehouse and go live for the first time,you do the initial loading of the data into the data warehouse storage. The initial loadmoves large volumes of data using up substantial amounts of time. As the data warehousestarts functioning, you continue to extract the changes to the source data, transform thedata revisions, and feed the incremental data revisions on an ongoing basis. Figure 2-7 il-lustrates the common types of data movements from the staging area to the data ware-house storage.\\nData Storage Component\\nThe data storage for the data warehouse is a separate repository. The operational systems\\nof your enterprise support the day-to-day operations. These are online transaction process-ing applications. The data repositories for the operational systems typically contain onlythe current data. Also, these data repositories contain the data structured in highly normal-ized formats for fast and efficient processing. In contrast, in the data repository for a datawarehouse, you need to keep large volumes of historical data for analysis. Further, youhave to keep the data in the data warehouse in structures suitable for analysis, and not forquick retrieval of individual pieces of information. Therefore, the data storage for the datawarehouse is kept separate from the data storage for operational systems.\\nIn your databases supporting operational systems, the updates to data happen as trans-\\nactions occur. These transactions hit the databases in a random fashion. How and whenthe transactions change the data in the databases is not completely within your control.The data in the operational databases could change from moment to moment. When youranalysts use the data in the data warehouse for analysis, they need to know that the data isstable and that it represents snapshots at specified periods. As they are working with theOVERVIEW OF THE COMPONENTS 33\\n/G75This function is time-consuming\\n/G75Initial load moves very large volumes of data\\n/G75The business conditions determine the refresh cycles\\nBase data loadQuarterly refresh\\nMonthly refreshYearly refresh\\nDaily refreshData \\nSources\\nDATA \\nWAREHOUSE\\nFigure 2-7 Data movements to the data warehouse.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0271f287-4699-4df7-bd41-30ef6f1a28e8', embedding=None, metadata={'page_label': '57', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='data, the data storage must not be in a state of continual updating. For this reason, the data\\nwarehouses are “read-only” data repositories. \\nGenerally, the database in your data warehouse must be open. Depending on your re-\\nquirements, you are likely to use tools from multiple vendors. The data warehouse mustbe open to different tools. Most of the data warehouses employ relational database man-agement systems. \\nMany of the data warehouses also employ multidimensional database management\\nsystems. Data extracted from the data warehouse storage is aggregated in many ways andthe summary data is kept in the multidimensional databases (MDDBs). Such multidimen-sional database systems are usually proprietary products.\\nInformation Delivery Component\\nWho are the users that need information from the data warehouse? The range is fairly\\ncomprehensive. The novice user comes to the data warehouse with no training and, there-fore, needs prefabricated reports and preset queries. The casual user needs informationonce in a while, not regularly. This type of user also needs prepackaged information. Thebusiness analyst looks for ability to do complex analysis using the information in the datawarehouse. The power user wants to be able to navigate throughout the data warehouse,pick up interesting data, format his or her own queries, drill through the data layers, andcreate custom reports and ad hoc queries. \\nIn order to provide information to the wide community of data warehouse users, the in-\\nformation delivery component includes different methods of information delivery. Figure2-8 shows the different information delivery methods. Ad hoc reports are predefined re-ports primarily meant for novice and casual users. Provision for complex queries, multidi-mensional (MD) analysis, and statistical analysis cater to the needs of the business ana-lysts and power users. Information fed into Executive Information Systems (EIS) is meantfor senior executives and high-level managers. Some data warehouses also provide data todata-mining applications. Data-mining applications are knowledge discovery systems34 DATA WAREHOUSE: THE BUILDING BLOCKS\\nData \\nWarehouse\\nData Marts\\nInformation  Delivery  ComponentAd hoc reports\\nEIS feedStatistical AnalysisMD AnalysisComplex queriesOnline\\nIntranet\\nInternet\\nE-Mail\\nData Mining\\nFigure 2-8 Information delivery component.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3f4ccafb-af60-477b-8c9f-c75a5eb9c5cb', embedding=None, metadata={'page_label': '58', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='where the mining algorithms help you discover trends and patterns from the usage of your\\ndata.\\nIn your data warehouse, you may include several information delivery mechanisms.\\nMost commonly, you provide for online queries and reports. The users will enter their re-quests online and will receive the results online. Y ou may set up delivery of scheduled re-ports through e-mail or you may make adequate use of your organization’ s intranet for in-formation delivery. Recently, information delivery over the Internet has been gainingground. \\nMetadata Component\\nMetadata in a data warehouse is similar to the data dictionary or the data catalog in a\\ndatabase management system. In the data dictionary, you keep the information about thelogical data structures, the information about the files and addresses, the informationabout the indexes, and so on. The data dictionary contains data about the data in thedatabase. \\nSimilarly, the metadata component is the data about the data in the data warehouse.\\nThis definition is a commonly used definition. We need to elaborate on this definition.Metadata in a data warehouse is similar to a data dictionary, but much more than a datadictionary. Later, in a separate section in this chapter, we will devote more time for thediscussion of metadata. Here, for the sake of completeness, we just want to list metadataas one of the components of the data warehouse architecture. \\nManagement and Control Component\\nThis component of the data warehouse architecture sits on top of all the other compo-\\nnents. The management and control component coordinates the services and activitieswithin the data warehouse. This component controls the data transformation and the datatransfer into the data warehouse storage. On the other hand, it moderates the informationdelivery to the users. It works with the database management systems and enables data tobe properly stored in the repositories. It monitors the movement of data into the stagingarea and from there into the data warehouse storage itself.\\nThe management and control component interacts with the metadata component to\\nperform the management and control functions. As the metadata component contains in-formation about the data warehouse itself, the metadata is the source of information forthe management module. \\nMETADATA IN THE DATA WAREHOUSE\\nThink of metadata as the Y ellow Pages\\n®of your town. Do you need information about the\\nstores in your town, where they are, what their names are, and what products they special-ize in? Go to the Y ellow Pages. The Y ellow Pages is a directory with data about the institu-tions in your town. Almost in the same manner, the metadata component serves as a direc-tory of the contents of your data warehouse.\\nBecause of the importance of metadata in a data warehouse, we have set apart all of\\nChapter 9 for this topic. At this stage, we just want to get an introduction to the topic andhighlight that metadata is a key architectural component of the data warehouse.METADATA IN THE DATA WAREHOUSE 35', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f841cbeb-99cb-4ad6-87eb-53abbdb28e0c', embedding=None, metadata={'page_label': '59', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Types of Metadata\\nMetadata in a data warehouse fall into three major categories: \\n/L50539Operational Metadata\\n/L50539Extraction and Transformation Metadata \\n/L50539End-User Metadata\\nOperational Metadata. As you know, data for the data warehouse comes from several\\noperational systems of the enterprise. These source systems contain different data struc-tures. The data elements selected for the data warehouse have various field lengths anddata types. In selecting data from the source systems for the data warehouse, you splitrecords, combine parts of records from different source files, and deal with multiple cod-ing schemes and field lengths. When you deliver information to the end-users, you mustbe able to tie that back to the original source data sets. Operational metadata contain all ofthis information about the operational data sources.\\nExtraction and Transformation Metadata. Extraction and transformation metada-\\nta contain data about the extraction of data from the source systems, namely, the extrac-tion frequencies, extraction methods, and business rules for the data extraction. Also, thiscategory of metadata contains information about all the data transformations that takeplace in the data staging area.\\nEnd-User Metadata. The end-user metadata is the navigational map of the data ware-\\nhouse. It enables the end-users to find information from the data warehouse. The end-usermetadata allows the end-users to use their own business terminology and look for infor-mation in those ways in which they normally think of the business. \\nSpecial Significance\\nWhy is metadata especially important in a data warehouse? \\n/L50539First, it acts as the glue that connects all parts of the data warehouse. \\n/L50539Next, it provides information about the contents and structures to the developers.\\n/L50539Finally, it opens the door to the end-users and makes the contents recognizable in\\ntheir own terms. \\nCHAPTER SUMMARY\\n/L50539Defining features of the data warehouse are: separate, subject-oriented, integrated,\\ntime-variant, and nonvolatile.\\n/L50539Y ou may use a top-down approach and build a large, comprehensive, enterprise data\\nwarehouse; or, you may use a bottom-up approach and build small, independent, de-partmental data marts. In spite of some advantages, both approaches have seriousshortcomings.36 DATA WAREHOUSE: THE BUILDING BLOCKS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bbc590d6-f3e7-43be-bc03-e3f52c71a7e6', embedding=None, metadata={'page_label': '60', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539A viable practical approach is to build conformed data marts, which together form\\nthe corporate data warehouse.\\n/L50539Data warehouse building blocks or components are: source data, data staging, data\\nstorage, information delivery, metadata, and management and control.\\n/L50539In a data warehouse, metadata is especially significant because it acts as the glue\\nholding all the components together and serves as a roadmap for the end-users.\\nREVIEW QUESTIONS\\n1. Name at least six characteristics or features of a data warehouse.\\n2. Why is data integration required in a data warehouse, more so there than in an op-\\nerational application?\\n3. Every data structure in the data warehouse contains the time element. Why?4. Explain data granularity and how it is applicable to the data warehouse.5. How are the top-down and bottom-up approaches for building a data warehouse\\ndifferent? Discuss the merits and disadvantages of each approach.\\n6. What are the various data sources for the data warehouse?7. Why do you need a separate data staging component?8. Under data transformation, list five different functions you can think of.9. Name any six different methods for information delivery.\\n10. What are the three major types of metadata in a data warehouse? Briefly mention\\nthe purpose of each type.\\nEXERCISES\\n1. Match the columns:\\na. nonvolatile data A. roadmap for users\\n2. dual data granularity B. subject-oriented3. dependent data mart C. knowledge discovery4. disparate data D. private spreadsheets5. decision support E. application flavor6. data staging F . because of multiple sources7. data mining G. details and summary8. metadata H. read-only 9. operational systems I. workbench for data integration \\n10. internal data J. data from main data warehouse\\n2. A data warehouse is subject-oriented. What would be the major critical business\\nsubjects for the following companies?\\na. an international manufacturing company\\nb. a local community bankc. a domestic hotel chainEXERCISES 37', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='60976873-96de-482d-bd4b-bfd4ad1083a4', embedding=None, metadata={'page_label': '61', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. Y ou are the data analyst on the project team building a data warehouse for an insur-\\nance company. List the possible data sources from which you will bring the datainto your data warehouse. State your assumptions.\\n4. For an airlines company, identify three operational applications that would feed into\\nthe data warehouse. What would be the data load and refresh cycles?\\n5. Prepare a table showing all the potential users and information delivery methods for\\na data warehouse supporting a large national grocery chain.38 DATA WAREHOUSE: THE BUILDING BLOCKS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6c021d7f-4ab8-40b7-8759-3280ab9e086f', embedding=None, metadata={'page_label': '62', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 3\\nTRENDS IN DATA WAREHOUSING\\nCHAPTER OBJECTIVES\\n/L50539Review the continued growth in data warehousing\\n/L50539Learn how data warehousing is becoming mainstream\\n/L50539Discuss several major trends, one by one \\n/L50539Grasp the need for standards and review the progress\\n/L50539Understand Web-enabled data warehouse\\nIn the previous chapters, we have seen why data warehousing is essential for enterprises\\nof all sizes in all industries. We have reviewed how businesses are reaping major benefitsfrom data warehousing. We have also discussed the building blocks of a data warehouse.Y ou now have a fairly good idea of the features and functions of the basic components anda reasonable definition of data warehousing. Y ou have understood that it is a fundamental-ly simple concept; at the same time, you know it is also a blend of many technologies.Several business and technological drivers have moved data warehousing forward in thepast few years. \\nBefore we proceed further, we are at the point where we want to ask some relevant\\nquestions. What is the current scenario and state of the market? What businesses haveadopted data warehousing? What are the technological advances? In short, what are thesignificant trends?\\nAre you wondering if it is too early in our discussion of the subject to talk about\\ntrends? The usual practice is to include a chapter on future trends towards the end, almostas an afterthought. The reader typically glosses over the discussion on future trends. Thischapter is not so much like looking into the crystal ball for possible future happenings; wewant to deal with the important current trends that are happening now. \\nIt is important for you to keep the knowledge about the current trends as a backdrop in\\nyour mind as you continue the deeper study of the subject. When you gather the informa-\\n39Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d4e86ff9-48a8-48b2-a0a9-048da86a5c19', embedding=None, metadata={'page_label': '63', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='tional requirements for your data warehouse, you need to be aware of the current trends.\\nWhen you get into the design phase, you need to be cognizant of the trends. When you im-plement your data warehouse, you need to ensure that your data warehouse is in line withthe trends. Knowledge of the trends is important and necessary even at a fairly early stageof your study.\\nIn this chapter, we will touch upon most of the major trends. Y ou will understand how\\nand why data warehousing continues to grow and become more and more pervasive. Wewill discuss the trends in vendor solutions and products. We will relate data warehousingwith other technological phenomena such as the Internet and the Worldwide Web. Wherevermore detailed discussions are necessary, we will revisit some of the trends in later chapters. \\nCONTINUED GROWTH IN DATA WAREHOUSING\\nData warehousing is no longer a purely novel idea for study and experimentation. It is be-\\ncoming mainstream. True, the data warehouse is not in every dentist’ s office yet, but nei-ther it is confined only to high-end businesses. More than half of all U.S. companies hasmade a commitment to data warehousing. About 90% of multinational companies havedata warehouses or are planning to implement data warehouses in the next 12 months.\\nIn every industry across the board, from retail chain stores to financial institutions,\\nfrom manufacturing enterprises to government departments, from airline companies toutility businesses, data warehousing is revolutionizing the way people perform businessanalysis and make strategic decisions. Every company that has a data warehouse is realiz-ing enormous benefits that get translated into positive results at the bottom line. Many ofthese companies, now incorporating Web-based technologies, are enhancing the potentialfor greater and easier delivery of vital information. \\nOver the past five years, hundreds of vendors have flooded the market with numerous\\nproducts. Vendor solutions and products run the gamut of data warehousing: data model-ing, data acquisition, data quality, data analysis, metadata, and so on. The buyer’ s guidepublished by the Data Warehousing Institute features no fewer than 105 leading products.The market is already huge and continues to grow.\\nData Warehousing is Becoming Mainstream\\nIn the early stages, four significant factors drove many companies to move into data ware-\\nhousing:\\n/L50539Fierce competition\\n/L50539Government deregulation\\n/L50539Need to revamp internal processes\\n/L50539Imperative for customized marketing\\nTelecommunications, banking, and retail were the first ones to adopt data warehous-\\ning. That was largely because of government deregulation in telecommunications andbanking. Retail businesses moved into data warehousing because of fiercer competition.Utility companies joined the group as that sector was deregulated. The next wave of busi-nesses to get into data warehousing consisted of companies in financial services, healthcare, insurance, manufacturing, pharmaceuticals, transportation, and distribution.40 TRENDS IN DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='af1fd005-f0ef-437c-b775-8953c1064795', embedding=None, metadata={'page_label': '64', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Today, telecommunications and banking industries continue to lead in data warehouse\\nspending. As much as 15% of technology budgets in these industries is spent on datawarehousing. Companies in these industries collect large volumes of transaction data.Data warehousing is able to transform such large volumes of data into strategic informa-tion useful for decision making. \\nAt present, data warehouses exist in every conceivable industry. Figure 3-1 lists the in-\\ndustries in the order of the average salaries paid to data warehousing professionals. Theutility industry leads the list with the highest average salary.\\nIn the early stages of data warehousing, it was, for the most part, used exclusively by\\nglobal corporations. It was expensive to build a data warehouse and the tools were notquite adequate. Only large companies had the resources to spend on the new paradigm.Now we are beginning to see a strong presence of data warehousing in medium-sized andsmaller companies, which are now able to afford the cost of building data warehouses orbuying turnkey data marts. Take a look at the database management systems (DBMSs)you have been using in the past. Y ou will find that the database vendors have now addedfeatures to assist you in building data warehouses using these DBMSs. Packaged solu-tions have also become less expensive and operating systems robust enough to supportdata warehousing functions. \\nData Warehouse Expansion\\nAlthough earlier data warehouses concentrated on keeping summary data for high-level\\nanalysis, we now see larger and larger data warehouses being built by different businesses.Now companies have the ability to capture, cleanse, maintain, and use the vast amounts ofdata generated by their business transactions. The quantities of data kept in the data ware-CONTINUED GROWTH IN DATA WAREHOUSING 41\\nConsumer Pkg.\\nTelecomInsuranceTransportationGovernmentHealthcareOtherBankingLegalEducationPetrochemical92\\n89888783838281797874Utility\\nMedia/PublishingAerospaceConsultingRetailHigh TechFinancial ServicePharmaceuticalHW/SW VendorBusiness ServicesManufacturing77\\n75746966666565615754\\nSource: 1999 Data Warehousing Salary Survey by the Data Warehousing InstituteAnnual average salary in $ 000\\nFigure 3-1 Industries using data warehousing.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='93bef3bd-fdd9-4d14-9faf-a09b385aa383', embedding=None, metadata={'page_label': '65', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='houses continue to swell to the terabyte range. Data warehouses storing several terabytes\\nof data are not uncommon in retail and telecommunications. \\nFor example, take the telecommunications industry. A telecommunications company\\ngenerates hundreds of millions of call-detail transactions in a year. For promoting theproper products and services, the company needs to analyze these detailed transactions.The data warehouse for the company has to store data at the lowest level of detail.\\nSimilarly, consider a retail chain with hundreds of stores. Every day, each store gener-\\nates many thousands of point-of-sale transactions. Again, another example is a companyin the pharmaceutical industry that processes thousands of tests and measurements forgetting product approvals from the government. Data warehouses in these industries tendto be very large. \\nFinally, let us look at the potential size of a typical Medicaid Fraud Control Unit of a\\nlarge state. This organization is exclusively responsible for investigating and prosecutinghealth care fraud arising out of billions of dollars spent on Medicaid in that state. The unitalso has to prosecute cases of patient abuse in nursing homes and monitor fraudulentbilling practices by physicians, pharmacists, and other health care providers and vendors.Usually there are several regional offices. A fraud scheme detected in one region must bechecked against all other regions. Can you imagine the size of the data warehouse neededto support such a fraud control unit? There could be many terabytes of data. \\nVendor Solutions and Products\\nAs an information technology professional, you are familiar with database vendors and\\ndatabase products. In the same way, you are familiar with most of the operating systemsand their vendors. How many leading database vendors are there? How many leading ven-dors of operating systems are there? A handful? The number of database and operatingsystem vendors pales in comparison with data warehousing products and vendors. Thereare hundreds of data warehousing vendors and thousands of data warehousing productsand solutions. \\nIn the beginning, the market was filled with confusion and vendor hype. Every vendor,\\nsmall or big, that had any product remotely connected to data warehousing jumped on thebandwagon. Data warehousing meant what each vendor defined it to be. Each companypositioned its own products as the proper set of data warehousing tools. Data warehousingwas a new concept for many of the businesses that adopted it. These businesses were atthe mercy of the marketing hype of the vendors.\\nOver the past decade, the situation has improved tremendously. The market is reaching\\nmaturity to the extent of producing off-the-shelf packages and becoming increasingly sta-ble. Figure 3-2 shows the current state of the data warehousing market.\\nWhat do we normally see in any maturing market? We expect to find a process of\\nconsolidation. And that is exactly what is taking place in the data warehousing market.Data warehousing vendors are merging to form stronger and more viable companies.Some major players in the industry are extending the range of their solutions by acqui-sition of other companies. Some vendors are positioning suites of products, their own orones from groups of other vendors, piecing them together as integrated data warehous-ing solutions. \\nNow the traditional database companies are also in the data warehousing market. They\\nhave begun to offer data warehousing solutions built around their database products. Onone hand, data extraction and transformation tools are packaged with the database man-42 TRENDS IN DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b7d28178-fee2-4327-bc89-1f712be0d015', embedding=None, metadata={'page_label': '66', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='agement system. On the other hand, inquiry and reporting tools are enhanced for data\\nwarehousing. Some database vendors take the enhancement further by offering sophisti-cated products such as data mining tools.\\nWith so many vendors and products, how can we classify the vendors and products,\\nand thereby make sense of the market? It is best to separate the market broadly into twodistinct groups. The first group consists of data warehouse vendors and products cateringto the needs of corporate data warehouses in which all of enterprise data is integrated andtransformed. This segment has been referred to as the market for strategic data warehous-es. This segment accounts for about a quarter of the total market. The second segment ismore loose and dispersed, consisting of departmental data marts, fragmented databasemarketing systems, and a wide range of decision support systems. Specific vendors andproducts dominate each segment. \\nWe may also look at the list of products in another way. Figure 3-3 shows a list of prod-\\nucts, grouped by the functions they perform in a data warehouse.\\nSIGNIFICANT TRENDS\\nSome experts feel that technology has been driving data warehousing until now. These ex-\\nperts declare that we are now beginning to see important progress in software. In the nextfew years, data warehousing is expected make big strides in software, especially for opti-mizing queries, indexing very large tables, enhancing SQL, improving data compression,and expanding dimensional modeling. \\nLet us separate out the significant trends and discuss each briefly. Be prepared to visit\\neach trend, one by one—every one has a serious impact on data warehousing. As we walkSIGNIFICANT TRENDS 43\\nVendor\\nhypeProliferation\\nof toolsConfusing\\ndefinitions\\nTotal lack of\\nstandardsVendor\\nacquisitions\\nVendor\\nmergers\\nProduct\\nSophisti -\\ncationNew\\nTechnologies\\n(OLAP, etc.)\\nSupport for\\nlarger DWs\\nWeb -\\nenabled\\nsolutionsDW market in a\\nstate of fluxDW market more\\nmature and stable\\nFigure 3-2 Current status of the data warehousing market.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='43a607fb-d899-40e6-9976-aa06852d3d1f', embedding=None, metadata={'page_label': '67', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='through each trend, try to grasp its significance and be sure that you perceive its relevance\\nto your company’ s data warehouse. Be prepared to answer the question: What must you doto take advantage of the trend in your data warehouse?\\nMultiple Data Types\\nWhen you build the first iteration of your data warehouse, you may just include numeric\\ndata. But soon you will realize that including structured numeric data alone is not enough.Be prepared to consider other data types as well. \\nTraditionally, companies included structured data, mostly numeric, in their data ware-\\nhouses. From this point of view, decision support systems were divided into two camps:data warehousing dealt with structured data; knowledge management involved unstruc-tured data. This distinction is being blurred. For example, most marketing data consistsof structured data in the form of numeric values. Marketing data also contains unstruc-tured data in the form of images. Let us say a decision maker is performing an analysisto find the top-selling product types. The decision maker arrives at a specific producttype in the course of the analysis. He or she would now like to see images of the prod-ucts in that type to make further decisions. How can this be made possible? Companiesare realizing there is a need to integrate both structured and unstructured data in theirdata warehouses.\\nWhat are the types of data we call unstructured data? Figure 3-4 shows the different\\ntypes of data that need to be integrated in the data warehouse to support decision makingmore effectively. \\nLet us now turn to the progress made in the industry for including some of the types of44 TRENDS IN DATA WAREHOUSING\\nPRODUCTS BY FUNCTIONS   (Number of leading products shown within parenthesis)\\nData Integrity & Cleansing   (12)\\nData Modeling   (10)Extraction/Transformation\\nGeneric   (26)Application-specific (9)\\nData Movement   (12)Information Servers\\nRelational DBs   (9)Specialized Indexed DBs   (5)Multidimensional DBs   (16)\\nDecision Support\\nRelational OLAP   (9)Desktop OLAP   (9)Query & Reporting   (19)Data Mining  (23) Application Development   (9)Administration & Management\\nMetadata Management   (14) Monitoring   (5) Job Scheduling   (2)Query Governing   (3)Systems Management   (1)\\nDW Enabled Applications\\nFinance   (10)Sales/Marketing/CRM   (23)Balanced Scorecard   (5) Industry specific   (21)\\nTurnkey Systems   (14) \\nSource: The Data Warehousing Institute\\nFigure 3-3 Data warehousing products by functions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='615c97aa-515d-4f14-9a94-f1dd827295df', embedding=None, metadata={'page_label': '68', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='unstructured data. Y ou will gain an understanding of what must be done to include these\\ndata types in your data warehouse. \\nAdding Unstructured Data. Some vendors are addressing the inclusion of unstruc-\\ntured data, especially text and images, by treating such multimedia data as just anotherdata type. These are defined as part of the relational data and stored as binary large ob-jects (BLOBs) up to 2 GB in size. User-defined functions (UDFs) are used to define theseas user-defined types (UDTs). \\nNot all BLOBs can be stored simply as another relational data type. For example, a\\nvideo clip would require a server supporting delivery of multiple streams of video at agiven rate and synchronization with the audio portion. For this purpose, specializedservers are being provided. \\nSearching Unstructured Data. Y ou have enhanced your data warehouse by adding\\nunstructured data. Is there anything else you need to do? Of course, without the ability tosearch unstructured data, integration of such data is of little value. Vendors are now pro-viding new search engines to find the information the user needs from unstructured data.Query by image content is an example of a search mechanism for images. The product al-lows you to preindex images based on shapes, colors, and textures. When more than oneimage fits the search argument, the selected images are displayed one after the other. \\nFor free-form text data, retrieval engines preindex the textual documents to allow\\nsearches by words, character strings, phrases, wild cards, proximity operators, and Booleanoperators. Some engines are powerful enough to substitute corresponding words andsearch. A search with a word mouse will also retrieve documents containing the word mice.SIGNIFICANT TRENDS 45\\n1234567\\n8901234\\n5678901\\n2345678\\n9012345\\nabcdefgh\\nijklmnop\\nqrstuvwx\\nyzabcdef\\nghijk\\nunstructuredData Warehouse\\nRepositoryStructured Numeric\\nStructured Text\\nUnstructured DocumentImageSpatial\\nVideo\\nAudio\\nFigure 3-4 Data warehouse: multiple data types.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d0fb767b-cbdf-48f8-8f3f-b0e8ebff41a2', embedding=None, metadata={'page_label': '69', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Searching audio and video data directly is still in the research stage. Usually, these are\\ndescribed with free-form text, and then searched using textual search methods that arecurrently available. \\nSpatial Data. Consider one of your important users, maybe the Marketing Director,\\nbeing online and performing an analysis using your data warehouse. The Marketing Di-rector runs a query: show me the sales for the first two quarters for all products comparedto last year in store XYZ. After reviewing the results, he or she thinks of two other ques-tions. What is the average income of people living in the neighborhood of that store?What is the average driving distance for those people to come to the store? These ques-tions may be answered only if you include spatial data in your data warehouse.\\nAdding spatial data will greatly enhance the value of your data warehouse. Address,\\nstreet block, city quadrant, county, state, and zone are examples of spatial data. Vendorshave begun to address the need to include spatial data. Some database vendors are provid-ing spatial extenders to their products using SQL extensions to bring spatial and businessdata together. \\nData Visualization\\nWhen a user queries your data warehouse and expects to see results only in the form of\\noutput lists or spreadsheets, your data warehouse is already outdated. Y ou need to displayresults in the form of graphics and charts as well. Every user now expects to see the re-sults shown as charts. Visualization of data in the result sets boosts the process of analysisfor the user, especially when the user is looking for trends over time. Data visualizationhelps the user to interpret query results quickly and easily. \\nMajor Visualization Trends. In the last few years, three major trends have shaped\\nthe direction of data visualization software. \\nMore Chart Types. Most data visualizations are in the form of some standard chart\\ntype. The numerical results are converted into a pie chart, a scatter plot, or another charttype. Now the list of chart types supported by data visualization software has grown muchlonger.\\nInteractive Visualization. Visualizations are no longer static. Dynamic chart types are\\nthemselves user interfaces. Y our users can review a result chart, manipulate it, and thensee newer views online. \\nVisualization of Complex and Large Result Sets. Y ou users can view a simple series\\nof numeric result points as a rudimentary pie or bar chart. But newer visualization soft-ware can visualize thousands of result points and complex data structures.\\nFigure 3-5 summarizes these major trends. See how the technologies are maturing,\\nevolving, and emerging.\\nVisualization Types. Visualization software now supports a large array of chart\\ntypes. Gone are the days of simple line graphs. The current needs of users vary enormous-ly. The business users demand pie and bar charts. The technical and scientific users needscatter plots and constellation graphs. Analysts looking at spatial data need maps and oth-46 TRENDS IN DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b36b2c44-31c7-4c73-9b21-1b005c1586b3', embedding=None, metadata={'page_label': '70', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='er three-dimensional representations. Executives and managers, who need to monitor per-\\nformance metrics, like digital dashboards that allow them to visualize the metrics asspeedometers, thermometers, or traffic lights. In the last few years, three major trendshave shaped the direction of data visualization software. \\nAdvanced Visualization Techniques. The most remarkable advance in visualiza-\\ntion techniques is the transition from static charts to dynamic interactive presentations. \\nChart Manipulation. A user can rotate a chart or dynamically change the chart type to\\nget a clearer view of the results. With complex visualization types such as constellationand scatter plots, a user can select data points with a mouse and then move the pointsaround to clarify the view.\\nDrill Down. The visualization first presents the results at the summary level. The user\\ncan then drill down the visualization to display further visualizations at subsequent levelsof detail.\\nAdvanced Interaction. These techniques provide a minimally invasive user interface.\\nThe user simply double clicks a part of the visualization and then drags and drops repre-sentations of data entities. Or, the user simply right clicks and chooses options from amenu. Visual query is the most advanced of user interaction features. For example, theuser may see the outlying data points in a scatter plot, then select a few of them with themouse and ask for a brand new visualization of just those selected points. The data visual-ization software generates the appropriate query from the selection, submits the query tothe database, and then displays the results in another representation.SIGNIFICANT TRENDS 47\\nSmall data sets to large, complex structuresStatic to Dynamic VisualizationDrill \\nDown\\nPrinted \\nReportsBasic \\nInteraction\\nOnline \\nDisplaysAdvanced \\nInteractionVisual \\nQuery\\nMATURINGEVOLVINGEMERGING\\nEnterprise \\nCharting \\nSystems\\nBasic \\nChartingEmbedded \\nCharting\\nPresentation \\nGraphicsScientific \\nChart \\nTypesMultiple Link \\nCharts\\nMassive \\nData Sets\\nSimple \\nNumeric \\nSeriesRealtime \\nData Feed\\nMultidimensional \\nData SeriesUnstructured \\nText DataNeural Data\\nFigure 3-5 Data visualization trends.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='73948e24-a70f-4f4e-87ec-be49b00f3d8c', embedding=None, metadata={'page_label': '71', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Parallel Processing\\nY ou know that the data warehouse is a user-centric and query-intensive environment. Y our\\nusers will constantly be executing complex queries to perform all types of analyses. Eachquery would need to read large volumes of data to produce result sets. Analysis, usuallyperformed interactively, requires the execution of several queries, one after the other, byeach user. If the data warehouse is not tuned properly for handling large, complex, simul-taneous queries efficiently, the value of the data warehouse will be lost. Performance is ofprimary importance.\\nThe other functions for which performance is crucial are the functions of loading data\\nand creating indexes. Because of large volumes, loading of data can be slow. Again, in-dexing is usually elaborate in a data warehouse because of the need to access the data inmany different ways. Because of large numbers of indexes, index creation could also beslow.\\nHow do you speed up query processing, data loading, and index creation? A very ef-\\nfective way to do accomplish this is to use parallel processing. Both hardware configura-tions and software techniques go hand in hand to accomplish parallel processing. A task isdivided into smaller units and these smaller units are executed concurrently.\\nParallel Processing Hardware Options. In a parallel processing environment, you\\nwill find these characteristics: multiple CPUs, memory modules, one or more servernodes, and high-speed communication links between interconnected nodes. \\nEssentially, you can choose from three architectural options. Figure 3-6 indicates the\\nthree options and their comparative merits. Please note the advantages and disadvantagesso that you may choose the proper option for your data warehouse.\\nParallel Processing Software Implementation. Y ou may choose the appropriate\\nparallel processing hardware configuration for your data warehouse. Hardware alonewould be worthless if the operating system and the database software cannot make use ofthe parallel features of the hardware. Y ou will have to ensure that the software can allocateunits of a larger task to the hardware components appropriately. \\nParallel processing software must be capable of performing the following steps:\\n/L50539Analyzing a large task to identify independent units that can be executed in parallel \\n/L50539Identifying which of the smaller units must be executed one after the other \\n/L50539Executing the independent units in parallel and the dependent units in the proper se-\\nquence \\n/L50539Collecting, collating, and consolidating the results returned by the smaller units \\nDatabase vendors usually provide two options for parallel processing: parallel server\\noption and parallel query option. Y ou may purchase each option separately. Depending onthe provisions made by the database vendors, these options may be used with one or moreof the parallel hardware configurations. \\nThe parallel server option allows each hardware node to have its own separate database\\ninstance, and enables all database instances to access a common set of underlying data-base files. \\nThe parallel query option supports key operations such as query processing, data load-\\ning, and index creation to be parallelized.48 TRENDS IN DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e61c1d27-392f-4627-ba2d-1f21b9002e8b', embedding=None, metadata={'page_label': '72', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Implementing a data warehouse without parallel processing options is almost unthink-\\nable in the current state of the technology. In summary, you will realize the following sig-nificant advantages when you adopt parallel processing in your data warehouse: \\n/L50539Performance improvement for query processing, data loading, and index creation \\n/L50539Scalability, allowing the addition of CPUs and memory modules without any\\nchanges to the existing application \\n/L50539Fault tolerance so that the database would be available even when some of the paral-\\nlel processors fail \\n/L50539Single logical view of the database even though the data may reside on the disks of\\nmultiple nodes \\nQuery Tools\\nIn a data warehouse, if there is one set of functional tools that are most significant, it is the\\nset of query tools. The success of your data warehouse depends on your query tools. Be-cause of this, data warehouse vendors have improved query tools during the past fewyears. \\nWe will discuss query tools in greater detail in Chapter 14. At this stage, just note the\\nfollowing functions for which vendors have greatly enhanced their query tools.\\n/L50539Flexible presentation —Easy to use and able to present results online and on reports\\nin many different formatsSIGNIFICANT TRENDS 49\\nCPU\\nShared\\nMemory Shared DisksCommon BusCPU CPU CPU\\nShared DisksCPU CPU CPU\\nShared\\nMemory\\nCommon High Speed Bus NodeCPU CPU CPU\\nShared\\nMemory\\nCPU CPU CPU\\nMEM\\n MEM\\n MEM\\nDisk Disk Disk\\nCLUSTERMPPSMP\\nNode\\nFigure 3-6 Parallel processing: hardware options.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e9e1b310-33dd-420d-a405-9faee10520f1', embedding=None, metadata={'page_label': '73', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Aggregate awareness —Able to recognize the existence of summary or aggregate ta-\\nbles and automatically route queries to the summary tables when summarized re-sults are desired \\n/L50539Crossing subject areas —Able to cross over from one subject data mart to another\\nautomatically\\n/L50539Multiple heterogeneous sources —Capable of accessing heterogeneous data sources\\non different platforms\\n/L50539Integration —Integrate query tools for online queries, batch reports, and data extrac-\\ntion for analysis, and provide seamless interface to go from one type of output to an-other\\n/L50539Overcoming SQL limitations —Provide SQL extensions to handle requests that can-\\nnot usually be done through standard SQL\\nBrowser Tools\\nHere we are using the term “browser” in a generic sense, not limiting it to Web browsers.\\nY our users will be running queries against your data warehouse. They will be generatingreports from your data warehouse. They will be performing these functions directly andnot with the assistance of someone like you in IT. This is expected to be one of the majoradvantages of the data warehouse approach.\\nIf the users have to go to  the data warehouse directly, they need to know what informa-\\ntion is available there. The users need good browser tools to browse through the informa-tional metadata and search to locate the specific pieces of information they want to re-ceive. Similarly, when you are part of the IT team to develop your company’ s datawarehouse, you need to identify the data sources, the data structures, and the businessrules. Y ou also need good browser tools to browse through the information about the datasources. Here are some recent trends in enhancements to browser tools:\\n/L50539Tools are extensible to allow definition of any type of data or informational object\\n/L50539Inclusion of open APIs (application program interfaces) \\n/L50539Provision of several types of browsing functions including navigation through hier-\\narchical groupings\\n/L50539Allowing users to browse the catalog (data dictionary or metadata), find an informa-\\ntional object of interest, and proceed further to launch the appropriate query toolwith the relevant parameters\\n/L50539Applying Web browsing and search techniques to browse through the information\\ncatalogs \\nData Fusion\\nA data warehouse is a place where data from numerous sources are integrated to provide a\\nunified view of the enterprise. Data may come from the various operational systems run-ning on multiple platforms where it may be stored in flat files or in databases supportedby different DBMSs. In addition to internal sources, data from external sources is also in-cluded in the data warehouse. In the data warehouse repository, you may also find varioustypes of unstructured data in the form of documents, images, audio, and video.50 TRENDS IN DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='20e60b2f-0249-4f25-bc2e-507f0789d32c', embedding=None, metadata={'page_label': '74', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='In essence, various types of data from multiple disparate sources need to be integrated\\nor fused together and stored in the data warehouse. Data fusion is a technology dealingwith the merging of data from disparate sources. It has a wider scope and includes real-time merging of data from instruments and monitoring systems. Serious research is beingconducted in the technology of data fusion. The principles and techniques of data fusiontechnology have a direct application in data warehousing.\\nData fusion not only deals with the merging of data from various sources, it also has\\nanother application in data warehousing. In present-day warehouses, we tend to collectdata in astronomical proportions. The more information stored, the more difficult it is tofind the right information at the right time. Data fusion technology is expected to addressthis problem also.\\nBy and large, data fusion is still in the realm of research. Vendors are not rushing to\\nproduce data fusion tools yet. At this stage, all you need to do is to keep your eyes openand watch for developments. \\nMultidimensional Analysis\\nToday, every data warehouse environment provides for multidimensional analysis. This is\\nbecoming an integral part of the information delivery system of the data warehouse. Pro-vision of multidimensional analysis to your users simply means that they will be able toanalyze business measurements in many different ways. Multidimensional analysis is alsosynonymous with online analytical processing (OLAP). \\nBecause of the enormous importance of OLAP , we will discuss this topic in greater de-\\ntail in Chapter 15. At this stage, just note that vendors have made tremendous progress inOLAP tools. Now vendor products are evaluated to a large extent by the strength of theirOLAP components. \\nAgent Technology\\nA software agent is a program that is capable of performing a predefined programmable\\ntask on behalf of the user. For example, on the Internet, software agents can be used tosort and filter out e-mail according to rules defined by the user. Within the data ware-house, software agents are beginning to be used to alert the users of predefined businessconditions. They are also beginning to be used extensively in conjunction with data min-ing and predictive modeling techniques. Some vendors specialize in alert system tools.Y ou should definitely consider software agent programs for your data warehouse.\\nAs the size of data warehouses continues to grow, agent technology gets applied more\\nand more. Let us say your marketing analyst needs to use your data warehouse with rigidregularity to identify threat and opportunity conditions that can offer business advantagesto the enterprise. The analyst has to run several queries and perform multilevel analysis tofind these conditions. Such conditions are exception conditions. So the analyst has to stepthrough very intense iterative analysis. Some threat and opportunity conditions may bediscovered only after long periods of iterative analysis. This takes up a lot of the analyst’ stime, perhaps on a daily basis.\\nWhenever a threat or opportunity condition is discovered through elaborate analysis, it\\nmakes sense to describe the event to a software agent program. This program will then au-tomatically signal to the analyst every time that condition is encountered in the future.This is the very essence of agent technology.SIGNIFICANT TRENDS 51', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='10ff2605-e54b-4d86-a41e-3f45288242e9', embedding=None, metadata={'page_label': '75', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Software agents may even be used for routine monitoring of business performance.\\nY our CEO may want to be notified every time the corporate-wide sales drop below themonthly targets, three months in a row. A software agent program may be used to alerthim or her every time this condition happens. Y our marketing VP may want to know everytime the monthly sales promotions in all the stores are successful. Again, a software agentprogram may be used for this purpose. \\nSyndicated Data\\nThe value of the data content is derived not only from the internal operational systems,\\nbut from suitable external data as well. With the escalating growth of data warehouse im-plementations, the market for syndicated data is rapidly expanding. \\nExamples of the traditional suppliers of syndicated data are A. C. Nielsen and Informa-\\ntion Resources, Inc. for retail data and Dun & Bradstreet and Reuters for financial andeconomic data. Some of the earlier data warehouses were incorporating syndicated datafrom such traditional suppliers to enrich the data content. \\nNow data warehouse developers are looking at a host of new suppliers dealing with\\nmany other types of syndicated data. The more recent data warehouses receive demo-graphic, psychographic, market research, and other kinds of useful data from new suppli-ers. Syndicated data is becoming big business.\\nData Warehousing and ERP\\nLook around to see what types of applications companies have been implementing in the\\nlast few years. Y ou will observe a predominant phenomenon. Many businesses are adopt-ing ERP (enterprise resource planning) application packages offered by major vendorslike SAP , Baan, JD Edwards, and PeopleSoft. The ERP market is huge, crossing the $45billion mark. \\nWhy are companies rushing into ERP applications? Most companies are plagued by\\nnumerous disparate applications that cannot present a single unified view of the corporateinformation. Many of the legacy systems are totally outdated. Reconciliation of data re-trieved from various systems to produce meaningful and correct information is extremelydifficult, and, at some large corporations, almost impossible. Some companies were look-ing for alternative ways to circumvent the enormous undertaking of making old legacysystems Y2K-compliant. ERP vendors seemingly came to the rescue of such companies.\\nData in ERP Packages. A remarkable feature of an ERP package is that it supports\\npractically every phase of the day-to-day business of an enterprise, from inventory controlto customer billing, from human resources to production management, from product cost-ing to budgetary control. Because of this feature, ERP packages are huge and complex.The ERP applications collect and integrate lots of corporate data. As these are proprietaryapplications, the large volumes of data are stored in proprietary formats available for ac-cess only through programs written in proprietary languages. Usually, thousands of rela-tional database tables are needed to support all the various functions. \\nIntegrating ERP and Data Warehouse. In the early 1990s, when ERP was intro-\\nduced, this grand solution promised to bring about the integrated corporate data reposito-ries companies were looking for. Because all data was cleansed, transformed, and integrat-52 TRENDS IN DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='67c7781f-a180-43d5-8cba-0abf794aee48', embedding=None, metadata={'page_label': '76', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ed in one place, the appealing vision was that decision making and action taking could\\ntake place from one integrated environment. Soon companies implementing ERP realizedthat the thousands of relational database tables, designed and normalized for running thebusiness operations, were not at all suitable for providing strategic information. Moreover,ERP data repositories lacked data from external sources and from other operational sys-tems in the company. If your company has ERP or is planning to get into ERP , you need toconsider the integration of ERP with data warehousing.\\nIntegration Options. Corporations integrating ERP and the data warehouse initia-\\ntives usually adopt one of three options shown in Figure 3-7. ERP vendors have begun tocomplement their packages with data warehousing solutions. Companies adopting Option1 implement the data warehousing solution of the ERP vendor with the currently availablefunctionality and await the enhancements. The downside to this approach is that you maybe waiting forever for the enhancements. In Option 2, companies implement customizeddata warehouses and use third-party tools to extract data from the ERP datasets. Retriev-ing and loading data from the proprietary ERP datasets is not easy. Option 3 is a hybridapproach that combines the functionalities provided by the vendor’ s data warehouse withadditional functionalities from third-party tools.\\nY ou need to examine these three approaches carefully and pick the one most suitable\\nfor your corporation. \\nData Warehousing and KM\\nIf 1998 marked the resurgence of ERP systems, 1999 marked the genesis of knowledge\\nmanagement (KM) systems in many corporations. Knowledge management is catching onSIGNIFICANT TRENDS 53\\nOther\\nOperational\\nSystemsExternal\\nData\\nERP\\nSystemERP   Data\\nWarehouse\\nOPTION  1 OPTION  2 OPTION  3\\nERP Data\\nWarehouse “as is”Custom -developed\\nData WarehouseHybrid: ERP Data\\nWarehouse enhanced\\nwith 3rd party toolsOther\\nOperational\\nSystemsExternal\\nData\\nCustom\\nData\\nWarehouse\\nERP\\nSystemOther\\nOperational\\nSystemsExternal\\nData\\nERP\\nSystemEnhanced\\nERP   Data\\nWarehouse\\nFigure 3-7 ERP and data warehouse integration: options.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1bcfb2a2-1f2a-4632-ab63-b97d9be5df55', embedding=None, metadata={'page_label': '77', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='very rapidly. Operational systems deal with data; informational systems such as data\\nwarehouses empower the users by capturing, integrating, storing, and transforming thedata into useful information for analysis and decision making. Knowledge managementtakes the empowerment to a higher level. It completes the process by providing users withknowledge to use the right information, at the right time, and at the right place. \\nKnowledge Management. Knowledge is actionable information. What do we mean\\nby knowledge management? It is a systematic process for capturing, integrating, organiz-ing, and communicating knowledge accumulated by employees. It is a vehicle to sharecorporate knowledge so that the employees may be more effective and be productive intheir work. Where does the knowledge exist in a corporation? Corporate procedures, doc-uments, reports analyzing exception conditions, objects, math models, what-if cases, textstreams, video clips—all of these and many more such instruments contain corporateknowledge. \\nA knowledge management system must store all such knowledge in a knowledge\\nrepository, sometimes called a knowledge warehouse. If a data warehouse contains struc-tured information, a knowledge warehouse holds unstructured information. Therefore, aknowledge management framework must have tools for searching and retrieving unstruc-tured information. \\nData Warehousing and KM. As a data warehouse developer, what are your con-\\ncerns about knowledge management? Take a specific corporate scenario. Let us say saleshave dropped in the South Central region. Y our Marketing VP is able to discern this fromyour data warehouse by running some queries and doing some preliminary analysis. Thevice president does not know why the sales are down, but things will begin to clear up if,just at that time, he or she has access to a document prepared by an analyst explaining whythe sales are low and suggesting remedial action. That document contains the pertinentknowledge, although this is a simplistic example. The VP needs numeric information, butsomething more as well.\\nKnowledge, stored in a free unstructured format, must be linked to the sale results to\\nprovide context to the sales numbers from the data warehouse. With technological ad-vances in organizing, searching, and retrieval of unstructured data, more knowledge phi-losophy will enter into data warehousing. Figure 3-8 shows how you can extend your datawarehouse to include retrievals from the knowledge repository that is part of the knowl-edge management framework of your company. \\nNow, in the above scenario, the VP can get the information about the sales drop from\\nthe data warehouse and then retrieve the relevant analyst’ s document from the knowledgerepository. Knowledge obtained from the knowledge management system can providecontext to the information received from the data warehouse to understand the story be-hind the numbers.\\nData Warehousing and CRM\\nFiercer competition has forced many companies to pay greater attention to retaining cus-\\ntomers and winning new ones. Customer loyalty programs have become the norm.Companies are moving away from mass marketing to one-on-one marketing. Customerfocus has become the watchword. Concentration on customer experience and customerintimacy has become the key to better customer service. More and more companies are54 TRENDS IN DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cbffbcf4-3db3-409c-9cc5-7d00f84a0308', embedding=None, metadata={'page_label': '78', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='embracing customer relationship management (CRM) systems. A number of leading\\nvendors offer turnkey CRM solutions that promise to enable one-on-one service to cus-tomers. \\nWhen your company is gearing up to be more attuned to high levels of customer ser-\\nvice, what can you, as a data warehouse architect, do? If you already have a data ware-house, how must you readjust it? If you are building a new data warehouse, what are thefactors for special emphasis? Y ou will have to make your data warehouse more focused onthe customer. Y ou will have to make your data warehouse CRM-ready, not an easy task byany means. In spite of the difficulties, the payoff from a CRM-ready data warehouse issubstantial.\\nCRM-Ready Data Warehouse. Y our data warehouse must hold details of every\\ntransaction at every touchpoint with each customer. This means every unit of every sale ofevery product to every customer must be gathered in the data warehouse repository. Y ounot only need sales data in detail but also details of every other type of encounter witheach customer. In addition to summary data, you have to load every encounter with everycustomer in the data warehouse. Atomic or detailed data provides maximum flexibility forthe CRM-ready data warehouse. Making your data warehouse CRM-ready will increasethe data volumes tremendously. Fortunately, today’ s technology facilitates large volumesof atomic data to be placed across multiple storage management devices that can be ac-cessed through common data warehouse tools.\\nTo make your data warehouse CRM-ready, you have to enhance some other functions\\nalso. For customer-related data, cleansing and transformation functions are more involvedand complex. Before taking the customer name and address records to the data ware-house, you have to parse unstructured data to eliminate duplicates, combine them to formSIGNIFICANT TRENDS 55\\nIntegrated Data Warehouse -- Knowledge RepositoryData \\nWarehouse\\nKnowledge \\nRepository\\nKR Query ConstructorUSER QUERY\\nKR  QUERYDW  QUERYRESULTS\\nRESULTS\\nFigure 3-8 Integration of KM and data warehouse.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3db34101-f78a-405d-be03-6ed5fc5f26d8', embedding=None, metadata={'page_label': '79', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='distinct households, and enrich them with external demographic and psychographic data.\\nThese are major efforts. Traditional data warehousing tools are not quite suited for thespecialized requirements of customer-focused applications. \\nActive Data Warehousing\\nSo far we have discussed a number of significant trends that are very relevant to what you\\nneed to bear in mind while building your data warehouse. Why not end our discussion ofthe significant trends with a bang? Let us look at what is known as active data warehous-ing.\\nWhat do you think of opening your data warehouse to 30,000 users worldwide, consist-\\ning of employees, customers, and business partners, in addition to allowing about 15 mil-lion users public access to the information every day? What do you think about making ita 24 × 7 continuous service delivery environment with 99.9% availability? Y our datawarehouse quickly becomes mission-critical instead of just being strategic. Y ou are intoactive data warehousing. \\nOne-on-One Service. This is what one global company has accomplished with an\\nactive data warehouse. The company operates in more than 60 countries, manufactures inmore than 40 countries, conducts research in nearly 30 countries, and sells over 50,000products in 200 countries. The advantages of opening up the data warehouse to outsideparties other than the employees are enormous. Suppliers work with the company on im-proved demand planning and supply chain management; the company and its distributorscooperate on planning between different sales strategies; customers make expeditiouspurchasing decisions. The active data warehouse truly provides one-on-one service to thecustomers and business partners. \\nEMERGENCE OF STANDARDS\\nThink back to our discussion in Chapter 1 of the data warehousing environment as blend\\nof many technologies. A combination of multiple types of technologies is needed forbuilding a data warehouse. The range is wide: data modeling, data extraction, data trans-formation, database management systems, control modules, alert system agents, querytools, analysis tools, report writers, and so on. \\nNow in a hot industry such as data warehousing, there is no scarcity of vendors and\\nproducts. In each of the multitude of technologies supporting the data warehouse, numer-ous vendors and products exist. The implication is that when you build your data ware-house, many choices are available to you to create an effective solution with the best-of-breed products. That is the good news. However, the bad news is that when you try to usemultivendor products, the result could also be total confusion and chaos. These multiven-dor products have to cooperate and work together in your data warehouse. \\nUnfortunately, there are no established standards for the various products to exchange\\ninformation and function together. When you use the database product from one vendor,the query and reporter tool from another vendor, and the OLAP (online analytical pro-cessing) product from yet another vendor, these three products have no standard methodfor exchanging data. Standards are especially critical in two areas: metadata interchangeand OLAP functions. 56 TRENDS IN DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bda63e00-5fe4-452a-9c8e-bfcb633de171', embedding=None, metadata={'page_label': '80', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Metadata is like the total roadmap to the information contained in a data warehouse.\\nEach product adds to the total metadata content; each product needs to use metadata cre-ated by the other products. Metadata is like the glue that holds all the functional pieces to-gether. \\nNo modern data warehouse is complete without OLAP functionality. Without OLAP ,\\nyou cannot provide your users full capability to perform multidimensional analysis, toview the information from many perspectives, and to execute complex calculations.OLAP is crucial.\\nIn the following sections, we will review the progress made so far in establishing stan-\\ndards in these two significant areas. Although progress has been made, as of mid-2000,we have not achieved fully adopted standards in either of the areas. \\nMetadata\\nTwo separate bodies are working on the standards for metadata: the Meta Data Coalition\\nand the Object Management Group. \\nMeta Data Coalition. Formed as a consortium of vendors and interested parties in\\nOctober 1995 to launch a metadata standards initiative, the coalition has been working ona standard known as the Open Information Model (OIM). Microsoft joined the coalitionin December 1998 and has been a staunch supporter along with some other leading ven-dors. In July 1999, the Meta Data Coalition accepted the Open Information Model as thestandard and began to work on extensions. In November 1999, the coalition was drivingnew key initiatives. \\nThe Object Management Group. Another group of vendors including Oracle,\\nIBM, Hewlett-Packard, Sun, and Unisys sought for metadata standards through the ObjectManagement Group, a larger established forum dealing with wider array of standards inobject technology. In June 2000, the Object Management Group unveiled the CommonWarehouse Metamodel (CWM) as the standard for metadata interchange for data ware-housing. \\nAlthough in April 2000, the Meta Data Coalition and the Object Management Group\\nsaid that they would cooperate in reaching a consensus on a single standard, this is still anelusive goal. As most corporate data is managed with tools from Oracle, IBM, and Mi-crosoft, cooperation between the two camps is all the more critical. \\nOLAP\\nThe OLAP Council was established in January 1995 as a customer advocacy group to\\nserve as an industry guide. Membership and participation are open to interested organiza-tions. As of mid-2000, the council includes sixteen general members, mainly vendors ofOLAP products. \\nOver the years, the council has worked on OLAP standards for the Multi-Dimensional\\nApplication Programmers Interface (MDAPI) and has come up with revisions. Figure 3-9shows a timeline of the major activities of the council. \\nSeveral OLAP vendors, platform vendors, consultants, and system integrators have an-\\nnounced their support for MDAPI 2.0.EMERGENCE OF STANDARDS 57', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d8ac1cf4-c221-49d7-8bba-34cfde6aaf4b', embedding=None, metadata={'page_label': '81', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='WEB-ENABLED DATA WAREHOUSE\\nWe all know that the single most remarkable phenomenon that has impacted computing\\nand communication during the last few years is the Internet. At every major industry con-ference and in every trade journal, most of the discussions relate to the Internet and theWorldwide Web in one way or another. \\nStarting with a meager number of just four host computer systems in 1969, the Internet\\nhas swelled to gigantic proportions with nearly 95 million hosts by 2000. It is still grow-ing exponentially. The number of Worldwide Web sites has escalated to nearly 26 millionby 2000. Nearly 150 million global users get on the Internet. Making full use of the ever-popular Web technology, numerous companies have built Intranets and Extranets to reachtheir employees, customers, and business partners. The Web has become the universal in-formation delivery system. \\nWe are also aware of how the Internet has fueled the tremendous growth of electronic\\ncommerce in recent years. Annual volume of business-to-business e-commerce exceeds$300 billion and total e-commerce will soon pass the $1 trillion mark. No business cancompete or survive without a Web presence. The number of companies conducting busi-ness over the Internet is expected to grow to 400,000 by 2003.\\nAs a data warehouse professional, what are the implications for you? Clearly, you\\nhave to tap into the enormous potential of the Internet and Web technology for enhanc-ing the value of your data warehouse. Also, you need to recognize the significance of e-commerce and enhance your warehouse to support and expand your company’ s e-busi-ness. \\nY ou have to transform your data warehouse into a Web-enabled data warehouse. On the\\none hand, you have to bring your data warehouse to the Web, and, on the other hand, you58 TRENDS IN DATA WAREHOUSING\\nFigure 3-9 OLAP Council: Activities timeline.JAN 1995MAR 1996MAY 1996JUL 1996SEP 1996MAY 1997JAN 1998NOV 1998JAN 1999\\nOLAP Council establishedAPR 1996\\nBusiness Objects company joins the CouncilThe Council releases first benchmark IBM joins the Council IQ Software company joins the Council Council releases MDAPI NCR joins the Council The Council releases Open Standard for Interoperability Vendors announce support for MDAPI 2.0 The council releases Enhanced Analytical Processing benchmark The council outlines areas of focus for 1999 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d725edfe-0bf5-4066-b14b-d41174e83b49', embedding=None, metadata={'page_label': '82', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='need to bring the Web to your data warehouse. In the next two subsections, we will discuss\\nthese two distinct aspects of a Web-enabled data warehouse. \\nThe Warehouse to the Web\\nIn early implementations, the corporate data warehouse was intended for managers, exec-\\nutives, business analysts, and a few other high-level employees as a tool for analysis anddecision making. Information from the data warehouse was delivered to this group ofusers in a client/server environment. But today’ s data warehouses are no longer confinedto a select group of internal users. Under present conditions, corporations need to increasethe productivity of all the members in the corporation’ s value chain. Useful informationfrom the corporate data warehouse must be provided not only to the employees but also tocustomers, suppliers, and all other business partners.\\nSo in today’ s business climate, you need to open your data warehouse to the entire\\ncommunity of users in the value chain, and perhaps also to the general public. This is a tallorder. How can you accomplish this requirement to serve information to thousands ofusers in 24 × 7 mode? How can you do this without incurring exorbitant costs for infor-mation delivery? The Internet along with Web technology is the answer. The Web will beyour primary information delivery mechanism. \\nThis new delivery method will radically change the ways your users will retrieve, ana-\\nlyze, and share information from your data warehouse. The components of your informa-tion delivery will be different. The Internet interface will include browser, search engine,push technology, home page, information content, hypertext links, and downloaded Javaor ActiveX applets.\\nWhen you bring your data warehouse to the Web, from the point of view of the users,\\nthe key requirements are: self-service data access, interactive analysis, high availabilityand performance, zero-administration client (thin client technology such as Java applets),tight security, and unified metadata. \\nThe Web to the Warehouse\\nBringing the Web to the warehouse essentially involves capturing the clickstream of all\\nthe visitors to your company’ s Web site and performing all the traditional data warehous-ing functions. And you must accomplish this, near real-time, in an environment that hasnow come to be known as the data Webhouse. Y our effort will involve extraction, transfor-mation, and loading of the clickstream data to the Webhouse repository. Y ou will have tobuild dimensional schemas from the clickstream data and deploy information deliverysystems from the Webhouse.\\nClickstream data tracks how people proceeded through your company’ s Web site, what\\ntriggers purchases, what attracts people, and what makes them come back. Clickstreamdata enables analysis of several key measures including:\\n/L50539Customer demand\\n/L50539Effectiveness of marketing promotions\\n/L50539Effectiveness of affiliate relationship among products\\n/L50539Demographic data collectionWEB-ENABLED DATA WAREHOUSE 59', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='48f78d32-811a-457d-a184-5326e4ee3efb', embedding=None, metadata={'page_label': '83', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Customer buying patterns\\n/L50539Feedback on Web site design\\nA clickstream Webhouse may be the single most important tool for identifying, priori-\\ntizing, and retaining e-commerce customers. The Webhouse can produce the followinguseful information:\\n/L50539Site statistics\\n/L50539Visitor conversions\\n/L50539Ad metrics\\n/L50539Referring partner links\\n/L50539Site navigation resulting in orders\\n/L50539Site navigation not resulting in orders\\n/L50539Pages that are session killers\\n/L50539Relationships between customer profiles and page activities\\n/L50539Best customer and worst customer analysis\\nThe Web-Enabled Configuration\\nFigure 3-10 indicates an architectural configuration for a Web-enabled data warehouse.\\nNotice the presence of the essential functional features of a traditional data warehouse. Inaddition to the data warehouse repository holding the usual types of information, theWebhouse repository contains clickstream data.60 TRENDS IN DATA WAREHOUSING\\nFigure 3-10 Web-enabled data warehouse.Warehouse\\nRepositoryWebhouse\\nRepositoryGeneral PublicCustomersBusiness PartnersEmployees\\nClickstream Data,\\nRequests through\\nExtranetsResults through\\nExtranets\\nThe Web\\nSimplified\\nView of Web -\\nenabled Data\\nWarehouse', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5b370130-97ec-42cf-a468-b7dc8e4d0a8f', embedding=None, metadata={'page_label': '84', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The convergence of the Web and data warehousing is of supreme importance to every\\ncorporation doing business in the 21st century. Because of its critical significance, we willdiscuss this topic in much greater detail in Chapter 16.\\nCHAPTER SUMMARY\\n/L50539Data warehousing is becoming mainstream with the spread of high-volume data\\nwarehouses and the rapid increase in the number of vendor products.\\n/L50539To be effective, modern data warehouses need to store multiple types of data: struc-\\ntured and unstructured, including documents, images, audio, and video.\\n/L50539Data visualization deals with displaying information in several types of visual\\nforms: text, numerical arrays, spreadsheets, charts, graphs, and so on. Tremendousprogress has been made in data visualization. \\n/L50539Data warehouse performance may be improved by using parallel processing with\\nappropriate hardware and software options. \\n/L50539It is critical to adapt data warehousing to work with ERP packages, knowledge man-\\nagement, and customer relationship systems.\\n/L50539Data warehousing industry is seriously seeking agreed-upon standards for metadata\\nand OLAP . The end is perhaps in sight. \\n/L50539Web-enabling the data warehouse means using the Web for information delivery\\nand integrating the clickstream data from the corporate Web site for analysis. Theconvergence of data warehousing and the Web technology is crucial to every busi-ness in the 21st century. \\nREVIEW QUESTIONS\\n1. State any three factors that indicate the continued growth in data warehousing.\\nCan you think of some examples? \\n2. Why do data warehouses continue to grow in size, storing huge amounts of data?\\nGive any three reasons.\\n3. Why is it important to store multiple types of data in the data warehouse? Give ex-\\namples of some nonstructured data likely to be found in the data warehouse of ahealth management organization (HMO). \\n4. What is meant by data fusion? Where does it fit in data warehousing?5. Describe four types of charts you are likely to see in the delivery of information\\nfrom a data mart supporting the finance department. \\n6. What is SMP (symmetric multiprocessing) parallel processing hardware? De-\\nscribe the configuration. \\n7. Explain what is meant by agent technology? How can this technology be used in a\\ndata warehouse?\\n8. Describe any one of the options available to integrate ERP with data warehousing. 9. What is CRM? How can you make your data warehouse CRM-ready? \\n10. What do we mean by a Web-enabled data warehouse? Describe three of its func-\\ntional features. REVIEW QUESTIONS 61', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b3654682-c706-4a63-b28b-033902beed1b', embedding=None, metadata={'page_label': '85', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='EXERCISES\\n1. Indicate if true or false:\\nA. Data warehousing helps in customized marketing.\\nB. It is more important to include unstructured data than structured data in a data\\nwarehouse.\\nC. Dynamic charts are themselves user interfaces.D. MPP is a shared-memory parallel hardware configuration.E. ERP systems may be substituted for data warehouses.F . Most of a corporation’ s knowledge base contains unstructured data.G. The traditional data transformation tools are quite adequate for a CRM-ready\\ndata warehouse.\\nH. Metadata standards facilitate deploying a combination of best-of-breed prod-\\nucts.\\nI. MDAPI is a data fusion standard.J. A Web-enabled data warehouse stores only the clickstream data captured at the\\ncorporation’ s Web site.\\n2. As the senior analyst on the data warehouse project of a large retail chain, you are\\nresponsible for improving data visualization of the output results. Make a list ofyour recommendations. \\n3. Explain how and why parallel processing can improve the performance for data\\nloading and index creation. \\n4. Discuss three specific ways in which agent technology may be used to enhance the\\nvalue of the data warehouse in a large manufacturing company.\\n5. Y our company is in the business of renting DVDs and video tapes. The company\\nhas recently entered into e-business and the senior management wants to make theexisting data warehouse Web-enabled. List and describe any three of the majortasks required for satisfying the management’ s directive.62 TRENDS IN DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='26f50721-6e03-4f98-8d2e-f665c45d6bfa', embedding=None, metadata={'page_label': '86', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 4\\nPLANNING AND PROJECT MANAGEMENT\\nCHAPTER OBJECTIVES\\n/L50539Review the essentials of planning for a data warehouse\\n/L50539Distinguish between data warehouse projects and OLTP system projects \\n/L50539Learn how to adapt the life cycle approach for a data warehouse project \\n/L50539Discuss project team organization, roles, and responsibilities\\n/L50539Consider the warning signs and success factors\\nAs soon as you read the title of this chapter, you might hasten to conclude that this is a\\nchapter intended for the project manager or the project coordinator. If you are not alreadya project manager or planning to be one in the near future, you might be inclined to justskim through the chapter. That would be a mistake. This chapter is very much designedfor all IT professionals irrespective of their roles in data warehousing projects. It willshow you how best you can fit into your specific role in a project. If you want to be part ofa team that is passionate about building a successful data warehouse, you need the detailspresented in this chapter. So please read on.\\nFirst read the following confession.\\nConsultant: So, your company is into data warehousing? How many data marts\\ndo you have?\\nProject Manager: Eleven.\\nConsultant: That’ s great. But why so many?\\nProject Manager: Ten mistakes.\\nAlthough this conversation is a bit exaggerated, according to industry experts, more\\nthan 50% of data warehouse projects are considered failures. In many cases, the project isnot completed and the system is not delivered. In a few cases, the project somehow gets\\n63Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e4fbf2e4-944f-4c31-bb01-905120a26385', embedding=None, metadata={'page_label': '87', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='completed but the data warehouse turns out to be a data basement. The project is improp-\\nerly sized and architected. The data warehouse is not aligned with the business. Projectsget abandoned in midstream.\\nSeveral factors contribute to the failures. When your company gets into data warehous-\\ning for the first time, the project will involve many organizational changes. At the presenttime, the emphasis is on enterprise-wide information analysis. Until now, each departmentand each user “owned” their data and were concerned with a set of their “own” computersystems. Data warehousing will change all of that and make managers, data owners, andend-users uneasy. Y ou are likely to uncover problems with the production systems as youbuild the data warehouse. \\nPLANNING YOUR DATA WAREHOUSE\\nMore than any other factor, improper planning and inadequate project management tend\\nto result in failures. First and foremost, determine if your company really needs a datawarehouse. Is it really ready for one? Y ou need to develop criteria for assessing the valueexpected from your data warehouse. Y our company has to decide on the type of data ware-house to be built and where to keep it. Y ou have to ascertain where the data is going tocome from and even whether you have all the needed data. Y ou have to establish who willbe using the data warehouse, how they will use it, and at what times. \\nWe will discuss the various issues related to the proper planning for a data warehouse.\\nY ou will learn how a data warehouse project differs from the types of projects you wereinvolved with in the past. We will study the guidelines for making your data warehouseproject a success.\\nKey Issues\\nPlanning for your data warehouse begins with a thorough consideration of the key issues.\\nAnswers to the key questions are vital for the proper planning and the successful comple-tion of the project. Therefore, let us consider the pertinent issues, one by one.\\nValue and Expectations. Some companies jump into data warehousing without as-\\nsessing the value to be derived from their proposed data warehouse. Of course, first youhave to be sure that, given the culture and the current requirements of your company, adata warehouse is the most viable solution. After you have established the suitability ofthis solution, then only can you begin to enumerate the benefits and value propositions.Will your data warehouse help the executives and managers to do better planning andmake better decisions? Is it going to improve the bottom line? Is it going to increase mar-ket share? If so, by how much? What are the expectations? What does the managementwant to accomplish through the data warehouse? As part of the overall planning process,make a list of realistic benefits and expectations. This is the starting point. \\nRisk Assessment. Planners generally associate project risks with the cost of the pro-\\nject. If the project fails, how much money will go down the drain? But the assessment ofrisks is more than calculating the loss from the project costs. What are the risks faced bythe company without the benefits derivable from a data warehouse? What losses are likelyto be incurred? What opportunities are likely to be missed? Risk assessment is broad and64 PLANNING AND PROJECT MANAGEMENT', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d17d15c6-1767-483c-98f4-f767ee677d44', embedding=None, metadata={'page_label': '88', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='relevant to each business. Use the culture and business conditions of your company to as-\\nsess the risks. Include this assessment as part of your planning document. \\nTop-down or Bottom-up. In Chapter 2, we discussed the top-down and bottom-up\\napproaches for building a data warehouse. The top-down approach is to start at the en-terprise-wide data warehouse, although possibly build it iteratively. Then data from theoverall, large enterprise-wide data warehouse flows into departmental and subject datamarts. On the other hand, the bottom-up approach is to start by building individual datamarts, one by one. The conglomerate of these data marts will make up the enterprisedata warehouse. \\nWe looked at the pros and cons of the two methods. We also discussed a practical ap-\\nproach of going bottom-up, but making sure that the individual data marts are conformedto one another so that they can be viewed as a whole. For this practical approach to be suc-cessful, you have to first plan and define requirements at the overall corporate level.\\nY ou have to weigh these options as they apply to your company. Do you have the large\\nresources needed to build a corporate-wide data warehouse first and then deploy the indi-vidual data marts? This option may also take more time for implementation and delay therealization of potential benefits. But this option, by its inherent approach, will ensure afully unified view of the corporate data. \\nIt is possible that your company would be satisfied with quick deployment of a few\\ndata marts for specific reasons. At this time, it may be important to just quickly react tosome market forces or ward off some fierce competitor. There may not be time to build anoverall data warehouse. Or, you may want to examine and adopt the practical approach ofconformed data marts. Whatever approach your company desires to adopt, scrutinize theoptions carefully and make the choice. Document the implications of the choice in theplanning document.\\nBuild or Buy. This is a major issue for all organizations. No one builds a data ware-\\nhouse totally from scratch by in-house programming. There is no need to reinvent thewheel every time. A wide and rich range of third-party tools and solutions are available.The real question is how much of your data marts should you build yourselves? Howmuch of these may be composed of ready-made solutions? What type of mix and matchmust be done? \\nIn a data warehouse, there is a large range of functions. Do you want to write more in-\\nhouse programs for data extraction and data transformation? Do you want to use in-houseprograms for loading the data warehouse storage? Do want to use vendor tools complete-ly for information delivery? Y ou retain control over the functions wherever you use in-house software. On the other hand, the buy option could lead to quick implementation ifmanaged effectively.\\nBe wary of the marts-in-the-box or the 15-minute-data-marts. There are no silver bul-\\nlets out there. The bottom line is to do your homework and find the proper balance be-tween in-house and vendor software. Do this at the planning stage itself.\\nSingle Vendor or Best-of-Breed. Vendors come in a variety of categories. There\\nare multiple vendors and products catering to the many functions of the data warehouse.So what are the options? How should you decide? Two major options are: (1) use theproducts of a single vendor, (2) use products from more than one vendor, selecting appro-priate tools. Choosing a single vendor solution has a few advantages:PLANNING YOUR DATA WAREHOUSE 65', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fd4bdede-ce3a-449e-9e31-bd130d7ca3bb', embedding=None, metadata={'page_label': '89', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539High level of integration among the tools\\n/L50539Constant look and feel\\n/L50539Seamless cooperation among components\\n/L50539Centrally managed information exchange\\n/L50539Overall price negotiable\\nThis approach will naturally enable your data warehouse to be well integrated and\\nfunction coherently. However, only a few vendors such as IBM and NCR offer fully inte-grated solutions.\\nReviewing this specific option further, here are the major advantages of the best-of-\\nbreed solution that combines products from multiple vendors:\\n/L50539Could build an environment to fit your organization\\n/L50539No need to compromise between database and support tools\\n/L50539Select products best suited for the function\\nWith the best-of-breed approach, compatibility among the tools from the different ven-\\ndors could become a serious problem. If you are taking this route, make sure the selectedtools are proven to be compatible. In this case, staying power of individual vendors is cru-cial. Also, you will have less bargaining power with regard to individual products and mayincur higher overall expense. Make a note of the recommended approach: have one ven-dor for the database and the information delivery functions, and pick and choose othervendors for the remaining functions. However, the multivendor approach is not advisableif your environment is not heavily technical.\\nBusiness Requirements, Not Technology\\nLet business requirements drive your data warehouse, not technology. Although this\\nseems so obvious, you would not believe how many data warehouse projects grossly vio-late this maxim. So many data warehouse developers are interested in putting pretty pic-tures on the user’ s screen and pay little attention to the real requirements. They like tobuild snappy systems exploiting the depths of technology and demonstrate their prowessin harnessing the power of technology. \\nRemember, data warehousing is not about technology, it is about solving users’ need\\nfor strategic information. Do not plan to build the data warehouse before understandingthe requirements. Start by focusing on what information is needed and not on how toprovide the information. Do not emphasize the tools. Tools and products come and go.The basic structure and the architecture to support the user requirements are more im-portant. \\nSo before making the overall plan, conduct a preliminary survey of requirements. How\\ndo you do that? No details are necessary at this stage. No in-depth probing is needed. Justtry to understand the overall requirements of the users. Y our intention is to gain a broadunderstanding of the business. The outcome of this preliminary survey will help you for-mulate the overall plan. It will be crucial to set the scope of the project. Also, it will assistyou in prioritizing and determining the rollout plan for individual data marts. For exam-ple, you may have to plan on rolling out the marketing data mart first, the finance martnext, and only then consider the human resources one.66 PLANNING AND PROJECT MANAGEMENT', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0c4d1f24-b9d3-4832-a457-032cac38feea', embedding=None, metadata={'page_label': '90', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='What types of information must you gather in the preliminary survey? At a minimum,\\nobtain general information on the following from each group of users:\\n/L50539Mission and functions of each user group\\n/L50539Computer systems used by the group\\n/L50539Key performance indicators\\n/L50539Factors affecting success of the user group\\n/L50539Who the customers are and how they are classified\\n/L50539Types of data tracked for the customers, individually and groups\\n/L50539Products manufactured or sold\\n/L50539Categorization of products and services\\n/L50539Locations where business is conducted\\n/L50539Levels at which profits are measured—per customer, per product, per district\\n/L50539Levels of cost details and revenue\\n/L50539Current queries and reports for strategic information\\nAs part of the preliminary survey, include a source system audit. Even at this stage,\\nyou must have a fairly good idea from where the data is going to be extracted for the datawarehouse. Review the architecture of the source systems. Find out about the relation-ships among the data structures. What is the quality of the data? What documentation isavailable? What are the possible mechanisms for extracting the data from the source sys-tems. Y our overall plan must contain information about the source systems.\\nTop Management Support\\nNo major initiative in a company can succeed without the support from senior manage-\\nment. This is very true in the case of the company’ s data warehouse project. The projectmust have the full support of the top management right from day one. \\nNo other venture unifies the information view of the entire corporation as the corpora-\\ntion’ s data warehouse does. The entire organization is involved and positioned for strate-gic advantage. No one department or group can sponsor the data warehousing initiative ina company. \\nMake sure you have a sponsor from the highest levels of management to keep the fo-\\ncus. The data warehouse must often satisfy conflicting requirements. The sponsor mustwield his or her influence to arbitrate and to mediate. In most companies that launch datawarehouses, the CEO is also directly interested in its success. In some companies, a seniorexecutive outside of IT becomes the primary sponsor. This person, in turn, nominatessome of the senior managers to be actively involved in the day-to-day progress of the pro-ject. Whenever the project encounters serious setbacks, the sponsor jumps in to resolvethe issues. \\nJustifying Your Data Warehouse\\nEven if your company is a medium-sized company, when everything is accounted for, the\\ntotal investment in your data warehouse could run to a few millions dollars. A roughbreakdown of the costs is as follows: hardware—31%; software, including the DBMS—PLANNING YOUR DATA WAREHOUSE 67', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='00e495c1-6208-470d-88e1-486c9cfa986b', embedding=None, metadata={'page_label': '91', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='24%; staff and system integrators—35%; administration—10%. How do you justify the\\ntotal cost by balancing the risks against the benefits, both tangible and intangible? Howcan you calculate the ROI and ROA? How can you make a business case? \\nIt is not easy. Real benefits may not be known until after your data warehouse is built\\nand put to use fully. Y our data warehouse will allow users to run queries and analyze thevariables in so many different ways. Y our users can run what-if analysis by moving intoseveral hypothetical scenarios and make strategic decisions. They will not be limited inthe ways in which they can query and analyze. Who can predict what queries and analysisthey might run, what significant decisions they will be able to make, and how beneficiallythese decisions will impact the bottom line? \\nMany companies are able to introduce data warehousing without a full cost-justifica-\\ntion analysis. Here the justification is based mainly on intuition and potential competi-tive pressures. In these companies, the top management is able to readily recognize thebenefits of data integration, improved data quality, user autonomy in running queriesand analyses, and the ease of information accessibility. If your company is such a com-pany, good luck to you. Do some basic justification and jump into the project with bothfeet in. \\nNot every company’ s top management is so easy to please. In many companies, some\\ntype of formal justification is required. We want to present the typical approaches takenfor justifying the data warehouse project. Review these examples and pick the approachthat is closest to what will work in your organization. Here are some sample approachesfor preparing the justification:\\n1. Calculate the current technology costs to produce the applications and reports sup-\\nporting strategic decision making. Compare this with the estimated costs for thedata warehouse and find the ratio between the current costs and proposed costs. Seeif this ratio is acceptable to senior management.\\n2. Calculate the business value of the proposed data warehouse with the estimated\\ndollar values for profits, dividends, earnings growth, revenue growth, and marketshare growth. Review this business value expressed in dollars against the data ware-house costs and come up with the justification. \\n3. Do the full-fledged exercise. Identify all the components that will be affected by the\\nproposed data warehouse and those that will affect the data warehouse. Start withthe cost items, one by one, including hardware purchase or lease, vendor software,in-house software, installation and conversion, ongoing support, and maintenancecosts. Then put a dollar value on each of the tangible and intangible benefits includ-ing cost reduction, revenue enhancement, and effectiveness in the business commu-nity. Go further to do a cash flow analysis and calculate the ROI.\\nThe Overall Plan\\nThe seed for a data warehousing initiative gets sown in many ways. The initiative may get\\nignited simply because the competition has a data warehouse. Or, the CIO makes a recom-mendation to the CEO or some other senior executive proposes a data warehouse as thesolution for the information problems in a company. In some cases, a senior executive wasexposed to the idea at a conference or seminar. Whatever may be the reason for your com-pany to think about data warehousing, the real initiative begins with a well-thought-out68 PLANNING AND PROJECT MANAGEMENT', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c139c21b-e04c-49ee-be76-4dda8ec58803', embedding=None, metadata={'page_label': '92', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='formal plan. This plan is a formal plan that sets the direction, tone, and goals of the initia-\\ntive. The plan lays down the motivation and the incentives. It considers the various optionsand reasons out the selection process. The plan discusses the type of data warehouse andenumerates the expectations. This is not a detailed project plan. It is an overall plan to laythe foundation, to recognize the need, and to authorize a formal project. \\nFigure 4-1 lists the types of content to be included in the formal overall plan. Review\\nthe list carefully and adapt it for your data warehouse initiative. \\nTHE DATA WAREHOUSE PROJECT\\nAs an IT professional, you have worked on application projects before. Y ou know what\\ngoes on in these projects and are aware of the methods needed to build the applicationsfrom planning through implementation. Y ou have been part of the analysis, the design, theprogramming, or the testing phases. If you have functioned as a project manager or a teamleader, you know how projects are monitored and controlled. A project is a project. If youhave seen one IT project, have you not seen them all? \\nThe answer in not a simple yes or no. Data warehouse projects are different from pro-\\njects building the transaction processing systems. If you are new to data warehousing,your first data warehouse project will reveal the major differences. We will discuss thesedifferences and also consider ways to react to them. We will also ask a basic questionabout the readiness of the IT and user departments to launch a data warehouse project.How about the traditional system development life cycle (SDLC) approach? Can we usethis approach to data warehouse projects as well? If so, what are the development phasesin the life cycle?THE DATA WAREHOUSE PROJECT 69\\nDATA WAREHOUSING INITIATIVE:  Outline for Overall Plan\\n/L50776INTRODUCTION\\n/L50776MISSION STATEMENT\\n/L50776SCOPE\\n/L50776GOALS & OBJECTIVES\\n/L50776KEY ISSUES & OPTIONS\\n/L50776VALUES & EXPECTATIONS\\n/L50776JUSTIFICATION\\n/L50776EXECUTIVE SPONSORSHIP\\n/L50776IMPLEMENTATION STRATEGY\\n/L50776TENTATIVE SCHEDULE\\n/L50776PROJECT AUTHORIZATION\\nFigure 4-1 Overall plan for data warehousing initiative.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eab51cf6-a66a-4230-a61a-1bcb263cb47c', embedding=None, metadata={'page_label': '93', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='How is it Different?\\nLet us understand why data warehouse projects are distinctive. Y ou are familiar with ap-\\nplication projects for OLTP systems. A comparison with an OLTP application project willhelp us recognize the differences. \\nTry to describe a data warehouse in terms of major functional pieces. First you have\\nthe data acquisition component. Next is the data storage component. Finally, there is theinformation delivery component. At a very general level, a data warehouse is made up ofthese three broad components. Y ou will notice that a data warehouse project differs from aproject on OLTP application in each of these three functional areas. Let us go over the dif-ferences. Figure 4-2 lists the differences and also describes them. \\nData warehousing is a new paradigm. We almost expect a data warehouse project to be\\ndifferent from an OLTP system project. We can accept the differences. But more impor-tant is the discussion of the consequences of the differences. What must you do about thedifferences? How should the project phases be changed and enhanced to deal with them?Please read the following suggestions to address the differences:\\n/L50539Consciously recognize that a data warehouse project has broader scope, tends to be\\nmore complex, and involves many different technologies.\\n/L50539Allow for extra time and effort for newer types of activities.\\n/L50539Do not hesitate to find and use specialists wherever in-house talent is not available.\\nA data warehouse project has many out-of-the-ordinary tasks.70 PLANNING AND PROJECT MANAGEMENT\\nData Warehouse Project Different From OLTP System Project\\nDATA ACQUISITION DATA STORAGE INFO. DELIVERY\\nLarge number of sources\\nMany disparate sourcesDifferent computing \\nplatforms\\nOutside sourcesHuge initial loadOngoing data feedsData replication \\nconsiderations\\nDifficult data integrationComplex data \\ntransformations\\nData cleansingSeveral user types\\nQueries stretched to limitsMultiple query typesWeb-enabled  Multidimensional analysisOLAP functionalityMetadata managementInterfaces to DSS apps.Feed into Data MiningMulti-vendor toolsStorage of large data \\nvolumes\\nRapid growthNeed for parallel \\nprocessing\\nData storage in staging \\narea\\nMultiple index typesSeveral index filesStorage of newer data \\ntypes\\nArchival of old dataCompatibility with toolsRDBMS & MDDBMS\\n/G44/G61/G74/G61/G20/G57/G61/G72/G65/G68/G6F/G75/G73/G65/G3A/G20/G44/G69/G73/G74/G69/G6E/G63/G74/G69/G76/G65/G20/G46/G65/G61/G74/G75/G72/G65/G73/G20/G61/G6E/G64/G20/G43/G68/G61/G6C/G6C/G65/G6E/G67/G65/G73/G20/G66/G6F/G72/G20/G50/G72/G6F/G6A/G65/G63/G74/G20/G4D/G61/G6E/G61/G67/G65/G6D/G65/G6E/G74\\nFigure 4-2 How a data warehouse project is different.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5e7e94b1-1a2f-4546-bfb5-80ea6e42abcf', embedding=None, metadata={'page_label': '94', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Metadata in a data warehouse is so significant that it needs special treatment\\nthroughout the project. Pay extra attention to building the metadata frameworkproperly.\\n/L50539Typically, you will be using a few third-party tools during the development and for\\nongoing functioning of the data warehouse. In your project schedule, plan to in-clude time for the evaluation and selection of tools.\\n/L50539Allow ample time to build and complete the infrastructure.\\n/L50539Include enough time for the architecture design.\\n/L50539Involve the users in every stage of the project. Data warehousing could be complete-\\nly new to both IT and the users in your company. A joint effort is imperative.\\n/L50539Allow sufficient time for training the users in the query and reporting tools.\\n/L50539Because of the large number of tasks in a data warehouse project, parallel develop-\\nment tracks are absolutely necessary. Be prepared for the challenges of running par-allel tracks in the project life cycle. \\nAssessment of Readiness\\nLet us say you have justified the data warehouse project and received the approval and\\nblessing of the top management. Y ou have an overall plan for the data warehousing initia-tive. Y ou have grasped the key issues and understood how a data warehouse project is dif-ferent and what you have to do to handle the differences. Are you then ready to jump intothe preparation of a project plan and get moving swiftly?\\nNot yet. Y ou need to do a formal readiness assessment. Normally, to many of the pro-\\nject team members and to almost all of the users, data warehousing would be a brand newconcept. A readiness assessment and orientation is important. Which person does thereadiness assessment? The project manager usually does it with the assistance of an out-side expert. By this time, the project manager would already be trained in data warehous-ing or he or she may have prior experience. Engage in discussions with the executivesponsor, users, and potential team members. The objective is to assess their familiaritywith data warehousing in general, assess their readiness, and uncover gaps in their knowl-edge. Prepare a formal readiness assessment report before the project plan is firmed up.\\nThe readiness assessment report is expected to serve the following purposes:\\n/L50539Lower the risks of big surprises occurring during implementation\\n/L50539Provide a proactive approach to problem resolution\\n/L50539Reassess corporate commitment\\n/L50539Review and reidentify project scope and size\\n/L50539Identify critical success factors\\n/L50539Restate user expectations\\n/L50539Ascertain training needs\\nThe Life-Cycle Approach\\nAs an IT professional you are all too familiar with the traditional system development life\\ncycle (SDLC). Y ou know how to begin with a project plan, move into the requirementsTHE DATA WAREHOUSE PROJECT 71', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dbe3af2b-e6ed-4543-91c6-6c6b6c815e2a', embedding=None, metadata={'page_label': '95', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='analysis phase, then into the design, construction, and testing phases, and finally into the\\nimplementation phase. The life cycle approach accomplishes all the major objectives inthe system development process. It enforces orderliness and enables a systematic ap-proach to building computer systems. The life cycle methodology breaks down the projectcomplexity and removes any ambiguity with regard to the responsibilities of project teammembers. It implies a predictable set of tasks and deliverables.\\nThat the life cycle approach breaks down the project complexity is alone reason\\nenough for this approach to be applied to a data warehouse project. A data warehouse pro-ject is complex in terms of tasks, technologies, and team member roles. But a one-size-fits-all life cycle approach will not work for a data warehouse project. Adapt the life cycleapproach to the special needs of your data warehouse project. Note that a life cycle fordata warehouse development is not a waterfall method in which one phase ends and cas-cades into the next one. \\nThe approach for a data warehouse project has to include iterative tasks going through\\ncycles of refinement. For example, if one of your tasks in the project is identification ofdata sources, you might begin by reviewing all the source systems and listing all thesource data structures. The next iteration of the task is meant to review the data elementswith the users. Y ou move on to the next iteration of reviewing the data elements with thedatabase administrator and some other IT staff. The next iteration of walking through thedata elements one more time completes the refinements and the task. This type of iterativeprocess is required for each task because of the complexity and broad scope of the project.\\nRemember that the broad functional components of a data warehouse are data acquisi-\\ntion, data storage, and information delivery. Make sure the phases of your developmentlife cycle wrap around these functional components. Figure 4-3 shows how to relate thefunctional components to SDLC.72 PLANNING AND PROJECT MANAGEMENT\\nProject Start\\nProject EndSYSTEM\\nPHASESDEVELOPMENT\\nLIFE CYCLEData Acquisition Data Storage Information Delivery\\nFigure 4-3 DW functional components and SDLC.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fc947213-024f-425d-9709-f47f9bd9ef29', embedding=None, metadata={'page_label': '96', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='As in any system development life cycle, the data warehouse project begins with the\\npreparation of a project plan. The project plan describes the project, identifies the specificobjectives, mentions the crucial success factors, lists the assumptions, and highlights thecritical issues. The plan includes the project schedule, lists the tasks and assignments, andprovides for monitoring progress. Figure 4-4 provides a sample outline of a data ware-house project plan.\\nThe Development Phases\\nIn the previous section, we again referred to the overall functional components of a data\\nwarehouse as data acquisition, data storage, and information delivery. These three func-tional components form the general architecture of the data warehouse. There must be theproper technical infrastructure to support these three functional components. Therefore,when we formulate the development phases in the life cycle, we have to ensure that thephases include tasks relating to the three components. The phases must also include tasksto define the architecture as composed of the three components and to establish the under-lying infrastructure to support the architecture. The design and construction phase forthese three components may run somewhat in parallel. \\nRefer to Figure 4-5 and notice the three tracks of the development phases. In the devel-\\nopment of every data warehouse, these tracks are present with varying sets of tasks. Y oumay change and adapt the tasks to suit your specific requirements. Y ou may want to em-phasize one track more than the others. If data quality is a problem in your company, youneed to pay special attention to the related phase. The figure shows the broad division ofthe project life cycle into the traditional phases:THE DATA WAREHOUSE PROJECT 73\\n/L50776INTRODUCTION\\n/L50776PURPOSE\\n/L50776ASSESSMENT OF READINESS\\n/L50776GOALS & OBJECTIVES\\n/L50776STAKEHOLDERS\\n/L50776ASSUMPTIONS\\n/L50776CRITICAL ISSUES\\n/L50776SUCCESS FACTORS\\n/L50776PROJECT TEAM\\n/L50776PROJECT SCHEDULE\\n/L50776DEPLOYMENT DETAILS\\nFigure 4-4 Data warehouse project plan: sample outline.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e8350265-041e-405e-9adf-51917b40cb5a', embedding=None, metadata={'page_label': '97', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Project plan\\n/L50539Requirements definition\\n/L50539Design\\n/L50539Construction\\n/L50539Deployment\\n/L50539Growth and maintenance\\nInterleaved within the design and construction phases are the three tracks along with\\nthe definition of the architecture and the establishment of the infrastructure. Each of theboxes shown in the diagram represents a major activity to be broken down further into in-dividual tasks and assigned to the appropriate team members. Use the diagram as a guidefor listing the activities and tasks for your data warehouse project. Although the major ac-tivities may remain the same for most warehouses, the individual tasks within each activi-ty are likely to vary for your specific data warehouse. \\nIn the following chapters, we will discuss these development activities in greater detail.\\nWhen you get to those chapters, you may want to refer back to this diagram.\\nTHE PROJECT TEAM\\nAs in any type of project, the success of a data warehouse project rides on the shoulders of\\nthe project team. The best team wins. A data warehouse project is similar to other soft-ware projects in that it is human-intensive. It takes several trained and specially skilledpersons to form the project team. Organizing the project team for a data warehouse pro-ject has to do with matching diverse roles with proper skills and levels of experience. Thatis not an easy feat to accomplish.\\nTwo things can break a project: complexity overload and responsibility ambiguity. In a74 PLANNING AND PROJECT MANAGEMENT\\nProject Planning\\nRequirements DefinitionDesign\\nConstruction MaintenanceDeploymentDATA \\nSTORAGEDATA \\nACQUISI-\\nTION\\nINFO. \\nDELIVERYINFRASTRUCTUREARCHITECTURE\\nFigure 4-5 Data warehouse development phases.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cdeabfd8-1e52-494d-8100-8f9af22b4b36', embedding=None, metadata={'page_label': '98', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='life cycle approach, the project team minimizes the complexity of the effort by sharing\\nand performing. When the right person on the team with the right type of skills and withthe right level of experience does an individual task, this person is really resolving thecomplexity issue.\\nIn a properly constituted project team, each person is given specific responsibilities of\\na particular role based on his or her skill and experience level. In such a team, there is noconfusion or ambiguity about responsibilities.\\nIn the following sections, we will discuss the fitting of the team members into suit-\\nable roles. We will also discuss the responsibilities associated with the roles. Further, wewill discuss the skills and experience levels needed for each of these roles. Please payclose attention and learn how to determine project roles for your data warehouse. Also,try to match your project roles with the responsibilities and tasks in your warehouseproject. \\nOrganizing the Project Team\\nOrganizing a project team involves putting the right person in the right job. If you are or-\\nganizing and putting together a team to work on an OLTP system development, you knowthat the required skills set is of a reasonable size and is manageable. Y ou would need spe-cialized skills in the areas of project management, requirements analysis, application de-sign, database design, and application testing. But a data warehouse project calls for manyother roles. How then do you fill all these varied roles?\\nA good starting point is to list all the project challenges and specialized skills needed.\\nY our list may run like this: planning, defining data requirements, defining types ofqueries, data modeling, tools selection, physical database design, source data extraction,data validation and quality control, setting up the metadata framework, and so on. As thenext step, using your list of skills and anticipated challenges, prepare a list of team rolesneeded to support the development work.\\nOnce you have a list of roles, you are ready to assign individual persons to the team\\nroles. It is not necessary to assign one or more persons to each of the identified roles. Ifyour data warehouse effort is not large and your company’ s resources are meager, trymaking the same person wear many hats. In this personnel allocation process, rememberthat the user representatives must also be considered as members of the project team. Donot fail to recognize the users as part of the team and to assign them to suitable roles.\\nSkills, experience, and knowledge are important for team members. Nevertheless, atti-\\ntude, team spirit, passion for the data warehouse effort, and strong commitment are equal-ly important, if not more so. Do not neglect to look for these critical traits.\\nRoles and Responsibilities\\nProject team roles are designated to perform one or more related tasks. In many data\\nwarehouse projects, the team roles are synonymous with the job titles given to the teammembers. If you review an OLTP system development project, you will find that the jobtitles for the team members are more or less standardized. In the OLTP system project,you will find the job titles of project manager, business analyst, systems analyst, program-mer, data analyst, database administrator, and so on. However, the data warehouse pro-jects are not yet standardized as far as the job tiles go. Still there is an element of experi-mentation and exploration. THE PROJECT TEAM 75', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='76e248ee-a91f-4da6-9261-9ec7de82c95e', embedding=None, metadata={'page_label': '99', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='So what are the prevailing job titles? Let us first look at the long list shown in Figure\\n4-6. Do not be alarmed by the length of the list. Unless your data warehouse is of mam-moth proportions, you will not need all these job titles. This list just indicates the possibil-ities and variations. Responsibilities of the same role may be attached to different job ti-tles in different projects. In many projects, the same team member will fulfill theresponsibilities of more than one role.\\nData warehousing authors and practitioners tend to classify the roles or job titles in\\nvarious ways. They first come up with broad classifications and then include individualjob titles within these classifications. Here are some of the classifications of the roles:\\n/L50539Classifications: Staffing for initial development, Staffing for testing, Staffing for\\nongoing maintenance, Staffing for data warehouse management\\n/L50539Broad classifications: IT and End-Users, then subclassifications within each of the\\ntwo broad classifications, followed by further subclassifications\\n/L50539Classifications: Front Office roles, Back Office roles\\n/L50539Classifications: Coaches, Regular lineup, Special teams\\n/L50539Classifications: Management, Development, Support\\n/L50539Classifications: Administration, Data Acquisition, Data Storage, Information\\nDelivery \\nIn your data warehouse project, you may want to come up with broad classifications\\nthat are best suited for your environment. How do you come up with the broad classifica-tions? Y ou will have to reexamine the goals and objectives. Y ou will have to assess theareas in the development phases that would need special attention. Is data extraction going76 PLANNING AND PROJECT MANAGEMENT\\nExecutive Sponsor\\nProject DirectorProject ManagerUser Representative ManagerData Warehouse AdministratorOrganizational Change ManagerDatabase AdministratorMetadata ManagerBusiness Requirements AnalystData Warehouse ArchitectData Acquisition DeveloperData Access DeveloperData Quality AnalystData Warehouse TesterMaintenance DeveloperData Provision Specialist\\nBusiness AnalystSystem AdministratorData Migration SpecialistData Grooming SpecialistData Mart LeaderInfrastructure SpecialistPower UserTraining LeaderTechnical WriterTools SpecialistVendor Relations SpecialistWeb MasterData ModelerSecurity Architect\\nFigure 4-6 Data warehouse project: job titles.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='603784c1-97d0-4c7d-9c8d-0aacde774fef', embedding=None, metadata={'page_label': '100', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='to be your greatest challenge? Then support that function with specialized roles. Is your\\ninformation delivery function going to be complex? Then have special project team rolesstrong in information delivery. Once you have determined the broad classifications, thenwork on the individual roles within each classification. If it is your first data warehouseproject, you may not come up with all the necessary roles up front. Do not be too con-cerned. Y ou may keep supporting the project with additional team roles here and there asthe project moves along. \\nY ou have read the long list of possible team roles and the ways the roles may be classi-\\nfied. This may be your first data warehouse project and you may be the one responsible todetermine the team roles for the project. Y ou want to get started and have a basic question:Is there a standard set of basic roles to get the project rolling? Not really. There is no suchstandard set. If you are inclined to follow traditional methodology, follow the classifica-tions of management, development, and support. If you want to find strengths for thethree major functional areas, then adopt the classifications of data acquisition, data stor-age, and information delivery. Y ou may also find that a combination of these two ways ofclassifying would work for your data warehouse.\\nDespite the absence of a standard set of roles, we would suggest a basic set of team\\nroles:\\n/L50539Executive Sponsor\\n/L50539Project Manager\\n/L50539User Liaison Manager\\n/L50539Lead Architect\\n/L50539Infrastructure Specialist\\n/L50539Business Analyst\\n/L50539Data Modeler\\n/L50539Data Warehouse Administrator\\n/L50539Data Transformation Specialist\\n/L50539Quality Assurance Analyst\\n/L50539Testing Coordinator\\n/L50539End-User Applications Specialist\\n/L50539Development Programmer\\n/L50539Lead Trainer\\nFigure 4-7 lists the usual responsibilities attached to the suggested set of roles. Please\\nreview the descriptions of the responsibilities. Add or modify the descriptions to makethem applicable to the special circumstances of your data warehouse. \\nSkills and Experience Levels\\nWe discussed the guidelines for determining the broad classifications of the team roles.\\nAfter you figure out the classifications relevant to your data warehouse project, you willcome up with the set of team roles appropriate to your situation. We reviewed some exam-ples of typical roles. The roles may also be called job titles in a project. Moving forward,you will write down the responsibilities associated with the roles you have established.Y ou have established the roles and you have listed the responsibilities. Are you then readyTHE PROJECT TEAM 77', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='27517be5-9d9d-4bad-a391-3f74b62e51de', embedding=None, metadata={'page_label': '101', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='to match the people to fill into these roles? There is one more step needed before you can\\ndo that.\\nTo fit into the roles and discharge the responsibilities, the selected persons must have\\nthe right abilities. They should possess suitable skills and need the proper work experi-ence. So you have to come up with a list of skills and experience required for the variousroles. Figure 4-8 describes the skills and experience levels for our sample set of teamroles. Use the descriptions found in the figure as examples to compose the descriptionsfor the team roles in your data warehouse project. \\nIt is not easy to find IT professionals to fill all the roles established for your data ware-\\nhouse. OLTP systems are ubiquitous. All IT professionals have assumed some role or theother in an OLTP system project. This is not the case with data warehouse projects. Nottoo many professionals have direct hands-on experience in the development of data ware-houses. Outstanding skills and abilities are in short supply. \\nIf people qualified to work on data warehouse projects are not readily available, what is\\nyour recourse? How can you fill the roles in your project? This is where training becomesimportant. Train suitable professionals in data warehousing concepts and techniques. Letthem learn the fundamentals and specialize for the specific roles. In addition to trainingyour in-house personnel, use external consultants in specific roles for which you are un-able to find people from the inside. However, as a general rule, consultants must not beused in leading roles. The project manager or the lead administrator must come fromwithin the organization. \\nUser Participation\\nIn a typical OLTP application, the users interact with the system through GUI screens.\\nThey use the screens for data input and for retrieving information. The users receive any78 PLANNING AND PROJECT MANAGEMENT\\nExecutive Sponsor\\nDirection, support, arbitration.\\nProject Manager\\nAssignments, monitoring, control.\\nUser Liaison Manager\\nCoordination with user groups.\\nLead Architect\\nArchitecture design.\\nInfrastructure Specialist\\nInfrastructure design/construction.\\nBusiness Analyst\\nRequirements definition.\\nData Modeler\\nRelational and dimensional modeling.  Data Warehouse Administrator\\nDBA functions.\\nData Transformation Specialist\\nData extraction,integration, transformation.\\nQuality Assurance Analyst\\nQuality control for warehouse data.\\nTesting Coordinator\\nProgram, system, tools testing.\\nEnd-User Applications Specialist\\nConfirmation of data meanings/relationships.\\nDevelopment Programmer\\nIn-house programs and scripts.\\nLead Trainer\\nCoordination of  User and Team training.  \\nFigure 4-7 Data warehouse project team: roles and responsibilities.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6b307458-b122-409a-80f7-58cbf59edd7d', embedding=None, metadata={'page_label': '102', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='additional information through reports produced by the system at periodic intervals. If the\\nusers need special reports, they have to get IT involved to write ad hoc programs that arenot part of the regular application. \\nIn striking contrast, user interaction with a data warehouse is direct and intimate. Usu-\\nally, there are no or just a few set reports or queries. When the implementation is com-plete, your users will begin to use the data warehouse directly with no mediation from IT.There is no predictability in the types of queries they will be running, the types of reportsthey will be requesting, or the types of analysis they will be performing. If there is onemajor difference between OLTP systems and data warehousing systems, it is in the usageof the system by the users. \\nWhat is the implication of this major difference in project team composition and data\\nwarehouse development? The implication is extremely consequential. What does thismean? This means that if the users will be using the data warehouse directly in unforeseenways, they must have a strong voice in its development. They must be part of the projectteam all the way. More than an OLTP system project, a data warehouse project calls forserious joint application development (JAD) techniques. \\nY our data warehouse project will succeed only if appropriate members of the user com-\\nmunity are accepted as team members with specific roles. Make use of their expertise andknowledge of the business. Tap into their experience in making business decisions. Ac-tively involve them in the selection of information delivery tools. Seek their help in test-ing the system before implementation. \\nFigure 4-9 illustrates how and where in the development process users must be made toTHE PROJECT TEAM 79\\nExecutive Sponsor\\nSenior level executive, in-depth knowledge of \\nthe business, enthusiasm and ability to moderate and arbitrate as necessary.\\nProject Manager\\nPeople skills, project management \\nexperience, business and user oriented, ability to be practical and effective.\\nUser Liaison Manager\\nPeople skills, respected in user community, \\norganization skills, team player, knowledge of systems from user viewpoint.\\nLead Architect\\nAnalytical skills, ability to see the big picture, \\nexpertise in interfaces, knowledge of data warehouse concepts.\\nInfrastructure Specialist\\nSpecialist in hardware, operating systems, \\ncomputing platforms, experience as operations staff.\\nBusiness Analyst\\nAnalytical skills, ability to interact with users, \\nsufficient industry experience as analyst.\\nData Modeler\\nExpertise in relational and dimensional \\nmodeling with case tools, experience as data analyst.  Data Warehouse Administrator\\nExpert in physical database design and \\nimplementation, experience as relational DBA, MDDBMS experience a plus.\\nData Transformation Specialist\\nKnowledge of data structures, in-depth knowledge \\nof source systems, experience as analyst.\\nQuality Assurance Analyst\\nKnowledge of data quality techniques, knowledge  \\nof source systems data, experience as analyst.\\nTesting Coordinator\\nFamiliarity with testing methods and standards,      \\nuse of testing tools, knowledge of some data          warehouse information delivery tools, experience as programmer/analyst.\\nEnd-User Applications Specialist\\nIn-depth knowledge of source applications.\\nDevelopment Programmer\\nProgramming and analysis skills, experience as \\nprogrammer in selected language and DBMS.\\nLead Trainer\\nTraining skills, experience in IT/User training, \\ncoordination and organization skills.\\nFigure 4-8 Data warehouse project team: skills and experience levels.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a5893679-80aa-4fb8-af40-7d558258a100', embedding=None, metadata={'page_label': '103', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='participate. Review each development phase and clearly decide how and where your users\\nneed to participate. This figure relates user participation to stages in the developmentprocess. Here is a list of a few team roles that users can assume to participate in the devel-opment:\\n/L50539Project Sponsor—executive responsible for supporting the project effort all the way\\n/L50539User Department Liaison Representatives—help IT to coordinate meetings and re-\\nview sessions; ensure active participation by the user departments \\n/L50539Subject Area Experts—provide guidance in the requirements of the users in specific\\nsubject areas; clarify semantic meanings of business terms used in the enterprise\\n/L50539Data Review Specialists—review the data models prepared by IT; confirm the data\\nelements and data relationships\\n/L50539Information Delivery Consultants—examine and test information delivery tools; as-\\nsist in the tool selection\\n/L50539User Support Technicians—act as the first-level, front-line support for the users in\\ntheir respective departments\\nPROJECT MANAGEMENT CONSIDERATIONS\\nY our project team was organized, the development phases were completed, the testing was\\ndone, the data warehouse was deployed, and the project was pronounced completed on80 PLANNING AND PROJECT MANAGEMENT\\nProject Planning\\nRequirements Definition\\nDesign\\nConstruction\\nDeployment\\nMaintenanceProvide goals, objectives, expectations, business information during preliminary survey; grant \\nactive top management support; initiate project as executive sponsor. \\nActively participate in meetings for defining requirements; identify all source systems; define \\nmetrics for measuring business success, and business dimensions for analysis; define information needed from data warehouse. \\nReview dimensional data model, data extraction and transformation design; provide \\nanticipated usage for database sizing; review architectural design and metadata; participate in tool selection; review information delivery design.  \\nActively participate in user acceptance testing; test information delivery tools; validate data \\nextraction and transformation functions; confirm data quality; test usage of metadata; benchmark query functions; test OLAP functions; participate in application documentation.  \\nVerify audit trails and confirm initial data load; match deliverables against stated \\nexpectations; arrange and participate in user training; provide final acceptance.  \\nProvide input for enhancements; test and accept enhancements.  \\nFigure 4-9 Data warehouse development: user participation.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d6224799-63a1-4d67-9f34-e672f112058e', embedding=None, metadata={'page_label': '104', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='time and within budget. Has the effort been successful? In spite of the best intentions of\\nthe project team, it is likely that the deployed data warehouse turns out to be anything buta data warehouse. Figure 4-10 shows possible scenarios of failure. How will your datawarehouse turn out in the end?\\nEffective project management is critical to the success of a data warehouse project. In\\nthis section, we will consider project management issues as they especially apply to datawarehouse projects, review some basic project management principles, and list the possi-ble success factors. We will review a real-life successful project and examine the reasonsfor its success. When all is said and done, you cannot always run your project totally bythe book. Adopt a practical approach that produces results without getting bogged downin unnecessary drudgery. \\nGuiding Principles \\nHaving worked on OLTP system projects, you are already aware of some of the guiding\\nprinciples of project management—do not give into analysis paralysis, do not allow scopecreep, monitor slippage, keep the project on track, and so on. Although most of thoseguiding principles also apply to data warehouse project management, we do not want torepeat them here. On the other hand, we want to consider some guiding principles thatpertain to data warehouse projects exclusively. At every stage of the project, you have tokeep the guiding principles as a backdrop so that these principles can condition each pro-ject management decision and action. The major guiding principles are: \\nSponsorship. No data warehouse project succeeds without strong and committed exec-\\nutive sponsorship.PROJECT MANAGEMENT CONSIDERATIONS 81\\nData Basement\\nPoor quality data \\nwithout proper access.\\nData Mausoleum\\nAn expensive data basement with poor access and performance.Data Shack\\nPathetic data dump collapsing even before completion.\\nData Tenement\\nBuilt by a legacy system vendor or an ignorant consultant with no idea of what users want.Data Cottage\\nStand-alone, aloof, fragmented, island data mart.\\nData Jailhouse\\nConfined and invisible data system keeping data imprisoned so that users cannot get at the data.\\nFigure 4-10 Possible scenarios of failure.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='15e68ad6-b4bf-48b1-be98-dbece283d6d8', embedding=None, metadata={'page_label': '105', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Project Manager. It is a serious mistake to have a project manager who is more tech-\\nnology-oriented than user-oriented and business-oriented. \\nNew Paradigm. Data warehousing is new for most companies; innovative project man-\\nagement methods are essential to deal with the unexpected challenges.\\nTeam Roles. Team roles are not to be assigned arbitrarily; the roles must reflect the\\nneeds of each individual data warehouse project.\\nData Quality. Three critical aspects of data in the data warehouse are: quality, quality,\\nand quality.\\nUser Requirements. Although obvious, user requirements alone form the driving force\\nof every task on the project schedule. \\nBuilding for Growth. Number of users and number of queries shoot up very quickly af-\\nter deployment; data warehouses not built for growth will crumble swiftly.\\nProject Politics. The first data warehouse project in a company poses challenges and\\nthreats to users at different levels; trying to handle project politics is like walkingthe proverbial tightrope, to be trodden with extreme caution.\\nRealistic Expectations. It is easy to promise the world in the first data warehouse pro-\\nject; setting expectations at the right and attainable levels is the best course.\\nDimensional Data Modeling. A well-designed dimensional data model is a required\\nfoundation and blueprint.\\nExternal Data. A data warehouse does not live by internal data alone; data from rele-\\nvant external sources is an absolutely necessary ingredient.\\nTraining. Data warehouse user tools are different and new. If the users do not know\\nhow to use the tools, they will not use the data warehouse. An unused data ware-house is a failed data warehouse.\\nWarning Signs\\nAs the life cycle of your data warehouse project runs its course and the development phas-\\nes are moving along, you must keep a close watch for any warning signs that may spelldisaster. Constantly be looking for any indicators suggesting doom and failure. Some ofthe warning signs may just point to inconveniences calling for little action. But there arelikely to be other warning signs indicative of wider problems that need corrective actionto ensure final success. Some warning signs may portend serious drawbacks that requireimmediate remedial action. \\nWhatever might be the nature of the warning sign, be vigilant and keep a close watch.\\nAs soon as you spot an omen, recognize the potential problem, and jump into correctiveaction. Figure 4-11 presents a list of typical warning signs and suggested corrective ac-tion. The list in the figure is just a collection of examples. In your data warehouse project,you may find other types of warning signs. Y our corrective action for potential problemsmay be different depending on your circumstances.\\nSuccess Factors\\nY ou have followed the tenets of effective project management and your data warehouse is\\ncompleted. How do you know that your data warehouse is a success? Do you need three orfive years to see if you get the ROI (return on investment) proposed in your plan? How82 PLANNING AND PROJECT MANAGEMENT', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='130b6d25-7b82-4003-b494-df322e49dc56', embedding=None, metadata={'page_label': '106', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='long do you have to wait before you can assert that your data warehouse effort is success-\\nful. Or, are there some immediate signs indicating success? \\nThere are some such indications of success that can be observed within a short time af-\\nter implementation. The following happenings generally indicate success:\\n/L50539Queries and reports— rapid increase in the number of queries and reports requested\\nby the users directly from the data warehouse\\n/L50539Query types— queries becoming more sophisticated\\n/L50539Active users— steady increase in the number of users\\n/L50539Usage— users spending more and more time in the data warehouse looking for solu-\\ntions\\n/L50539Turnaround times— marked decrease in the times required for obtaining strategic in-\\nformation\\nFigure 4-12 provides a list of key factors for a successful data warehouse project. By\\nno means is this list an exhaustive compilation of all possible ingredients for success. Noris it a magic wand to guarantee success in every situation. Y ou very well know that a goodpart of ensuring success depends on your specific project, its definite objectives, and itsunique project management challenges. Therefore, use the list for general guidance.\\nAnatomy of a Successful Project\\nNo matter how many success factors you review, and no matter how many guidelines you\\nstudy, you get a better grasp of the success principles by analyzing the details of whatreally made a real-world project a success. We will now do just that. Let us review a casePROJECT MANAGEMENT CONSIDERATIONS 83\\nThe Requirements \\nDefinition phase is well past the target date.Suffering from “analysis \\nparalysis.”Stop the capturing of unwanted \\ninformation. Remove any problems by meeting with users. Set firm final target date.\\nNeed to write too \\nmany in-house programs.Selected third party tools \\nrunning out of steam.If there is time and budget, get \\ndifferent tools. Otherwise increase programming staff.WARNING SIGN INDICATION ACTION\\nUsers not cooperating \\nto provide details of data.Possible turf concerns \\nover data ownership.Very delicate issue. Work with \\nexecutive sponsor to resolve the issue.\\nUsers not comfortable \\nwith the query tools.Users not trained \\nadequately.First, ensure that the selected \\nquery tool is appropriate. Then provide additional training.\\nContinuing problems \\nwith data brought over to the staging area.Data transformation and \\nmapping not complete.Revisit all data transformation \\nand integration routines. Ensure that no data is missing. Include the user representative in the verification process. \\nFigure 4-11 Data warehouse project: warning signs.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='14ca3800-1f30-4e88-81f1-67e20f4314bb', embedding=None, metadata={'page_label': '107', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='study of an actual business in which the data warehouse project was a tremendous suc-\\ncess. The warehouse met the goals and produced the desired results. Figure 4-13 depictsthis data warehouse, indicating the success factors and benefits. A fictional name is usedfor the business. \\nAdopt a Practical Approach\\nAfter the entire project management principles are enunciated, numerous planning meth-\\nods are described, and several theoretical nuances are explored, a practical approach isstill best for achieving results. Do not get bogged down in the strictness of the principles,rules, and methods. Adopt a practical approach to managing the project. Results alonematter; just being active and running around chasing the theoretical principles will notproduce the desired outcome. \\nA practical approach is simply a common-sense approach that has a nice blend of prac-\\ntical wisdom and hard-core theory. While using a practical approach, you are totally re-sults-oriented. Y ou constantly balance the significant activities against the less importantones and adjust the priorities. Y ou are not driven by technology just for the sake of tech-nology itself; you are motivated by business requirements.\\nIn the context of a data warehouse project, here are a few tips on adopting a practical\\napproach:\\n/L50539Running a project in a pragmatic way means constantly monitoring the deviations\\nand slippage, and making in-flight corrections to stay the course. Rearrange the pri-orities as and when necessary.\\n/L50539Let project schedules act as guides for smooth workflow and achieving results, not\\njust to control and inhibit creativity. Please do not try to control each task to the mi-84 PLANNING AND PROJECT MANAGEMENT\\nFigure 4-12 Data warehouse project: key success factors.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cbfe7377-a9d5-4677-9e8e-9960c7483704', embedding=None, metadata={'page_label': '108', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='nutest detail. Y ou will then only have time to keep the schedules up-to-date, with\\nless time to do the real job.\\n/L50539Review project task dependencies continuously. Minimize wait times for dependent\\ntasks.\\n/L50539There is really such a thing as “too much planning.” Do not give into the temptation.\\nOccasionally, ready–fire–aim may be a worthwhile principle for a practical ap-\\nproach.\\n/L50539Similarly, “too much analysis” can produce “analysis paralysis.” \\n/L50539Avoid “bleeding edge” and unproven technologies. This is very important if the pro-\\nject is the first data warehouse project in your company.\\n/L50539Always produce early deliverables as part of the project. These deliverables will sus-\\ntain the interest of the users and also serve as proof-of-concept systems.\\n/L50539Architecture first, and then only the tools. Do not choose the tools and build your\\ndata warehouse around the selected tools. Build the architecture first, based on busi-ness requirements, and then pick the tools to support the architecture.\\nReview these suggestions and use them appropriately in your data warehouse project.\\nEspecially if this is their first data warehouse project, the users will be interested in quickand easily noticeable benefits. Y ou will soon find out that they are never interested in yourfanciest project scheduling tool that empowers them to track each task by the hour orminute. They are satisfied only by results. They are attracted to the data warehouse onlyby how useful and easy to use it is.PROJECT MANAGEMENT CONSIDERATIONS 85\\nBusiness Context\\nBigCom, Inc., world’s leading supplier of \\ndata, voice, and video communication \\ntechnology with more than 300 million \\ncustomers and significant recent growth. Challenges\\nLimited availability of global information; \\nlack of common data definitions; critical business data locked in numerous disparate applications; fragmented reporting needing elaborate reconciliation; significant system downtime for daily backups and updates.  \\nTechnology and Approach\\nDeploy large-scale corporate data \\nwarehouse to provide strategic \\ninformation to 1,000 users for making \\nbusiness decisions; use proven tools from \\nsingle vendor for data extraction and \\nbuilding  data marts; query and analysis \\ntool from another reputable vendor.   Success Factors\\nClear business goals; strong executive support; user departments actively involved; selection of appropriate and proven tools; building of proper architecture first; adequate attention to data integration and transformation; emphasis on flexibility and scalability.\\nBenefits Achieved\\nTrue enterprise decision support; improved sales measurement; de creased cost of \\nownership; streamlined business processes; improved customer rel ationship management; \\nreduced IT development; ability to incorporate clickstream data from company’s Web site.\\nFigure 4-13 Analysis of a successful data warehouse.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='453f9420-eb53-46b4-af1e-e22790f7d50b', embedding=None, metadata={'page_label': '109', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER SUMMARY\\n/L50539While planning for your data warehouse, key issues to be considered include: set-\\nting proper expectations, assessing risks, deciding between top-down or bottom-upapproaches, choosing from vendor solutions.\\n/L50539Business requirements, not technology, must drive your project.\\n/L50539A data warehouse project without the full support of the top management and\\nwithout a strong and enthusiastic executive sponsor is doomed to failure from dayone. \\n/L50539Benefits from a data warehouse accrue only after the users put it to full use. Justifi-\\ncation through stiff ROI calculations is not always easy. Some data warehouses arejustified and the projects started by just reviewing the potential benefits. \\n/L50539A data warehouse project is much different from a typical OLTP system project.\\nThe traditional life cycle approach of application development must be changed andadapted for the data warehouse project.\\n/L50539Standards for organization and assignment of team roles are still in the experimental\\nstage in many projects. Modify the roles to match what is important for your pro-ject. \\n/L50539Participation of the users is mandatory for success of the data warehouse project.\\nUsers can participate in a variety of ways.\\n/L50539Consider the warning signs and success factors; in the final analysis, adopt a practi-\\ncal approach to build a successful data warehouse.\\nREVIEW QUESTIONS\\n1. Name four key issues to be considered while planning for a data warehouse. \\n2. Explain the difference between the top-down and bottom-up approaches for build-\\ning data warehouses. Do you have a preference? If so, why?\\n3. List three advantages for each of the single-vendor and multivendor solutions. 4. What is meant by a preliminary survey of requirements? List six types of informa-\\ntion you will gather during a preliminary survey.\\n5. How are data warehouse projects different from OLTP system projects? Describe\\nfour such differences.\\n6. List and explain any four of the development phases in the life cycle of data ware-\\nhouse project. \\n7. What do you consider to be a core set of team roles for a data warehouse project?\\nDescribe the responsibilities of three roles from your set.\\n8. List any three warning signs likely to be encountered in a data warehouse project.\\nWhat corrective actions will you need to take to resolve the potential problems in-dicated by these three warning signs? \\n9. Name and describe any five of the success factors in a data warehouse project.\\n10. What is meant by “taking a practical approach” to the management of a data ware-\\nhouse project? Give any two reasons why you think a practical approach is likelyto succeed. 86 PLANNING AND PROJECT MANAGEMENT', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='be578930-3df0-4a8b-83db-fce1a682c899', embedding=None, metadata={'page_label': '110', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='EXERCISES\\n1. Match the columns:\\n1. top-down approach A. tightrope walking \\n2. single-vendor solution B. not standardized3. team roles C. requisite for success4. team organization D. enterprise data warehouse5. role classifications E. consistent look and feel6. user support technician F . front office, back office7. executive sponsor G. part of overall plan8. project politics H. right person in right role 9. active user participation I. front-line support\\n10. source system structures J. guide and support project\\n2. As the recently assigned project manager, you are required to work with the execu-\\ntive sponsor to write a justification without detailed ROI calculations for the firstdata warehouse project in your company. Write a justification report to be includedin the planning document.\\n3. Y ou are the data transformation specialist for the first data warehouse project in an\\nairlines company. Prepare a project task list to include all the detailed tasks neededfor data extraction and transformation. \\n4. Why do you think user participation is absolutely essential for success? As a mem-\\nber of the recently formed data warehouse team in a banking business, your job is towrite a report on how the user departments can best participate in the development.What specific responsibilities for the users will you include in your report?\\n5. As the lead architect for a data warehouse in a large domestic retail store chain, pre-\\npare a list of project tasks relating to designing the architecture. In which develop-ment phases will these tasks be performed? EXERCISES 87', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d247264c-4627-49ca-9d4e-559428edf085', embedding=None, metadata={'page_label': '111', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 5\\nDEFINING THE BUSINESS\\nREQUIREMENTS\\nCHAPTER OBJECTIVES\\n/L50539Discuss how and why defining requirements is different for a data warehouse \\n/L50539Understand the role of business dimensions\\n/L50539Learn about information packages and their use in defining requirements\\n/L50539Review methods for gathering requirements\\n/L50539Grasp the significance of a formal requirements definition document\\nA data warehouse is an information delivery system. It is not about technology, but about\\nsolving users’ problems and providing strategic information to the user. In the phase ofdefining requirements, you need to concentrate on what information the users need, not somuch on how you are going to provide the required information. The actual methods forproviding information will come later, not while you are collecting requirements. \\nMost of the developers of data warehouses come from a background of developing op-\\nerational or OLTP (online transactions processing) systems. OLTP systems are primarilydata capture systems. On the other hand, data warehouse systems are information deliverysystems. When you begin to collect requirements for your proposed data warehouse, yourmindset will have to be different. Y ou have to go from a data capture model to an informa-tion delivery model. This difference will have to show through all phases of the data ware-house project.\\nThe users also have a different perspective about a data warehouse system. Unlike an\\nOLTP system which is needed to run the day-to-day business, no immediate payout isseen in a decision support system. The users do not see a compelling need to use a deci-sion support system whereas they cannot refrain from using an operational system, with-out which they cannot run their business.\\n89Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='936b5148-b891-4b40-8a7f-cc8b6fc6f355', embedding=None, metadata={'page_label': '112', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='DIMENSIONAL ANALYSIS\\nIn several ways, building a data warehouse is very different from building an operational\\nsystem. This becomes notable especially in the requirements gathering phase. Because ofthis difference, the traditional methods of collecting requirements that work well for oper-ational systems cannot be applied to data warehouses.\\nUsage of Information Unpredictable\\nLet us imagine you are building an operational system for order processing in your com-\\npany. For gathering requirements, you interview the users in the Order Processing depart-ment. The users will list all the functions that need to be performed. They will inform youhow they receive the orders, check stock, verify customers’ credit arrangements, price theorder, determine the shipping arrangements, and route the order to the appropriate ware-house. They will show you how they would like the various data elements to be presentedon the GUI (graphical user interface) screen for the application. The users will also giveyou a list of reports they would need from the order processing application. They will beable to let you know how and when they would use the application daily.\\nIn providing information about the requirements for an operational system, the users\\nare able to give you precise details of the required functions, information content, and us-age patterns. In striking contrast, for a data warehousing system, the users are generallyunable to define their requirements clearly. They cannot define precisely what informa-tion they really want from the data warehouse, nor can they express how they would liketo use the information or process it.\\nFor most of the users, this could be the very first data warehouse they are being ex-\\nposed to. The users are familiar with operational systems because they use these in theirdaily work, so they are able to visualize the requirements for other new operational sys-tems. They cannot relate a data warehouse system to anything they have used before.\\nIf, therefore, the whole process of defining requirements for a data warehouse is so\\nnebulous, how can you proceed as one of the analysts in the data warehouse project? Y ouare in a quandary. To be on the safe side, do you then include every piece of data you thinkthe users will be able to use? How can you build something the users are unable to defineclearly and precisely?\\nInitially, you may collect data on the overall business of the organization. Y ou may\\ncheck on the industry’ s best practices. Y ou may gather some business rules guiding theday-to-day decision making. Y ou may find out how products are developed and marketed.But these are generalities and are not sufficient to determine detailed requirements.\\nDimensional Nature of Business Data\\nFortunately, the situation is not as hopeless as it seems. Even though the users cannot ful-\\nly describe what they want in a data warehouse, they can provide you with very importantinsights into how they think about the business. They can tell you what measurement unitsare important for them. Each user department can let you know how they measure successin that particular department. The users can give you insights into how they combine thevarious pieces of information for strategic decision making. \\nManagers think of the business in terms of business dimensions. Figure 5-1 shows the90 DEFINING THE BUSINESS REQUIREMENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='166b7123-a1a2-4274-8375-307578eb8acc', embedding=None, metadata={'page_label': '113', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='kinds of questions managers are likely to ask for decision making. The figure shows what\\nquestions a typical Marketing Vice President, a Marketing Manager, and a Financial Con-troller may ask.\\nLet us briefly examine these questions. The Marketing Vice President is interested in\\nthe revenue generated by her new product, but she is not interested in a single number.She is interested in the revenue numbers by month, in a certain division, by demographic,by sales office, relative to the previous product version, and compared to plan. So theMarketing Vice President wants the revenue numbers broken down by month, division,customer demographic, sales office, product version, and plan. These are her business di-mensions along which she wants to analyze her numbers. \\nSimilarly, for the Marketing Manager, his business dimensions are product, product\\ncategory, time (day, week, month), sale district, and distribution channel. For the FinancialController, the business dimensions are budget line, time (month, quarter, year), district,and division. \\nIf your users of the data warehouse think in terms of business dimensions for decision\\nmaking, you should also think of business dimensions while collecting requirements. Al-though the actual proposed usage of a data warehouse could be unclear, the business di-mensions used by the managers for decision making are not nebulous at all. The users willbe able to describe these business dimensions to you. Y ou are not totally lost in the processof requirements definition. Y ou can find out about the business dimensions.\\nLet us try to get a good grasp of the dimensional nature of business data. Figure 5-2\\nshows the analysis of sales units along the three business dimensions of product, time, andgeography. These three dimensions are plotted against three axes of coordinates. Y ou willsee that the three dimensions form a collection of cubes. In each of the small dimensionalcubes, you will find the sales units for that particular slice of time, product, and geograph-ical division. In this case, the business data of sales units is three dimensional becauseDIMENSIONAL ANALYSIS 91\\nHow much did my new product generate                            \\nmonth by month, in the southern division, by user demographic, by sales office, relative to the previous version, and compared to plan?\\nGive me sales statistics                                        \\nby products, summarized by product categories, daily, weekly, and monthly, by sale districts, by distribution channels.\\nShow me expenses                                                \\nlisting actual vs budget, by months, quarters, and annual, by budget line  items, by district, division, summarized for the whole company.Marketing ManagerMarketing Vice President\\nFinancial Controller\\nFigure 5-1 Managers think in business dimensions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8e828e41-f68e-4790-9eaf-f168d8ebde8e', embedding=None, metadata={'page_label': '114', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='there are just three dimensions used in this analysis. If there are more than three dimen-\\nsions, we extend the concept to multiple dimensions and visualize multidimensionalcubes, also called hypercubes.\\nExamples of Business Dimensions\\nThe concept of business dimensions is fundamental to the requirements definition for a\\ndata warehouse. Therefore, we want to look at some more examples of business dimen-sions in a few other cases. Figure 5-3 displays the business dimensions in four differentcases.\\nLet us quickly look at each of these examples. For the supermarket chain, the measure-\\nments that are analyzed are the sales units. These are analyzed along four business dimen-sions. When you are looking for the hypercubes, the sides of such cubes are time, promo-tion, product, and store. If you are the Marketing Manager for the supermarket chain, youwould want your sales broken down by product, at each store, in time sequence, and in re-lation to the promotions that take place. \\nFor the insurance company, the business dimensions are different and appropriate for\\nthat business. Here you would want to analyze the claims data by agent, individual claim,time, insured party, individual policy, and status of the claim. The example of the airlinescompany shows the dimensions for analysis of frequent flyer data. Here the business di-mensions are time, customer, specific flight, fare class, airport, and frequent flyer status. \\nThe example analyzing shipments for a manufacturing company show some other\\nbusiness dimensions. In this case, the business dimensions used for the analysis of ship-ments are the ones relevant to that business and the subject of the analysis. Here you seethe dimensions of time, ship-to and ship-from locations, shipping mode, product, and anyspecial deals. \\nWhat we find from these examples is that the business dimensions are different and\\nrelevant to the industry and to the subject for analysis. We also find the time dimension to92 DEFINING THE BUSINESS REQUIREMENTS\\nSlices of product\\nsales information\\n(units sold)PRODUCT\\nTIMEJuneTV SetBoston\\nJulyChicagoTV Set\\nFigure 5-2 Dimensional nature of business data.GEOGRAPHY', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0a074a14-4a32-4f00-af85-6a1a870a5499', embedding=None, metadata={'page_label': '115', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='be a common dimension in all examples. Almost all business analyses are performed over\\ntime. \\nINFORMATION PACKAGES—A NEW CONCEPT\\nWe will now introduce a novel idea for determining and recording information require-\\nments for a data warehouse. This concept helps us to give a concrete form to the variousinsights, nebulous thoughts, and opinions expressed during the process of collecting re-quirements. The information packages, put together while collecting requirements, arevery useful for taking the development of the data warehouse to the next phases.\\nRequirements Not Fully Determinate\\nAs we have discussed, the users are unable to describe fully what they expect to see in the\\ndata warehouse. Y ou are unable to get a handle on what pieces of information you want tokeep in the data warehouse. Y ou are unsure of the usage patterns. Y ou cannot determinehow each class of users will use the new system. So, when requirements cannot be fullydetermined, we need a new and innovative concept to gather and record the requirements.The traditional methods applicable to operational systems are not adequate in this context.We cannot start with the functions, screens, and reports. We cannot begin with the datastructures. We have noted that the users tend to think in terms of business dimensions andanalyze measurements along such business dimensions. This is a significant observationand can form the very basis for gathering information. \\nThe new methodology for determining requirements for a data warehouse system is\\nbased on business dimensions. It flows out of the need of the users to base their analysison business dimensions. The new concept incorporates the basic measurements and theINFORMATION PACKAGES—A NEW CONCEPT 93\\n/G53/G75/G70/G65/G72/G6D/G61/G72/G6B/G65/G74/G20\\n/G43/G68/G61/G69/G6E\\nSALES \\nUNITS/G54/G49/G4D/G45/G50/G52/G4F/G4D/G4F/G54/G49/G4F/G4E\\n/G50/G52/G4F/G44/G55/G43/G54\\n/G53/G54/G4F/G52/G45/G4D/G61/G6E/G75/G66/G61/G63/G74/G75/G72/G69/G6E/G67/G20/G43/G6F/G6D/G70/G61/G6E/G79\\nSHIPMENTS/G54/G49/G4D/G45/G43/G55/G53/G54/G20/G53/G48/G49/G50/G2D/G54/G4F\\n/G50/G52/G4F/G44/G55/G43/G54\\n/G44/G45/G41/G4C\\n/G49/G6E/G73/G75/G72/G61/G6E/G63/G65/G20/G42/G75/G73/G69/G6E/G65/G73/G73\\nCLAIMS/G54/G49/G4D/G45/G41/G47/G45/G4E/G54\\n/G50/G4F/G4C/G49/G43/G59\\n/G53/G54/G41/G54/G55/G53/G41/G69/G72/G6C/G69/G6E/G65/G73/G20/G43/G6F/G6D/G70/G61/G6E/G79\\nFREQUENT FLYER FLIGHTS/G54/G49/G4D/G45/G43/G55/G53/G54/G4F/G4D/G45/G52\\n/G41/G49/G52/G50/G4F/G52/G54\\n/G53/G54/G41/G54/G55/G53/G53/G48/G49/G50/G20/G46/G52/G4F/G4D\\n/G53/G48/G49/G50/G20/G4D/G4F/G44/G45\\n/G43/G4C/G41/G49/G4D\\n/G49/G4E/G53/G55/G52/G45/G44/G20/G50/G41/G52/G54/G59/G46/G4C/G49/G47/G48/G54\\n/G46/G41/G52/G45/G20/G43/G4C/G41/G53/G53\\nFigure 5-3 Examples of business dimensions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='067a4711-bebb-4317-97c1-da07d68f1b06', embedding=None, metadata={'page_label': '116', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='business dimensions along which the users analyze these basic measurements. Using the\\nnew methodology, you come up with the measurements and the relevant dimensions thatmust be captured and kept in the data warehouse. Y ou come up with what is known as aninformation package for the specific subject. \\nLet us look at an information package for analyzing sales for a certain business. Figure\\n5-4 contains such an information package. The subject here is sales. The measured factsor the measurements that are of interest for analysis are shown in the bottom section of thepackage diagram. In this case, the measurements are actual sales, forecast sales, and bud-get sales. The business dimensions along which these measurements are to be analyzedare shown at the top of diagram as column headings. In our example, these dimensions aretime, location, product, and demographic age group. Each of these business dimensionscontains a hierarchy or levels. For example, the time dimension has the hierarchy goingfrom year down to the level of individual day. The other intermediary levels in the time di-mension could be quarter, month, and week. These levels or hierarchical components areshown in the information package diagram. \\nY our primary goal in the requirements definition phase is to compile information pack-\\nages for all the subjects for the data warehouse. Once you have firmed up the informationpackages, you’ll be able to proceed to the other phases.\\nEssentially, information packages enable you to:\\n/L50539Define the common subject areas\\n/L50539Design key business metrics\\n/L50539Decide how data must be presented\\n/L50539Determine how users will aggregate or roll up\\n/L50539Decide the data quantity for user analysis or query\\n/L50539Decide how data will be accessed94 DEFINING THE BUSINESS REQUIREMENTS\\nMeasured Facts : Forecast Sales, Budget Sales, Actual SalesTime \\nPeriodsLocations ProductsAge \\nGroups\\nYear Country Class Group 1DimensionsInformation Subject :Sales AnalysisHierarchies\\nFigure 5-4 An information package.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e04995fa-b050-4db2-a527-aa4aad507aee', embedding=None, metadata={'page_label': '117', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Establish data granularity\\n/L50539Estimate data warehouse size\\n/L50539Determine the frequency for data refreshing\\n/L50539Ascertain how information must be packaged\\nBusiness Dimensions\\nAs we have seen, business dimensions form the underlying basis of the new methodology\\nfor requirements definition. Data must be stored to provide for the business dimensions.The business dimensions and their hierarchical levels form the basis for all further phases.So we want to take a closer look at business dimensions. We should be able to identifybusiness dimensions and their hierarchical levels. We must be able to choose the properand optimal set of dimensions related to the measurements.\\nWe begin by examining the business dimensions for an automobile manufacturer. Let\\nus say that the goal is to analyze sales. We want to build a data warehouse that will allowthe user to analyze automobile sales in a number of ways. The first obvious dimension isthe product dimension. Again for the automaker, analysis of sales must include analysisby breaking the sales down by dealers. Dealer, therefore, is another important dimensionfor analysis. As an automaker, you would want to know how your sales break down alongcustomer demographics. Y ou would want to know who is buying your automobiles and inwhat quantities. Customer demographics would be another useful business dimension foranalysis. How do the customers pay for the automobiles? What effect does financing forthe purchases have on the sales? These questions can be answered by including themethod of payment as another dimension for analysis. What about time as a business di-mension? Almost every query or analysis involves the time element. In summary, we havecome up with the following dimensions for the subject of sales for an automaker: product,dealer, customer demographic, method of payment, and time.\\nLet us take one more example. In this case, we want to come up with an information\\npackage for a hotel chain. The subject in this case is hotel occupancy. We want to analyzeoccupancy of the rooms in the various branches of the hotel chain. We want to analyze theoccupancy by individual hotels and by room types. So hotel and room type are criticalbusiness dimensions for the analysis. As in the other case, we also need to include thetime dimension. In the hotel occupancy information package, the dimensions included arehotel, room type, and time.\\nDimension Hierarchies/Categories\\nWhen a user analyzes the measurements along a business dimension, the user usually\\nwould like to see the numbers first in summary and then at various levels of detail. Whatthe user does here is to traverse the hierarchical levels of a business dimension for gettingthe details at various levels. For example, the user first sees the total sales for the entireyear. Then the user moves down to the level of quarters and looks at the sales by individ-ual quarters. After this, the user moves down further to the level of individual months tolook at monthly numbers. What we notice here is that the hierarchy of the time dimensionconsists of the levels of year, quarter, and month. The dimension hierarchies are the pathsfor drilling down or rolling up in our analysis.\\nWithin each major business dimension there are categories of data elements that canINFORMATION PACKAGES—A NEW CONCEPT 95', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7b415b08-86f3-4a49-89c5-913624ff0df1', embedding=None, metadata={'page_label': '118', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='also be useful for analysis. In the time dimension, you may have a data element to indicate\\nwhether a particular day is a holiday. This data element would enable you to analyze byholidays and see how sales on holidays compare with sales on other days. Similarly, in theproduct dimension, you may want to analyze by type of package. The package type is onesuch data element within the product dimension. The holiday flag in the time dimensionand the package type in the product dimension do not necessarily indicate hierarchicallevels in these dimensions. Such data elements within the business dimension may becalled categories.\\nHierarchies and categories are included in the information packages for each dimen-\\nsion. Let us go back to the two examples in the previous section and find out which hier-archical levels and categories must be included for the dimensions. Let us examine theproduct dimension. Here, the product is the basic automobile. Therefore, we include thedata elements relevant to product as hierarchies and categories. These would be modelname, model year, package styling, product line, product category, exterior color, interiorcolor, and first model year. Looking at the other business dimensions for the auto salesanalysis, we summarize the hierarchies and categories for each dimension as follows:\\nProduct: Model name, model year, package styling, product line, product category, ex-\\nterior color, interior color, first model year\\nDealer: Dealer name, city, state, single brand flag, date first operation\\nCustomer demographics: Age, gender, income range, marital status, household size,\\nvehicles owned, home value, own or rent\\nPayment method: Finance type, term in months, interest rate, agent\\nTime: Date, month, quarter, year, day of week, day of month, season, holiday flag\\nLet us go back to the hotel occupancy analysis. We have included three business di-\\nmensions. Let us list the possible hierarchies and categories for the three dimensions.\\nHotel: Hotel line, branch name, branch code, region, address, city, state, Zip Code,\\nmanager, construction year, renovation year\\nRoom type: Room type, room size, number of beds, type of bed, maximum occupants,\\nsuite, refrigerator, kitchenette\\nTime: Date, day of month, day of week, month, quarter, year, holiday flag\\nKey Business Metrics or Facts\\nSo far we have discussed the business dimensions in the above two examples. These are\\nthe business dimensions relevant to the users of these two data warehouses for performinganalysis. The respective users think of their business subjects in terms of these businessdimensions for obtaining information and for doing analysis.\\nBut using these business dimensions, what exactly are the users analyzing? What num-\\nbers are they analyzing? The numbers the users analyze are the measurements or metricsthat measure the success of their departments. These are the facts that indicate to the usershow their departments are doing in fulfilling their departmental objectives. \\nIn the case of the automaker, these metrics relate to the sales. These are the numbers\\nthat tell the users about their performance in sales. These are numbers about the sale of96 DEFINING THE BUSINESS REQUIREMENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5a668d0f-b034-4096-944f-5c5f955e7a3c', embedding=None, metadata={'page_label': '119', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='each individual automobile. The set of meaningful and useful metrics for analyzing auto-\\nmobile sales is as follows:\\nActual sale price\\nMSRP sale priceOptions priceFull priceDealer add-onsDealer creditsDealer invoiceAmount of downpaymentManufacturer proceedsAmount financed\\nIn the second example of hotel occupancy, the numbers or metrics are different. The\\nnature of the metrics depends on what is being analyzed. For hotel occupancy, the metricswould therefore relate to the occupancy of rooms in each branch of the hotel chain. Hereis a list of metrics for analyzing hotel occupancy:\\nOccupied rooms\\nVacant roomsUnavailable roomsNumber of occupantsRevenue \\nNow putting it all together, let us discuss what goes into the information package dia-\\ngrams for these two examples. In each case, the metrics or facts go into the bottom sectionof the information package. The business dimensions will be the column headings. Ineach column, you will include the hierarchies and categories for the business dimensions.\\nFigures 5-5 and 5-6 show the information packages for the two examples we just dis-\\ncussed.\\nREQUIREMENTS GATHERING METHODS\\nNow that we have a way of formalizing requirements definition through information\\npackage diagrams, let us discuss the methods for gathering requirements. Remember thata data warehouse is an information delivery system for providing information for strategicdecision making. It is not a system for running the day-to-day business. Who are the usersthat can make use of the information in the data warehouse? Where do you go for gettingthe requirements?\\nBroadly, we can classify the users of the data warehouse as follows:\\nSenior executives (including the sponsors)\\nKey departmental managersREQUIREMENTS GATHERING METHODS 97', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d0edb58b-63d6-419a-97ac-4e43bffb0a6b', embedding=None, metadata={'page_label': '120', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='98 DEFINING THE BUSINESS REQUIREMENTS\\nFacts : Actual Sale Price, MSRP Sale Price, Options Price, Full Price, Dealer   \\nAdd-ons, Dealer Credits, Dealer Invoice, Down Payment, Proceeds, FinanceTime ProductPayment \\nMethodCustomer \\nDemo-\\ngraphics\\nYearDimensionsInformation Subject :Automaker SalesHierarchies / \\nCategories  Quarter\\nMonth\\nDate\\nDay of \\nWeek\\nDay of \\nMonth\\nSeason\\nHoliday \\nFlagModel \\nName\\nModel \\nYear\\nPackage \\nStyling\\nProduct \\nLine\\nProduct \\nCategory\\nExterior \\nColor\\nInterior \\nColor\\nFirst YearFinance \\nType\\nTerm \\n(Months)\\nInterest \\nRate\\nAgentDealer\\nAge\\nGender\\nIncome \\nRange\\nMarital \\nStatus\\nHouse-\\nhold Size\\nVehicles \\nOwned\\nHome \\nValue\\nOwn or \\nRentDealer \\nName\\nCity\\nState\\nSingle \\nBrand Flag\\nDate First \\nOperation\\nFigure 5-5 Information package: automaker sales.\\nFacts : Occupied Rooms, Vacant Rooms, Unavailable Rooms, Number of \\nOccupants, RevenueTime HotelRoom   \\nType\\nYearDimensionsInformation Subject :Hotel OccupancyHierarchies / \\nCategories  Quarter\\nMonth\\nDate\\nDay of \\nWeek\\nDay of \\nMonth\\nHoliday \\nFlagHotel Line\\nBranch \\nName\\nBranch \\nCode\\nRegion\\nAddress\\nCity/State/\\nZip\\nConstruc-\\ntion Year\\nRenova-\\ntion YearRoom \\nType\\nRoom \\nSize\\nNumber \\nof Beds\\nType of \\nBed\\nMax. \\nOccupants\\nSuite\\nRefrige-\\nrator\\nKichen-\\nnette\\nFigure 5-6 Information package: hotel occupancy.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='95bac38f-30b6-464f-9965-aa8a3e5ae26f', embedding=None, metadata={'page_label': '121', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Business analysts\\nOperational system DBAsOthers nominated by the above\\nExecutives will give you a sense of direction and scope for your data warehouse. They\\nare the ones closely involved in the focused area. The key departmental managers are theones that report to the executives in the area of focus. Business analysts are the ones whoprepare reports and analyses for the executives and managers. The operational systemDBAs and IT applications staff will give you information about the data sources for thewarehouse.\\nWhat requirements do you need to gather? Here is a broad list:\\nData elements: fact classes, dimensions\\nRecording of data in terms of timeData extracts from source systemsBusiness rules: attributes, ranges, domains, operational records\\nY ou will have to go to different groups of people in the various departments to gather\\nthe requirements. Two basic techniques are universally adopted for meeting with groupsof people: (1) interviews, one-on-one or in small groups; (2) Joint application develop-ment (JAD) sessions. A few thoughts about these two basic approaches follow.\\nInterviews\\n/L50539Two or three persons at a time\\n/L50539Easy to schedule\\n/L50539Good approach when details are intricate\\n/L50539Some users are comfortable only with one-on-one interviews\\n/L50539Need good preparation to be effective\\n/L50539Always conduct preinterview research\\n/L50539Also encourage users to prepare for the interview\\nGroup Sessions\\n/L50539Groups of twenty or less persons at a time\\n/L50539Use only after getting a baseline understanding of the requirements\\n/L50539Not good for initial data gathering\\n/L50539Useful for confirming requirements\\n/L50539Need to be very well organized\\nInterview Techniques\\nThe interview sessions can use up a good percentage of the project time. Therefore, these\\nwill have to be organized and managed well. Before your project team launches the inter-view process, make sure the following major tasks are completed.REQUIREMENTS GATHERING METHODS 99', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='59fb8891-3167-482b-997d-812fd90f89f0', embedding=None, metadata={'page_label': '122', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Select and train the project team members conducting the interviews\\n/L50539Assign specific roles for each team member (lead interviewer/scribe)\\n/L50539Prepare list of users to be interviewed and prepare broad schedule\\n/L50539List your expectations from each set of interviews\\n/L50539Complete preinterview research\\n/L50539Prepare interview questionnaires\\n/L50539Prepare the users for the interviews\\n/L50539Conduct a kick-off meeting of all users to be interviewed\\nMost of the users you will be interviewing fall into three broad categories: senior exec-\\nutives, departmental managers/analysts, IT department professionals. What are the expec-tations from interviewing each of these categories? Figure 5-7 shows the baseline expec-tations.\\nPreinterview research is important for the success of the interviews. Here is a list of\\nsome key research topics:\\n/L50539History and current structure of the business unit\\n/L50539Number of employees and their roles and responsibilities\\n/L50539Locations of the users\\n/L50539Primary purpose of the business unit in the enterprise\\n/L50539Relationship of the business unit to the strategic initiatives of the enterprise100 DEFINING THE BUSINESS REQUIREMENTS\\n•Organization objectives\\n\\x7fCriteria for measuring \\nsuccess\\n\\x7fKey business issues, current \\n& future\\n\\x7fProblem identification\\x7fVision and direction for the \\norganization\\n\\x7fAnticipated usage of the DW\\x7fDepartmental objectives \\n\\x7fSuccess metrics \\x7fFactors limiting success \\x7fKey business issues \\x7fProducts & Services \\x7fUseful business dimensions \\nfor analysis \\n\\x7fAnticipated usage of the DW \\n\\x7fKey operational source \\nsystems\\n\\x7fCurrent information delivery \\nprocesses\\n\\x7fTypes of routing analysis\\x7fKnown quality issues\\x7fCurrent IT support for \\ninformation requests\\n\\x7fConcerns about proposed DW Senior Executives Dept. Managers / Analysts\\nIT Dept. Professionals\\nFigure 5-7 Expectations from interviews.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aa704436-1d76-4a02-a5d6-cb82788ad77a', embedding=None, metadata={'page_label': '123', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Secondary purposes of the business unit\\n/L50539Relationship of the business unit to other units and to outside organizations\\n/L50539Contribution of the business unit to corporate revenues and costs\\n/L50539Company’ s market\\n/L50539Competition in the market\\nSome tips on the types of questions to be asked in the interviews follow.\\nCurrent Information Sources\\nWhich operational systems generate data about important business subject areas?\\nWhat are the types of computer systems that support these subject areas?What information is currently delivered in existing reports and online queries?How about the level of details in the existing information delivery systems?\\nSubject Areas\\nWhich subject areas are most valuable for analysis?\\nWhat are the business dimensions? Do these have natural hierarchies?What are the business partitions for decision making?Do the various locations need global information or just local information for decision\\nmaking? What is the mix?\\nAre certain products and services offered only in certain areas?\\nKey Performance Metrics\\nHow is the performance of the business unit currently measured?\\nWhat are the critical success factors and how are these monitored?How do the key metrics roll up?Are all markets measured in the same way?\\nInformation Frequency\\nHow often must the data be updated for decision making? What is the time frame?\\nHow does each type of analysis compare the metrics over time?What is the timeliness requirement for the information in the data warehouse?\\nAs initial documentation for the requirements definition, prepare interview write-ups\\nusing this general outline:\\n1. User profile\\n2. Background and objectives3. Information requirements4. Analytical requirements5. Current tools used6. Success criteriaREQUIREMENTS GATHERING METHODS 101', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2cd981b8-df87-4018-8ac8-0c6a2fb809e4', embedding=None, metadata={'page_label': '124', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7. Useful business metrics\\n8. Relevant business dimensions\\nAdapting the JAD Methodology\\nIf you are able to gather a lot of baseline data up front from different sources, group ses-\\nsions may be a good substitute for individual interviews. In this method, you are able toget a number of interested users to meet together in group sessions. On the whole, thismethod could result in fewer group sessions than individual interview sessions. Theoverall time for requirements gathering may prove to be less and therefore shorten theproject. Also, group sessions may be more effective if the users are dispersed in remotelocations.\\nJoint application development (JAD) techniques were successfully utilized to gather\\nrequirements for operational systems in the 1980s. Users of computer systems had grownto be more computer-savvy and their direct participation in the development of applica-tions proved to be very useful. \\nAs the name implies, JAD is a joint process, with all the concerned groups getting to-\\ngether for a well-defined purpose. It is a methodology for developing computer applica-tions jointly by the users and the IT professionals in a well-structured manner. JAD cen-ters around discussion workshops lasting a certain number of days under the direction of afacilitator. Under suitable conditions, the JAD approach may be adapted for building adata warehouse.\\nJAD consists of a five-phased approach:\\nProject Definition\\nComplete high-level interviewsConduct management interviewsPrepare management definition guide\\nResearch\\nBecome familiar with the business area and systemsDocument user information requirementsDocument business processesGather preliminary informationPrepare agenda for the sessions\\nPreparation\\nCreate working document from previous phaseTrain the scribesPrepare visual aidsConduct presession meetingsSet up a venue for the sessionsPrepare checklist for objectives\\nJAD Sessions\\nOpen with review of agenda and purposeReview assumptions102 DEFINING THE BUSINESS REQUIREMENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='790afab2-fc6e-4083-bef1-e09ca07939a8', embedding=None, metadata={'page_label': '125', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Review data requirements\\nReview business metrics and dimensionsDiscuss dimension hierarchies and roll-upsResolve all open issuesClose sessions with lists of action items\\nFinal Document\\nConvert the working documentMap the gathered informationList all data sourcesIdentify all business metricsList all business dimensions and hierarchiesAssemble and edit the documentConduct review sessionsGet final approvalsEstablish procedure to change requirements\\nThe success of a project using the JAD approach very much depends on the composi-\\ntion of the JAD team. The size and mix of the team will vary based on the nature and pur-pose of the data warehouse. The typical composition, however, must have pertinent rolespresent in the team. For each of the following roles, usually one or more persons are as-signed.\\nExecutive sponsor— Person controlling the funding, providing the direction, and em-\\npowering the team members\\nFacilitator— Person guiding the team throughout the JAD process\\nScribe— Person designated to record all decisions\\nFull-time participants— Everyone involved in making decisions about the data ware-\\nhouse\\nOn-call participants— Persons affected by the project, but only in specific areas\\nObservers— Persons who would like to sit in on specific sessions without participating\\nin the decision making\\nReview of Existing Documentation\\nAlthough most of the requirements gathering will be done through interviews and group\\nsessions, you will be able to gather useful information from the review of existing docu-mentation. Review of existing documentation can be done by the project team without toomuch involvement from the users of the business units. Scheduling of the review of exist-ing documentation involves only the members of the project team.\\nDocumentation from User Departments. What can you get out of the existing\\ndocumentation? First, let us look at the reports and screens used by the users in the busi-ness areas that will be using the data warehouse. Y ou need to find out everything about thefunctions of the business units, the operational information gathered and used by theseREQUIREMENTS GATHERING METHODS 103', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1c904350-b075-4cc3-9d61-c129078f3485', embedding=None, metadata={'page_label': '126', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='users, what is important to them, and whether they use any of the existing reports for\\nanalysis. Y ou need to look at the user documentation for all the operational systems used.Y ou need to grasp what is important to the users.\\nThe business units usually have documentation on the processes and procedures in\\nthose units. How do the users perform their functions? Review in detail all the processesand procedures. Y ou are trying to find out what types of analyses the users in these busi-ness units are likely to be interested in. Review the documentation and then augment whatyou have learned from the documentation prepared from the interview sessions.\\nDocumentation from IT. The documentation from the users and the interviews with\\nthe users will give you information on the metrics used for analysis and the business di-mensions along which the analysis gets done. But from where do you get the data for themetrics and business dimensions? These will have to come from internal operational sys-tems. Y ou need to know what is available in the source systems.\\nWhere do you turn to for information available in the source systems? This is where\\nthe operational system DBAs (database administrators) and application experts from ITbecome very important for gathering data. The DBAs will provide you with all the datastructures, individual data elements, attributes, value domains, and relationships amongfields and data structures. From the information you have gathered from the users, youwill then be able to relate the user information to the source systems as ascertained fromthe IT personnel. \\nWork with your DBAs to obtain copies of the data dictionary or data catalog entries for\\nthe relevant source systems. Study the data structures, data fields, and relationships.Eventually, you will be populating the data warehouse from these source systems, so youneed to understand completely the source data, the source platforms, and the operatingsystems. \\nNow let us turn to the IT application experts. These professionals will give you the\\nbusiness rules and help you to understand and appreciate the various data elements fromthe source systems. Y ou will learn about data ownership, about people responsible for dataquality, and how data is gathered and processed in the source systems. Review the pro-grams and modules that make up the source systems. Look at the copy books inside theprograms to understand how the data structures are used in the programs. \\nREQUIREMENTS DEFINITION: SCOPE AND CONTENT\\nFormal documentation is often neglected in computer system projects. The project team\\ngoes through the requirements definition phase. They conduct the interviews and groupsessions. They review the existing documentation. They gather enough material to supportthe next phases in the system development life cycle. But they skip the detailed documen-tation of the requirements definition. \\nThere are several reasons why you should commit the results of your requirements de-\\nfinition phase. First of all, the requirements definition document is the basis for the nextphases. If project team members have to leave the project for any reason at all, the projectwill not suffer from people walking away with the knowledge they have gathered. The for-mal documentation will also validate your findings when reviewed with the users. \\nWe will come up with a suggested outline for the formal requirements definition docu-\\nment. Before that, let us look at the types of information this document must contain. 104 DEFINING THE BUSINESS REQUIREMENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6b2faa67-3abf-4a8d-845a-450c6ca8db71', embedding=None, metadata={'page_label': '127', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Data Sources\\nThis piece of information is essential in the requirements definition document. Include all\\nthe details you have gathered about the source systems. Y ou will be using the source sys-tem data in the data warehouse. Y ou will collect the data from these source systems, mergeand integrate it, transform the data appropriately, and populate the data warehouse.\\nTypically, the requirements definition document should include the following informa-\\ntion:\\n/L50539Available data sources\\n/L50539Data structures within the data sources\\n/L50539Location of the data sources\\n/L50539Operating systems, networks, protocols, and client architectures\\n/L50539Data extraction procedures\\n/L50539Availability of historical data \\nData Transformation\\nIt is not sufficient just to list the possible data sources. Y ou will list relevant data structures\\nas possible sources because of the relationships of the data structures with the potentialdata in the data warehouse. Once you have listed the data sources, you need to determinehow the source data will have to be transformed appropriately into the type of data suit-able to be stored in the data warehouse.\\nIn your requirements definition document, include details of data transformation. This\\nwill necessarily involve mapping of source data to the data in the data warehouse. Indicatewhere the data about your metrics and business dimensions will come from. Describe themerging, conversion, and splitting that need to take place before moving the data into thedata warehouse.\\nData Storage\\nFrom your interviews with the users, you would have found out the level of detailed data\\nyou need to keep in the data warehouse. Y ou will have an idea of the number of data martsyou need for supporting the users. Also, you will know the details of the metrics and thebusiness dimensions.\\nWhen you find out about the types of analyses the users will usually do, you can deter-\\nmine the types of aggregations that must be kept in the data warehouse. This will give youinformation about additional storage requirements. \\nY our requirements definition document must include sufficient details about storage\\nrequirements. Prepare preliminary estimates on the amount of storage needed for detailedand summary data. Estimate how much historical and archived data needs to be in the datawarehouse. \\nInformation Delivery\\nY our requirements definition document must contain the following requirements on infor-\\nmation delivery to the users:REQUIREMENTS DEFINITION: SCOPE AND CONTENT 105', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='07c48e7f-beeb-4bbe-97c6-388c94265620', embedding=None, metadata={'page_label': '128', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Drill-down analysis\\n/L50539Roll-up analysis\\n/L50539Drill-through analysis\\n/L50539Slicing and dicing analysis\\n/L50539Ad hoc reports\\nInformation Package Diagrams\\nThe presence of information package diagrams in the requirements definition document\\nis the major and significant difference between operational systems and data warehousesystems. Remember that information package diagrams are the best approach for deter-mining requirements for a data warehouse.\\nThe information package diagrams crystallize the information requirements for the\\ndata warehouse. They contain the critical metrics measuring the performance of the busi-ness units, the business dimensions along which the metrics are analyzed, and the detailshow drill-down and roll-up analyses are done.\\nSpend as much time as needed to make sure that the information package diagrams are\\ncomplete and accurate. Y our data design for the data warehouse will be totally dependenton the accuracy and adequacy of the information package diagrams. \\nRequirements Definition Document Outline\\n1. Introduction. State the purpose and scope of the project. Include broad project jus-\\ntification. Provide an executive summary of each subsequent section.\\n2. General requirements descriptions. Describe the source systems reviewed. In-\\nclude interview summaries. Broadly state what types of information requirements areneeded in the data warehouse.\\n3. Specific requirements. Include details of source data needed. List the data trans-\\nformation and storage requirements. Describe the types of information delivery methodsneeded by the users.\\n4. Information packages. Provide as much detail as possible for each information\\npackage. Include in the form of package diagrams.\\n5. Other requirements. Cover miscellaneous requirements such as data extract fre-\\nquencies, data loading methods, and locations to which information must be delivered.\\n6. User expectations. State the expectations in terms of problems and opportunities.\\nIndicate how the users expect to use the data warehouse.\\n7. User participation and sign-off. List the tasks and activities in which the users are\\nexpected to participate throughout the development life cycle.\\n8. General implementation plan. At this stage, give a high-level plan for implemen-\\ntation.\\nCHAPTER SUMMARY\\n/L50539Unlike the requirements for an operational system, the requirements for a data\\nwarehouse are quite nebulous.\\n/L50539Business data is dimensional in nature and the users of the data warehouse think in\\nterms of business dimensions.106 DEFINING THE BUSINESS REQUIREMENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b3ab071a-54c9-4505-841e-0a5bdb94c9fb', embedding=None, metadata={'page_label': '129', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539A requirements definition for the data warehouse can, therefore, be based on busi-\\nness dimensions such as product, geography, time, and promotion. \\n/L50539Information packages—a new concept—are the backbone of the requirements defi-\\nnition. An information package records the critical measurements or facts and busi-ness dimensions along which the facts are normally analyzed.\\n/L50539Interviews and group sessions are standard methods for collecting requirements.\\n/L50539Key people to be interviewed or to be included in group sessions are senior execu-\\ntives (including the sponsors), departmental managers, business analysts, and oper-ational systems DBAs. \\n/L50539Review all existing documentation of related operational systems.\\n/L50539Scope and content of the requirements definition document include data sources,\\ndata transformation, data storage, information delivery, and information package di-agrams. \\nREVIEW QUESTIONS\\n1. What are the essential differences between defining requirements for operational\\nsystems and for data warehouses?\\n2. Explain business dimensions. Why and how can business dimensions be useful for\\ndefining requirements for the data warehouse?\\n3. What data does an information package contain?4. What are dimension hierarchies? Give three examples.5. Explain business metrics or facts with five examples.6. List the types of users who must be interviewed for collecting requirements. What\\ninformation can you expect to get from them? \\n7. In which situations can JAD methodology be successful for collecting require-\\nments?\\n8. Why are reviews of existing documents important? What can you expect to get out\\nof such reviews?\\n9. Various data sources feed the data warehouse. What are the pieces of information\\nyou need to get about data sources?\\n10. Name any five major components of the formal requirements definition docu-\\nment. Describe what goes into each of these components.\\nEXERCISES\\n1. Indicate if true or false:\\nA. Requirements definitions for a sales processing operational system and a sales\\nanalysis data warehouse are very similar.\\nB. Managers think in terms of business dimensions for analysis.C. Unit sales and product costs are examples of business dimensions.D. Dimension hierarchies relate to drill-down analysis.E. Categories are attributes of business dimensions.EXERCISES 107', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='00909b02-1080-4303-86a2-62a11f3abe40', embedding=None, metadata={'page_label': '130', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='F . JAD is a methodology for one-on-one interviews.\\nG. It is not always necessary to conduct preinterview research.H. The departmental users provide information about the company’ s overall direc-\\ntion.\\nI. Departmental managers are very good sources for information on data struc-\\ntures of operational systems.\\nJ. Information package diagrams are essential parts of the formal requirements de-\\nfinition document.\\n2. Y ou are the Vice President of Marketing for a nation-wide appliance manufacturer\\nwith three production plants. Describe any three different ways you will tend to an-alyze your sales. What are the business dimensions for your analysis?\\n3. BigBook, Inc. is a large book distributor with domestic and international distribu-\\ntion channels. The company orders from publishers and distributes publications toall the leading booksellers. Initially, you want to build a data warehouse to analyzeshipments that are made from the company’ s many warehouses. Determine the met-rics or facts and the business dimensions. Prepare an information package diagram. \\n4. Y ou are on the data warehouse project of AuctionsPlus.com, an Internet auction\\ncompany selling upscale works of art. Y our responsibility is to gather requirementsfor sales analysis. Find out the key metrics, business dimensions, hierarchies, andcategories. Draw the information package diagram.\\n5. Create a detailed outline for the formal requirements definition document for a data\\nwarehouse to analyze product profitability of a large department store chain.108 DEFINING THE BUSINESS REQUIREMENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f0ce0a33-d2d9-4c53-9497-12837af5d764', embedding=None, metadata={'page_label': '131', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 6\\nREQUIREMENTS AS THE DRIVING FORCE\\nFOR DATA WAREHOUSING\\nCHAPTER OBJECTIVES\\n/L50539Understand why business requirements are the driving force\\n/L50539Discuss how requirements drive every development phase\\n/L50539Specifically learn how requirements influence data design \\n/L50539Review the impact of requirements on architecture\\n/L50539Note the special considerations for ETL and metadata\\n/L50539Examine how requirements shape information delivery\\nIn the previous chapter, we discussed the requirements definition phase in detail. Y ou\\nlearned that gathering requirements for a data warehouse is not the same as defining therequirements for an operational system. We arrived at a new way of creating informationpackages to express the requirements. Finally, we put everything together and producedthe requirements definition document. \\nWhen you design and develop any system, it is obvious that the system must exactly\\nreflect what the users need to perform their business processes. They should have theproper GUI screens, the system must have the correct logic to perform the functions, andthe users must receive the required output screens and reports. Requirements definitionguides the whole process of system design and development. \\nWhat about the requirements definition for a data warehouse? If accurate require-\\nments definition is important for any operational system, it is many times more impor-tant for a data warehouse. Why? The data warehouse environment is an information de-livery system where the users themselves will access the data warehouse repository andcreate their own outputs. In an operational system, you provide the users with prede-fined outputs. \\nIt is therefore extremely important that your data warehouse contain the right elements\\nof information in the most optimal formats. Y our users must be able to find all the strate-\\n109Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='452598c5-905c-42b0-a6f8-f9cc6b3c9cee', embedding=None, metadata={'page_label': '132', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='gic information they would need in exactly the way they want it. They must be able to ac-\\ncess the data warehouse easily, run their queries, get results painlessly, and perform vari-ous types of data analysis without any problems.\\nIn a data warehouse, business requirements of the users form the single and most pow-\\nerful driving force. Every task that is performed in every phase in the development of thedata warehouse is determined by the requirements. Every decision made during the de-sign phase—whether it may be the data design, the design of the architecture, the config-uration of the infrastructure, or the scheme of the information delivery methods—is total-ly influenced by the requirements. Figure 6-1 depicts this fundamental principle.\\nBecause requirements form the primary driving force for every phase of the develop-\\nment process, you need to ensure especially that your requirements definition contains ad-equate details to support each phase. This chapter particularly highlights a few significantdevelopment activities and specifies how requirements must guide, influence, and directthese activities. Why is this kind of special attention necessary? When you gather businessrequirements and produce the requirements definition document, you must always bear inmind that what you are doing in this phase of the project is of immense importance toevery other phase. Y our requirements definition will drive every phase of the project, soplease pay special attention. \\nDATA DESIGN\\nIn the data design phase, you come up with the data model for the following data reposito-\\nries:110 REQUIREMENTS AS THE DRIVING FORCE FOR DATA WAREHOUSING\\nPLANNING               \\nAND        \\nMANAGE-\\nMENT\\nDESIGN\\nArchitecture \\nInfrastructure \\nData Acquisition     \\nData Storage            \\nInformation Delivery        MAIN-\\nTENANCE\\nDEPLOY-\\nMENTCONSTRUCTION\\nArchitecture \\nInfrastructure \\nData Acquisition      \\nData Storage            \\nInformation Delivery        \\nFigure 6-1 Business requirements as the driving force.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ca9da17e-9bdd-4f6f-a19c-ca5d911095db', embedding=None, metadata={'page_label': '133', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539The staging area where you transform, cleanse, and integrate the data from the\\nsource systems in preparation for loading into the data warehouse repository\\n/L50539The data warehouse repository itself\\nIf you are adopting the practical approach of building your data warehouse as a con-\\nglomeration of conformed data marts, your data model at this point will consist of the di-mensional data model for your first set of data marts. On the other hand, your companymay decide to build the large corporate-wide data warehouse first along with the initialdata mart fed by the large data warehouse. In this case, your data model will include boththe model for the large data warehouse and the data model for the initial data mart. \\nThese data models will form the blueprint for the physical design and implementation\\nof the data repositories. Y ou will be using these models for communicating among theteam members on what data elements will be available in the data warehouse and howthey will all fit together. Y ou will be walking through these data models with the users toinform them of the data content and the data relationships. The data models for individualdata marts play a strong and useful role in communication with the users. \\nWhich portions of the requirements definition drive the data design? To understand the\\nimpact of requirements on data design, imagine the data model as a pyramid of data con-tents as shown in Figure 6-2. The base of the pyramid represents the data model for theenterprise-wide data repository and the top half of the pyramid denotes the dimensionaldata model for the data marts. What do you need in the requirements definition to buildand meld the two halves of the pyramid? Two basic pieces of information are needed: thesource system data models and the information package diagrams. \\nThe data models of the current source systems will be used for the lower half. There-\\nfore, ensure that your requirements definition document contains adequate informationabout the components and the relationships of the source system data. In the previouschapter, we discussed information package diagrams in sufficient detail. Please take spe-cial care that the information package diagrams that are part of the requirements defini-DATA DESIGN 111\\nRELATIONAL   \\nMODELDIMEN-\\nSIONAL   \\nMODELInformation \\nPackage \\nDiagrams\\nEnterprise \\nData     \\nModelEnterprise Data \\nWarehouseData Marts \\n(Conformed / \\nDependent)\\nFigure 6-2 Requirements driving the data model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='465a1a53-047a-4a14-8673-4cec72503921', embedding=None, metadata={'page_label': '134', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='tion document truly reflect the actual business requirements. Otherwise, your data model\\nwill not signify what the users really want to see in the data warehouse.\\nStructure for Business Dimensions\\nIn the data models for the data marts, the business dimensions along which the users ana-\\nlyze the business metrics must be featured prominently. In the last chapter, while dis-cussing information package diagrams, we reviewed a few examples. In an informationpackage diagram, the business dimensions are listed as column headings. For example,look at the business dimensions for Automaker Sales in Figure 6-3, which is a partial re-production of the earlier Figure 5-5.\\nIf you create a data model for this data mart, the business dimensions as shown in the\\nfigure must necessarily be included in the model. The usefulness of the data mart is di-rectly related to the accuracy of the data model. To where does this lead you? It leads youto the paramount importance of having the appropriate dimensions and the right contentsin the information package diagrams. \\nStructure for Key Measurements\\nKey measurements are the metrics or measures that are used for business analysis and\\nmonitoring. Users measure performance by using and comparing key measurements. For112 REQUIREMENTS AS THE DRIVING FORCE FOR DATA WAREHOUSING\\nMetrics : Actual Sale Price, MSRP Sale Price, Options Price, Full Price, Dealer   \\nAdd-ons, Dealer Credits, Dealer Invoice, Down PaymentTime ProductPayment \\nMethodCustomer \\nDemo-\\ngraphics\\nYearDimensionsInformation Package Diagram :Automaker Sales\\nQuarter\\nMonth\\nDate\\nDay of \\nWeek\\nDay of \\nMonth\\nSeason\\nHoliday \\nFlagModel \\nName\\nModel \\nYear\\nPackage \\nStyling\\nProduct \\nLine\\nProduct \\nCategory\\nExterior \\nColor\\nInterior \\nColor\\nFirst YearFinance \\nType\\nTerm \\n(Months)\\nInterest \\nRate\\nAgentDealer\\nAge\\nGender\\nIncome \\nRange\\nMarital \\nStatus\\nHouse-\\nhold Size\\nVehicles \\nOwned\\nHome \\nValue\\nOwn or \\nRentDealer \\nName\\nCity\\nState\\nSingle \\nBrand Flag\\nDate First \\nOperationDIMENSIONAL  DATA  \\nMODEL\\nFigure 6-3 Business dimensions in the data model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='57c84c25-5727-4f46-a3d6-7e7d3c053930', embedding=None, metadata={'page_label': '135', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='automaker sales, the key measurements include actual sale price, MSRP sale price, op-\\ntions price, full price, and so on. Users measure their success in terms of the key measure-ments. They tend to make calculations and summarizations in terms of such metrics. \\nIn addition to getting query results based on any combination of the dimensions, the\\nfacts or metrics are used for analysis. When your users analyze the sales along the prod-uct, time, and location dimensions, they see the results displayed in the metrics such assale units, revenue, cost, and profit margin. In order for the users to the review the resultsin proper key measurements, you have to guarantee that the information package dia-grams you include as part of the requirements definition contain all the relevant key mea-surements.\\nBusiness dimensions and key measures form the backbone of the dimensional data\\nmodel. The structure of the data model is directly related to the number of business di-mensions. The data content of each business dimension forms part of the data model. Forexample, if an information package diagram has product, customer, time, and location asthe business dimensions, these four dimensions will be four distinct components in thestructure of the data model. In addition to the business dimensions, the group of key mea-surements also forms another distinct component of the data model. \\nLevels of Detail\\nWhat else must be reflected in the data model? To answer this question, let us scrutinize\\nhow your users plan to use the data warehouse for analysis. Let us take a specific exam-ple. The senior analyst wants to analyze the sales in the various regions. First he or shestarts with the total countrywide sales by product in this year. Then the next step is to viewtotal countrywide sales by product in individual regions during the year. Moving on, thenext step is to get a breakdown by quarters. After this step, the user may want to get com-parisons with the budget and with the prior year performance. \\nWhat we observe is that in this kind of analysis you need to provide drill down and roll\\nup facilities for analysis. Do you want to keep data at the lowest level of detail? If so,when your user desires to see countrywide totals for the full year, the system must do theaggregation during analysis while the user is waiting at the workstation. On the otherhand, do you have to keep the details for displaying data at the lowest levels, and sum-maries for displaying data at higher levels of aggregation?\\nThis discussion brings us to another specific aspect of requirements definition as it re-\\nlates to the data model. If you need summaries in your data warehouse, then your datamodel must include structures to hold details as well as summary data. If you can afford tolet the system sum up on the fly during analysis, then your data model need not have sum-mary structures. Find out about the essential drill down and roll up functions and includeenough particulars about the types of summary and detail levels of data your data ware-house must hold. \\nTHE ARCHITECTURAL PLAN\\nY ou know that data warehouse architecture refers to the proper arrangement of the archi-\\ntectural components for maximum benefit. How do you plan your data warehouse archi-tecture? Basically, every data warehouse is composed of pretty much the same compo-nents. Therefore, when you are planning the architecture, you are not inventing any newTHE ARCHITECTURAL PLAN 113', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8ab11117-2cc6-453d-b645-8420b571fe5a', embedding=None, metadata={'page_label': '136', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='components to go into your particular warehouse. Y ou are really sizing up each compo-\\nnent for your environment. Y ou are planning how all the components must be knit togeth-er so that they will work as an integrated system. \\nBefore we proceed further, let us recap the major architectural components as dis-\\ncussed in Chapter 2:\\n/L50539Source data\\nProduction dataInternal dataArchived dataExternal data\\n/L50539Data staging\\nData extractionData transformationData loading\\n/L50539Data storage\\n/L50539Information delivery\\n/L50539Metadata\\n/L50539Management and control\\nWhen you plan the overall architecture for your data warehouse, you will be setting\\nthe scope and contents of each of these components. For example, in your company allof the source data might fortunately reside on a single computing platform and also ona single relational database. If this were the case, then the data extraction component inthe architecture would be substantially smaller and straightforward. Again, if your com-pany decides on using just the facilities provided by the DBMS, such as alias definitionand comments features, for metadata storage, then your metadata component would besimple.\\nPlanning the architecture, therefore, involves reviewing each of the components in the\\nlight of your particular context, and setting the parameters. Also, it involves the interfacesamong the various components. How can the management and control module be de-signed to coordinate and control the functions of the different components? What is theinformation you need to do the planning? How will you know to size up each componentand provide the appropriate infrastructure to support it? Of course, the answer is business\\nrequirements. All the information you need to plan the architecture must come from the\\nrequirements definition. In the following subsections, we will explore the importance ofbusiness requirements for the architectural plan. We will take each component and reviewhow proper requirements drive the size and content of the data warehouse. \\nComposition of the Components\\nLet us review each component and ascertain what exactly is needed in the requirements\\ndefinition to plan for the data warehouse architecture. Again, remember that planning forthe architecture involves the determination of the size and content of each component. Inthe following list, the bulleted points under each component indicate the type of informa-tion that must be contained in the requirements definition to drive the architectural plan.114 REQUIREMENTS AS THE DRIVING FORCE FOR DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c41ade0b-015a-4143-9980-b85fb2d80821', embedding=None, metadata={'page_label': '137', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Source Data\\n/L50539Operational source systems\\n/L50539Computing platforms, operating systems, databases, files\\n/L50539Departmental data such as files, documents, and spreadsheets\\n/L50539External data sources\\nData Staging\\n/L50539Data mapping between data sources and staging area data structures\\n/L50539Data transformations\\n/L50539Data cleansing\\n/L50539Data integration\\nData Storage\\n/L50539Size of extracted and integrated data\\n/L50539DBMS features\\n/L50539Growth potential\\n/L50539Centralized or distributed\\nInformation Delivery\\n/L50539Types and number of users\\n/L50539Types of queries and reports\\n/L50539Classes of analysis\\n/L50539Front-end DSS applications\\nMetadata\\n/L50539Operational metadata\\n/L50539ETL (data extraction/transformation/loading) metadata\\n/L50539End-user metadata\\n/L50539Metadata storage\\nManagement and Contol\\n/L50539Data loading\\n/L50539External sources\\n/L50539Alert systems\\n/L50539End-user information delivery\\nFigure 6-4 provides a useful summary of the architectural components driven by re-\\nquirements. The figure indicates the impact of business requirements on the data ware-house architecture. \\nSpecial Considerations\\nHaving reviewed the impact of requirements on the architectural components in some de-\\ntail, we now turn our attention to a few functions that deserve special consideration. Weneed to bring out these special considerations because if these are missed in the require-ments definition, serious consequences will occur. When you are in the requirements def-inition phase, you have to pay special attention to these factors. THE ARCHITECTURAL PLAN 115', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a336c226-a070-4bcf-829a-a46c0c64f53a', embedding=None, metadata={'page_label': '138', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Data Extraction/Transformation/Loading (ETL). The activities that relate to\\nETL in a data warehouse are by far most time-consuming and human-intensive. Specialrecognition of the extent and complexity of these activities in the requirements will go along way in easing the pain while setting up the architecture. Let us separate out the func-tions and state the special considerations needed in the requirements definition.\\nData Extraction. Clearly identify all the internal data sources. Specify all the comput-\\ning platforms and source files from which the data is to be extracted. If you are going toinclude external data sources, determine the compatibility of your data structures withthose of the outside sources. Also indicate the methods for data extraction.\\nData Transformation. Many types of transformation functions are needed before data\\ncan be mapped and prepared for loading into the data warehouse repository. These func-tions include input selection, separation of input structures, normalization and denor-malization of source structures, aggregation, conversion, resolving of missing values,and conversions of names and addresses. In practice, this turns out to be a long andcomplex list of functions. Examine each data element planned to be stored in the datawarehouse against the source data elements and ascertain the mappings and transforma-tions. \\nData Loading. Define the initial load. Determine how often each major group of data\\nmust be kept up-to-date in the data warehouse. How much of the updates will be nightlyupdates? Does your environment warrant more than one update cycle in a day? How arethe changes going to be captured in the source systems? Define how the daily, weekly, andmonthly updates will be initiated and carried out.116 REQUIREMENTS AS THE DRIVING FORCE FOR DATA WAREHOUSING\\n/G4D/G61/G6E/G61/G67/G65/G6D/G65/G6E/G74/G20/G26/G20/G43/G6F/G6E/G74/G72/G6F/G6C\\n/G53/G6F/G75/G72/G63/G65/G20/G44/G61/G74/G61\\n/G44/G61/G74/G61/G20/G53/G74/G61/G67/G69/G6E/G67\\n/G44/G61/G74/G61/G20/G53/G74/G6F/G72/G61/G67/G65/G4D/G65/G74/G61/G64/G61/G74/G61\\nFigure 6-4 Impact of requirements on architecture.Information Delivery', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7b6c3c6d-6af2-4066-a367-f436edf97a0f', embedding=None, metadata={'page_label': '139', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Data Quality. Bad data leads to bad decisions. No matter how well you tune your data\\nwarehouse, and no matter how adeptly you provide queries and analysis functions to theusers, if the data quality of your data warehouse is suspect, the users will quickly lose con-fidence and flee the data warehouse. Even simple discrepancies can result in seriousrepercussions while making strategic decisions of far-reaching consequences. Data quali-ty in a data warehouse is sacrosanct. Therefore, right in the early phase of requirementsdefinition, identify potential sources of data pollution in the source systems. Also, beaware of all the possible types of data quality problems likely to be encountered in youroperational systems. Please note the following tips.\\nData Pollution Sources\\n/L50539System conversions and migrations\\n/L50539Heterogeneous systems integration\\n/L50539Inadequate database design of source systems\\n/L50539Data aging\\n/L50539Incomplete information from customers\\n/L50539Input errors\\n/L50539Internationalization/localization of systems\\n/L50539Lack of data management policies/procedures\\nTypes of Data Quality Problems\\n/L50539Dummy values in source system fields\\n/L50539Absence of data in source system fields\\n/L50539Multipurpose fields\\n/L50539Cryptic data\\n/L50539Contradicting data\\n/L50539Improper use of name and address lines\\n/L50539Violation of business rules\\n/L50539Reused primary keys\\n/L50539Nonunique identifiers\\nMetadata. Y ou already know that metadata in a data warehouse is not merely data dic-\\ntionary entries. Metadata in a data warehouse is much more than details that can be car-ried in a data dictionary or data catalog. Metadata acts as a glue to tie all the componentstogether. When data moves from one component to another, that movement is governed bythe relevant portion of metadata. When a user queries the data warehouse, metadata actsas the information resource to connect the query parameters with the database compo-nents. \\nEarlier, we had categorized the metadata in a data warehouse into three groups: opera-\\ntional, data extraction and transformation, and end-user. Figure 6-5 displays the impact ofbusiness requirements on the metadata architectural component. \\nIt is needless to reiterate the significance of the metadata component. Study the figure\\nand apply it to your data warehouse project. For each type of metadata, figure out howmuch detail would be necessary in your requirements definition. Have sufficient detail toenable vital decisions such as choosing the type of metadata repository and reckoningwhether the repository must be centralized or distributed.THE ARCHITECTURAL PLAN 117', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8a26f420-632a-46a7-9106-298dc41d51ec', embedding=None, metadata={'page_label': '140', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Tools and Products\\nWhen tools are mentioned in the data warehousing context, you probably think only of end-\\nuser tools. Many people do so. But for building and maintaining your data warehouse, youneed many types of tools to support the various components of the architecture. \\nAs we discuss the impact of requirements on the data warehouse architecture in this sec-\\ntion, we want to bring up the subject of tools and products for two reasons. First, require-ments do not directly impact the selection of tools. Do not select the tools based on re-quirements and then adjust the architecture to suit the tools. This is like putting the cartbefore the horse. Design the data warehouse architecture and then look for the proper toolsto support the architecture. A specific tool, ideally suited for the functions in one data ware-house, may be a complete misfit in another data warehouse. That is because the architec-tures are different. What do we mean by the statement that the architectures are different?Although the architectural components are generally the same ones in both the data ware-houses, the scope, size, content, and the make-up of each component are not the same. \\nThe second reason for mentioning tools and products is this. While collecting require-\\nments to plan the architecture, sometimes you may feel constrained to make the architec-ture suit the requirements. Y ou may think that you will not be able to design the type of ar-chitecture dictated by the requirements because appropriate tools to support that type ofarchitecture may not be available. Please note that there are numerous production-worthytools available in the market. We want to point out that once your architectural design iscompleted, you can obtain the most suitable third-party tools and products.\\nIn general, tools are available for the following functions:\\n/L50539Data Extraction and Transformation\\nMiddlewareData extractionData transformation118 REQUIREMENTS AS THE DRIVING FORCE FOR DATA WAREHOUSING\\nOPERATIONAL\\nEXTRACTION / \\nTRANSFORMATION\\nEND-USERSource system data \\nstructures, External \\ndata formats\\nData cleansing, \\nconversion, integration\\nQuerying, reporting, \\nAnalysis, OLAP, \\nSpecial Apps.\\nDATA WAREHOUSE \\nMETADATABUSINESS    \\nREQUIREMENTS\\nFigure 6-5 Impact of requirements on metadata.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='688fd89b-baea-474f-97e0-d6797897c8ea', embedding=None, metadata={'page_label': '141', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Data quality assurance\\nLoad image creation\\n/L50539Warehouse Storage\\nData martsMetadata\\n/L50539Information Access/Delivery\\nReport writersQuery processorsOLAPAlert systemsDSS applicationsData mining\\nDATA STORAGE SPECIFICATIONS\\nIf your company is adopting the top-down approach of developing the data warehouse,\\nthen you have to define the storage specifications for\\n/L50539The data staging area\\n/L50539The overall corporate data warehouse\\n/L50539Each of the dependent data marts, beginning with the first\\n/L50539Any multidimensional databases for OLAP\\nAlternatively, if your company opts for the bottom-up approach, you need specifica-\\ntions for\\n/L50539The data staging area\\n/L50539Each of the conformed data marts, beginning with the first\\n/L50539Any multidimensional databases for OLAP\\nTypically, the overall corporate data warehouse will be based on the relational model\\nsupported by a relational database management system (RDBMS). The data marts areusually structured on the dimensional model implemented using an RDBMS. Many ven-dors offer proprietary multidimensional database systems (MDDBs). Specification foryour MDDB will be based on your choice of vendor. The extent and sophistication of thestaging area depends on the complexity and breadth of data transformation, cleansing,and conversion. The staging area may just be a bunch of flat files or, at the other extreme,a fully developed relational database.\\nWhatever your choice of the database management system may be, that system will\\nhave to interact with back-end and front-end tools. The back-end tools are the products fordata transformation, data cleansing, and data loading. The front-end tools relate to infor-mation delivery to the users. If you are trying to find the best tools to suit your environ-ment, the chances are these tools may not be from the same vendors who supplied thedatabase products. Therefore, one important criterion for the database management sys-DATA STORAGE SPECIFICATIONS 119', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d5610da0-8d2f-4970-93ed-e0e5a9456a67', embedding=None, metadata={'page_label': '142', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='tem is that the system must be open. It must be compatible with the chosen back-end and\\nfront-end tools.\\nSo what are we saying about the impact of business requirements on the data storage\\nspecifications? Business requirements determine how robust and how open the databasesystems must be. While defining requirements, bear in mind their influence on data stor-age specifications and collect all the necessary details about the back-end and the front-end architectural components.\\nWe will next examine the impact of business requirements on the selection of the\\nDBMS and on estimating storage for the data warehouse.\\nDBMS Selection\\nIn the requirements definition phase, when you are interviewing the users and having for-\\nmal meetings with them, you are not particularly discussing the type of DBMS to be se-lected. However, many of the user requirements affect the selection of the proper DBMS.The relational DBMS products on the market are usually bundled with a set of tools forprocessing queries, writing reports, interfacing with other products, and so on. Y ourchoice of the DBMS may be conditioned by its tool kit component. And the business re-quirements are likely to determine the type of the tool kit component needed. Broadly, thefollowing elements of business requirements affect the choice of the DBMS:\\nLevel of user experience. If the users are totally inexperienced with database systems,\\nthe DBMS must have features to monitor and control runaway queries. On the otherhand, if many of your users are power users, then they will be formulating their ownqueries. In this case, the DBMS must support an easy SQL-type language interface. \\nTypes of queries. The DBMS must have a powerful optimizer if most of the queries\\nare complex and produce large result sets. Alternatively, if there is an even mix ofsimple and complex queries, there must be some sort of query management in thedatabase software to balance the query execution. \\nNeed for openness. The degree of openness depends on the back-end and front-end ar-\\nchitectural components and those, in turn, depend on the business requirements.\\nData loads. The data volumes and load frequencies determine the strengths in the\\nareas of data loading, recovery, and restart.\\nMetadata management. If your metadata component does not have to be elaborate,\\nthen a DBMS with an active data dictionary may be sufficient. Let your require-ments definition reflect the type and extent of the metadata framework.\\nData repository locations. Is your data warehouse going to reside in one central loca-\\ntion, or is it going to be distributed? The answer to this question will establishwhether the selected DBMS must support distributed databases.\\nData warehouse growth. Y our business requirements definition must contain informa-\\ntion on the estimated growth in the number of users, and in the number and com-plexity of queries. The growth estimates will have a direct relation to how the select-ed DBMS supports scalability. \\nStorage Sizing\\nHow big will your data warehouse be? How much storage will be needed for all the data\\nrepositories? What is the total storage size? Answers to these questions will impact the120 REQUIREMENTS AS THE DRIVING FORCE FOR DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ae96329e-c889-41b5-a001-063a62cbce25', embedding=None, metadata={'page_label': '143', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='type and size of storage medium. How do you find answers to these questions? Again, it\\ngoes back to business requirements. In the requirements definition, you must have enoughinformation to answer these questions.\\nLet us summarize. Y ou need to estimate the storage sizes for the following in the re-\\nquirements definition phase:\\nData staging area. Calculate storage estimates for the data staging area of the overall\\ncorporate data warehouse from the sizes of the source system data structures foreach business subject. Figure the data transformations and mapping into your calcu-lation. For the data marts, initially estimate the staging area storage based on thebusiness dimensions and metrics for the first data mart. \\nOverall corporate data warehouse. Estimate the storage size based on the data struc-\\ntures for each business subject. Y ou know that data in the data warehouse is storedby business subjects. For each business subject, list the various attributes, estimatetheir field lengths, and arrive at the calculation for the storage needed for that sub-ject. \\nData Marts, dependent or conformed. While defining requirements, you create in-\\nformation diagrams. A set of these diagrams constitutes a data mart. Each informa-tion diagram contains business dimensions and their attributes. The information di-agram also holds the metrics or business measurements that are meant for analysis.Use the details of the business dimensions and business measures found in the in-formation diagrams to estimate the storage size for the data marts. Begin with yourfirst data mart. \\nMultidimensional databases. These databases support OLAP or multidimensional\\nanalysis. How much online analytical processing (OLAP) is necessary for yourusers? The corporate data warehouse or the individual conformed or dependent datamart supplies the data for the multidimensional databases. Work out the details ofOLAP planned for your users and then use those details to estimate storage for thesemultidimensional databases.\\nINFORMATION DELIVERY STRATEGY\\nThe impact of business requirements on the information delivery mechanism in a data\\nwarehouse is straightforward. During the requirements definition phase, users tell youwhat information they want to retrieve from the data warehouse. Y ou record these require-ments in the requirements definition document. Y ou then provide all the desired featuresand content in the information delivery component. Does this sound simple and straight-forward? Although the impact appears to be straightforward and simple, there are severalissues to be considered. Many different aspects of the requirements impact various ele-ments of the information delivery component in different ways.\\nThe composition of the user community that is expected to use the data warehouse af-\\nfects the information delivery strategy. Are most of the users power users and analysts?Then the information strategy must be slanted toward providing potent analytical tools.Are many of the users expecting to receive preformatted reports and to run precomposedqueries? Then query and reporting facilities in the information delivery component mustbe strengthened. INFORMATION DELIVERY STRATEGY 121', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='02b2fb47-3595-4382-b66e-58e14031bd95', embedding=None, metadata={'page_label': '144', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The broad areas of the information delivery component directly impacted by business\\nrequirements are: \\n/L50539Queries and reports\\n/L50539Types of analysis\\n/L50539Information distribution\\n/L50539Decision support applications\\n/L50539Growth and expansion\\nFigure 6-6 shows the impact of business requirements on information delivery. \\nA data warehouse exists for one reason and one reason alone—to provide strategic in-\\nformation to users. Information delivery tops the list of architectural components. Most ofthe other components are transparent to the users, but they see and experience what ismade available to them in the information delivery component. The importance of busi-ness requirements relating to information delivery cannot be overemphasized. \\nThe following subsections contain some valuable tips for requirements definition in\\norder to make the highly significant information delivery component effective and useful.Please study these carefully.\\nQueries and Reports \\nFind out who will be using predefined queries and preformatted reports. Get the specifi-\\ncations. Also, get the specifications for the production and distribution frequency for thereports. How many users will be running the predefined queries? How often?\\nThe second type of queries is not a set of predefined ones. In this case, the users formu-\\nlate their own queries and run them by themselves. Also in this class is the set of reports inwhich the users supply the report parameters and print fairly sophisticated reports them-selves. Get as many details of this type of queries and this type of report sets as you can.122 REQUIREMENTS AS THE DRIVING FORCE FOR DATA WAREHOUSING\\nQueries / Reports\\nSpecial Apps.OLAPAd Hoc ReportsComplex queriesOnline Intranet Internet Data Mining\\nInformation  Delivery  ComponentBUSINESS    \\nREQUIREMENTS\\nREQUIREMENTS  DEFINITION\\nON\\nUsers, Locations, Queries, Reports, Analysis\\nFigure 6-6 Impact of business requirements on information delivery.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8917a0aa-0ebc-46fc-bd08-682b5ae3c97f', embedding=None, metadata={'page_label': '145', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Power users may run complex queries, most of the time as part of an interactive analy-\\nsis session. Apart from analysis, do your power users need the ability to run single com-plex queries? \\nTypes of Analysis\\nMost data warehouses provide several features to run interactive sessions and perform\\ncomplex data analysis. Analysis encompassing drill-down and roll-up methods is fairlycommon. Review with your users all the types of analysis they would like to perform. Getinformation on the anticipated complexity of the types of analysis.\\nIn addition to the analysis performed directly on the data marts, most of today’ s data\\nwarehouse environments equip users with OLAP . Using the OLAP facilities, users canperform multidimensional analysis and obtain multiple views of the data from multidi-mensional databases. This type of analysis is called slicing and dicing. Estimate the natureand extent of the drill-down and roll-up facilities to be provided for. Determine how muchslicing and dicing has to be made available. \\nInformation Distribution\\nWhere are your users? Are they in one location? Are they in one local site connected by a\\nlocal area network (LAN)? Are they spread out on a wide area network (WAN)? Thesefactors determine how information must be distributed to your users. Clearly indicatethese details in the requirements definition.\\nIn many companies, users get access to information through the corporate intranet.\\nWeb-based technologies are used. If this is the case in your company, Web-based tech-nologies must be incorporated into the information delivery component. Let your require-ments definition be explicit about these factors.\\nDecision Support Applications\\nThese are specialized applications designed to support individual groups of users for spe-\\ncific purposes. An executive information system provides decision support to senior exec-utives. A data mining application is a special-purpose system to discover new patterns ofrelationships and predictive possibilities. We will discuss data mining in more detail inChapter 17.\\nThe data warehouse supplies data for these decision support applications. Sometimes\\nthe design and development of these ad hoc applications are outside the scope of the datawarehouse project. The only connection with the data warehouse is the feeding of the datafrom the data warehouse repository.\\nWhatever may be the development strategy for the specialized decision support appli-\\ncations in your company, make sure that the requirements definition spells out the details.If the data warehouse will be used just for data feeds, define the data elements and the fre-quencies of the data movements.\\nGrowth and Expansion\\nLet us say your data warehouse is deployed. Y ou have provided your users with abilities to\\nrun queries, print reports, perform analysis, use OLAP for complex analysis, and feed theINFORMATION DELIVERY STRATEGY 123', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='46af3f5d-470d-457e-9571-85a94b4fa07b', embedding=None, metadata={'page_label': '146', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='specialized applications with data. The information delivery component is complete and\\nworking well. Is that then the end of the effort? Y es, maybe just for the first iteration. \\nThe information delivery component continues to grow and expand. It continues to\\ngrow in the number and complexity of queries and reports. It expands in the enhance-ments to each part of the component. In your original requirements definition you need toanticipate the growth and expansion. Enough details about the growth and expansion caninfluence the proper design of the information delivery component, so collect enough de-tails to estimate the growth and enhancements.\\nCHAPTER SUMMARY\\n/L50539Accurate requirements definition in a data warehouse project is many times more\\nimportant than in other types of projects. Clearly understand the impact of businessrequirements on every development phase.\\n/L50539Business requirements condition the outcome of the data design phase.\\n/L50539Every component of the data warehouse architecture is strongly influenced by the\\nbusiness requirements.\\n/L50539In order to provide data quality, identify the data pollution sources, the prevalent\\ntypes of quality problems, and the means to eliminate data corruption early in therequirements definition phase itself. \\n/L50539Data storage specifications, especially the selection of the DBMS, are determined\\nby business requirements. Make sure you collect enough relevant details during therequirements phase.\\n/L50539Business requirements strongly influence the information delivery mechanism. Re-\\nquirements define how, when, and where the users will receive information from thedata warehouse.\\nREVIEW QUESTIONS\\n1. “In a data warehouse, business requirements of the users form the single and most\\npowerful driving force.” Do you agree? If you do, state four reasons why. If not, isthere any other such driving force? \\n2. How do accurate information diagrams turn into sound data models for your data\\nmarts? Explain briefly. \\n3. Name five architectural components that are strongly impacted by business re-\\nquirements. Explain the impact of business requirements on any one of those fivecomponents. \\n4. What is the impact of requirements on the selection of vendor tools and products?\\nDo requirements directly determine the choice of tools?\\n5. List any four aspects of information delivery that are directly impacted by busi-\\nness requirements. For two of those aspects, describe the impact.\\n6. How do business requirements affect the choice of DBMS? Describe any three of\\nthe ways in which the selection of DBMS is affected.124 REQUIREMENTS AS THE DRIVING FORCE FOR DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8dd3b6b1-d682-4a91-8c52-4603aa2b7912', embedding=None, metadata={'page_label': '147', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7. What are MDDBs? What types of business requirements determine the use of\\nMDDBs in a data warehouse? \\n8. How do requirements affect the choice of the metadata framework? Explain very\\nbriefly.\\n9. What types of user requirements dictate the granularity or the levels of detail in a\\ndata warehouse?\\n10. How do you estimate the storage size? What factors determine the size? \\nEXERCISES\\n1. Match the columns:\\n1. information package diagrams A. determine data extraction \\n2. need for drill-down B. provide OLAP3. data transformations C. provide data feed4. data sources D. influences load management5. data aging E. query management in DBMS6. sophisticated analysis F . low levels of data7. simple and complex queries G. larger staging area8. data volume H. influence data design 9. specialized DSS I. possible pollution source\\n10. corporate data warehouse J. data staging design\\n2. It is a known fact that data quality in the source systems is poor in your company.\\nY ou are assigned to be the Data Quality Assurance Specialist on the project team.Describe what details you will include in the requirements definition document toaddress the data quality problem.\\n3. As the analyst responsible for data loads and data refreshes, describe all the details\\nyou will look for and document during the requirements definition phase.\\n4. Y ou are the manager for the data warehouse project at a retail chain with stores all\\nacross the country and users in every store. How will you ensure that all the detailsnecessary to decide on the DBMS are gathered during the requirements phase?Write a memo to the Senior Analyst directly responsible to coordinate the require-ments definition phase. \\n5. Y ou are the Query Tools Specialist on the project team for a manufacturing compa-\\nny with the primary users based in the main office. These power users need sophis-ticated tools for analysis. How will you determine what types of information deliv-ery methods are needed? What kinds of details are to be gathered in therequirements definition phase? EXERCISES 125', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a7c0fcaf-2581-427f-a222-bc2abb59cffc', embedding=None, metadata={'page_label': '148', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 7\\nTHE ARCHITECTURAL COMPONENTS\\nCHAPTER OBJECTIVES\\n/L50539Understand data warehouse architecture\\n/L50539Learn about the architectural components\\n/L50539Review the distinguishing characteristics of data warehouse architecture \\n/L50539Examine how the architectural framework supports the flow of data\\n/L50539Comprehend what technical architecture means\\n/L50539Study the functions and services of the architectural components \\nUNDERSTANDING DATA WAREHOUSE ARCHITECTURE\\nIn Chapter 2, you were introduced to the building blocks of the data warehouse. At that\\nstage, we quickly looked at the list of components and reviewed each very briefly. InChapter 6, we revisited the data warehouse architecture and established that the businessrequirements form the principal driving force for all design and development, includingthe architectural plan.\\nIn this chapter, we want to review the data warehouse architecture from different per-\\nspectives. Y ou will study the architectural components in the manner in which they enablethe flow of data from the sources to the end-users. Then you will be able to look at eacharea of the architecture and examine the functions, procedures, and features in that area.That discussion will lead you into the technical architecture in those architectural areas.\\nArchitecture: Definitions\\nThe structure that brings all the components of a data warehouse together is known as the\\narchitecture. For example, take the case of the architecture of a school building. The archi-tecture of the building is not just the visual style. It includes the various classrooms, of-\\n127Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2e626fb7-388e-464c-a2dd-496f448175f7', embedding=None, metadata={'page_label': '149', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='fices, library, corridors, gymnasiums, doors, windows, roof, and a large number of other\\nsuch components. When all of these components are brought and placed together, thestructure that ties all of the components together is the architecture of the school building.If you can extend this comparison to a data warehouse, the various components of the datawarehouse together form the architecture of the data warehouse.\\nWhile building the school building, let us say that the builders were told to make the\\nclassrooms large. So they made the classrooms larger but eliminated the offices altogeth-er, thus constructing the school building with a faulty architecture. What went wrong withthe architecture? For one thing, all the necessary components were not present. Probably,the arrangement of the remaining components was also not right. Correct architecture iscritical for the success of your data warehouse. Therefore, in this chapter, we will take an-other close look at data warehouse architecture.\\nIn your data warehouse, architecture includes a number of factors. Primarily, it in-\\ncludes the integrated data that is the centerpiece. The architecture includes everything thatis needed to prepare the data and store it. On the other hand, it also includes all the meansfor delivering information from your data warehouse. The architecture is further com-posed of the rules, procedures, and functions that enable your data warehouse to work andfulfill the business requirements. Finally, the architecture is made up of the technologythat empowers your data warehouse.\\nWhat is the general purpose of the data warehouse architecture? The architecture pro-\\nvides the overall framework for developing and deploying your data warehouse; it is acomprehensive blueprint. The architecture defines the standards, measurements, generaldesign, and support techniques. \\nArchitecture in Three Major Areas\\nAs you already know, the three major areas in the data warehouse are:\\n/L50539Data acquisition\\n/L50539Data storage\\n/L50539Information delivery\\nIn Chapter 2, we identified the following major building blocks of the data warehouse:/L50539Source data\\n/L50539Data staging\\n/L50539Data storage\\n/L50539Information delivery\\n/L50539Metadata\\n/L50539Management and control\\nFigure 7-1 groups these major architectural components into the three areas. In this\\nchapter, we will study the architecture as it relates to these three areas. In each area, wewill consider the supporting architectural components. Each of the components has defi-nite functions and provides specific services. We will probe these functions and servicesand also examine the underlying technical architecture of the individual components inthese three areas. 128 THE ARCHITECTURAL COMPONENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='206449a6-b7f4-482d-a7b6-0bbfe77fa011', embedding=None, metadata={'page_label': '150', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Because of the importance of the architectural components, you will also receive ad-\\nditional details in later chapters. For now, for the three data warehouse areas, let us con-centrate on the functions, services, and technical architecture in these major areas ashighlighted in Figure 7-1.\\nDISTINGUISHING CHARACTERISTICS\\nAs an IT professional, when you were involved in the development of an OLTP system\\nsuch as order processing or inventory control, or sales reporting, were you consideringan architecture for each system? Although the term architecture is not usually mentionedin the context of operational systems, nevertheless, an underlying architecture does existfor these systems as well. For example, the architecture for such a system would includethe file conversions, initial population of the database, methods for data input, informa-tion delivery through online screens, and the entire suite of online and batch reporting.But for such systems we do not deal with architectural considerations so much and ingreat detail. If that is so for operational systems, what is so different and distinctiveabout the data warehouse that compels us to consider architecture in such elaboratedetail?\\nData warehouse architecture is wide, complex, and expansive. In a data warehouse,\\nthe architecture consists of distinct components. The architecture has distinguishingcharacteristics worth considering in detail. Before moving on to discuss the architectur-DISTINGUISHING CHARACTERISTICS 129\\nData Warehouse \\nDBMS\\nData MartsMDDBExternal Production   \\nReport/QueryOLAPData MiningSource DataInternal   Archived   \\nData Staging Data StorageMetadataManagement & Control\\nInformation Delivery\\nDATA ACQUISITIONDATA STORAGE\\nINFORMATION DELIVERY\\nFigure 7-1 Architectural components in the three major areas.Archived  Internal      Production', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='741d5ad8-17db-442f-a3cc-fbea3ad11c95', embedding=None, metadata={'page_label': '151', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='al framework itself, let us review the distinguishing characteristics of data warehouse ar-\\nchitecture.\\nDifferent Objectives and Scope\\nThe architecture has to support the requirements for providing strategic information.\\nStrategic information is markedly different from information obtained from operationalsystems. When you provide information from an operational application, the informationcontent and quantity per user session is limited. As an example, at a particular time, theuser is interested only in information about one customer and all the related orders. Froma data warehouse, however, the user is interested in obtaining large result sets. An exampleof a large result set from your data warehouse is all sales for the year broken down byquarters, products, and sales regions. \\nPrimarily, therefore, the data warehouse architecture must have components that will\\nwork to provide data to the users in large volumes in a single session. Basically, the extentto which a decision support system is different from an operational system directly trans-lates into just one essential principle: a data warehouse must have a different and moreelaborate architecture.\\nDefining the scope for a data warehouse is also difficult. How do you scope an opera-\\ntional system? Y ou consider the group of users, the range of functions, the data repository,and the output screens and reports. For a data warehouse with the architecture as the blue-print, what are all the factors you must consider for defining the scope?\\nThere are several sets of factors to consider. First, you must consider the number and\\nextent of the data sources. How many legacy systems are you going to extract the datafrom? What are the external sources? Are you planning to include departmental files,spreadsheets, and private databases? What about including the archived data? Scope ofthe architecture may again be measured in terms of the data transformations and integra-tion functions. In a data warehouse, data granularity and data volumes are also importantconsiderations. \\nY et another serious consideration is the impact of the data warehouse on the existing\\noperational systems. Because of the data extractions, comparisons, and reconciliation,you have to determine how much negative impact the data warehouse will have on theperformance of operational systems. When will your batch extracts be run and how willthey affect the production source systems? \\nData Content\\nThe “read-only” data in the data warehouse sits in the middle as the primary component in\\nthe architecture. In an operational system, although the database is important, this impor-tance does not measure up to that of a data warehouse data repository. Before data isbrought into your data warehouse and stored as read-only data, a number of functionsmust be performed. These exhaustive and critical functions do not compare with the dataconversion that happens in an operational system. \\nIn your data warehouse, you keep data integrated from multiple sources. After extract-\\ning the data, which by itself is an elaborate process, you transform the data, cleanse it, andintegrate it in a staging area. Only then you move the integrated data into the data ware-house repository as read-only data. Operational data is not “read-only” data. \\nFurther, your data warehouse architecture must support the storing of data grouped by130 THE ARCHITECTURAL COMPONENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5ef11b04-11d0-4d2b-9a99-85b6ad91a558', embedding=None, metadata={'page_label': '152', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='business subjects, not grouped by applications as in the case of operational systems. The\\ndata in your data warehouse does not represent a snapshot containing the values of thevariables as they are at the current time. This is different and distinct from most opera-tional systems. \\nWhen we mention historical data stored in the data warehouse, we are talking about\\nvery high data volumes. Most companies opt to keep data going back 10 years in the datawarehouse. Some companies want to keep even more, if the data is available. This is an-other reason why the data warehouse architecture must support high data volumes.\\nComplex Analysis and Quick Response\\nY our data warehouse architecture must support complex analysis of the strategic informa-\\ntion by the users. Information retrieval processes in an operational system dwindle incomplexity when compared to the use of information from a data warehouse. Most of theonline information retrieval during a session by a user is interactive analysis. A user doesnot run an isolated query, go away from the data warehouse, and come back much later forthe next single query. A session by the user is continuous and lasts a long time because theuser usually starts with a query at a high level, reviews the result set, initiates the nextquery looking at the data in a slightly different way, and so on.\\nY our data warehouse architecture must, therefore, support variations for providing\\nanalysis. Users must be able to drill down, roll up, slice and dice data, and play with“what-if ” scenarios. Users must have the capability to review the result sets in differentoutput options. Users are no longer content with textual result sets or results displayed intabular formats. Every result set in tabular format must be translated into graphical charts.\\nProvision of strategic information is meant for making rapid decisions and to deal with\\nsituations quickly. For example, let us say your Vice President of Marketing wants toquickly discover the reasons for the drop in sales for three consecutive weeks in the Cen-tral Region and make prompt decisions to remedy the situation. Y our data warehouse mustgive him or her the tools and information for a quick response to the problem.\\nY our data warehouse architecture must make it easy to make strategic decisions quickly.\\nThere must be appropriate components in the architecture to support quick response by theusers to deal with situations by using the information provided by your data warehouse. \\nFlexible and Dynamic\\nEspecially in the case of the design and development of a data warehouse, you do not\\nknow all business requirements up front. Using the technique for creating informationpackages, you are able to assess most of the requirements and dimensionally model thedata requirements. Nevertheless, the missing parts of the requirements show up after yourusers begin to use the data warehouse. What is the implication of this? Y ou have to makesure your data warehouse architecture is flexible enough to accommodate additional re-quirements as and when they surface.\\nAdditional requirements surface to include the missed items in the business require-\\nments. Moreover, business conditions themselves change. In fact, they keep on changing.Changing business conditions call for additional business requirements to be included inthe data warehouse. If the data warehouse architecture is designed to be flexible and dy-namic, then your data warehouse can cater to the supplemental requirements as and whenthey arise. DISTINGUISHING CHARACTERISTICS 131', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='62f47ccc-667f-42a8-9114-f0617883d352', embedding=None, metadata={'page_label': '153', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Metadata-driven\\nAs the data moves from the source systems to the end-users as useful, strategic informa-\\ntion, metadata surrounds the entire movement. The metadata component of the architec-ture holds data about every phase of the movement, and, in a true sense, makes the move-ment happen. \\nIn an operational system, there is no component that is equivalent to metadata in a data\\nwarehouse. The data dictionary of the DBMS of the operational system is just a faintshadow of the metadata in a data warehouse. So, in your data warehouse architecture, themetadata component interleaves with and connects the other components. Metadata in adata warehouse is so important that we have dedicated Chapter 9 in its entirety to discussmetadata.\\nARCHITECTURAL FRAMEWORK\\nEarlier in a previous section of this chapter, we grouped the architectural components as\\nbuilding blocks in the three distinct areas of data acquisition, data storage, and informa-tion delivery. In each of these broad areas of the data warehouse, the architectural compo-nents serve specific purposes. \\nArchitecture Supporting Flow of Data\\nNow we want to associate the components as forming a framework to condition and en-\\nable the flow of data from beginning to end. As you know very well, data that finallyreaches the end-user as useful strategic information begins as disparate data elements inthe various data sources. This collection of data from the various sources moves to thestaging area. What happens next? The extracted data goes through a detailed preparationprocess in the staging area before it is sent forward to the data warehouse to be properlystored. From the data warehouse storage, data transformed into useful information is re-trieved by the users or delivered to the user desktops as required. In a basic sense, whatthen is data warehousing? Do you agree that data warehousing just means taking all thenecessary source data, preparing it, storing it in suitable formats, and then delivering use-ful information to the end-users?\\nPlease look at Figure 7-2. This figure shows the flow of data from beginning to end and\\nalso highlights the architectural components enabling the flow of data as the data movesalong.\\nLet us now follow the flow of the data. At each stop along the passage, let us identify\\nthe architectural components. Some of the architectural components govern the flow ofdata from beginning to end. The management and control module is one such component.This module touches every step along the data movement. \\nWhat happens at critical points of the flow of data? What are the architectural compo-\\nnents, and how do these components enable the data flow?\\nAt the Data Source. Here the internal and external data sources form the source data\\narchitectural component. Source data governs the extraction of data for preparation andstorage in the data warehouse. The data staging architectural component governs the\\ntransformation, cleansing, and integration of data. 132 THE ARCHITECTURAL COMPONENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c77dc09f-cfc3-4e52-9a46-0e90d4ff20c6', embedding=None, metadata={'page_label': '154', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='In the Data Warehouse Repository. The data storage architectural component in-\\ncludes the loading of data from the staging area and also storing the data in suitable for-mats for information delivery. The metadata architectural component is also a storage\\nmechanism to contain data about the data at every point of the flow of data from begin-ning to end. \\nAt the User End. The information delivery architectural component includes depen-\\ndent data marts, special multidimensional databases, and a full range of query and report-ing facilities.\\nThe Management and Control Module\\nThis architectural component is an overall module managing and controlling the entire\\ndata warehouse environment. It is an umbrella component working at various levels andcovering all the operations. This component has two major functions: first to constantlymonitor all the ongoing operations, and next to step in and recover from problems whenthings go wrong. Figure 7-3 shows how the management component relates to and man-ages all of the data warehouse operations.\\nAt the outset in your data warehouse, you have operations relating to data acquisition.\\nThese include extracting data from the source systems either for full refresh or for incre-mental loads. Moving the data into the staging area and performing the data transforma-tion functions is also part of data acquisition. The management architectural componentARCHITECTURAL FRAMEWORK 133\\nData \\nWarehouse DBMS\\nData MartsMDDBExternal Production   \\nReport/QueryOLAP\\nData MiningSource DataInternal   Archived   \\nData Staging Data StorageMetadataManagement & ControlInformation Delivery\\nFigure 7-2 Architectural framework supporting the flow of data.Archived  Internal      ProductionInformation Delivery', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='08f7e220-7125-4f38-a3d2-bcad5e00f1e0', embedding=None, metadata={'page_label': '155', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='manages and controls these data acquisition functions, ensuring that extracts and transfor-\\nmations are carried out correctly and in a timely fashion. \\nThe management module also manages backing up significant parts of the data ware-\\nhouse and recovering from failures. Management services include monitoring the growthand periodically archiving data from the data warehouse. This architectural componentalso governs data security and provides authorized access to the data warehouse. Also, themanagement component interfaces with the end-user information delivery component toensure that information delivery is carried out properly. \\nOnly a few tools specially designed for data warehouse administration are presently\\navailable. Generally, data warehouse administrators perform the functions of the manage-ment and control component by using the tools available in the data warehouse DBMS. \\nTECHNICAL ARCHITECTURE\\nWe have already reviewed the various components of the data warehouse architecture in a\\nfew different ways. First, we grouped the components into the three major areas of dataacquisition, data storage, and information delivery. Then, we explored the distinguishingcharacteristics of the data warehouse architecture. We examined the architecture and high-lighted the distinguishing characteristics of the data warehouse architecture in comparisonwith that of any operational system. We also traced the flow of data through the data ware-house and linked individual architectural components to stations along the passage ofdata. 134 THE ARCHITECTURAL COMPONENTS\\nData \\nWarehouse DBMS\\nData MartsMDDBExternal Production   \\nReport/QueryOLAPData MiningSource DataInternal   Archived   \\nData Staging Data StorageMetadataMANAGEMENT & CONTROL\\nInformation Delivery\\nFigure 7-3 The management and control component.Information DeliveryArchived  Internal      Production', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6f66eedc-9211-4614-a9ec-51829a11e063', embedding=None, metadata={'page_label': '156', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Y ou now have a good grasp of what the term architecture means and what data ware-\\nhouse architecture consists of. Each component of the architecture is there to perform a setof definite functions and provide a group of specific services. When all the componentsperform their predefined functions and provide the required services, then the whole archi-tecture supports the data warehouse to fulfill the goals and business requirements. \\nThe technical architecture of a data warehouse is, therefore, the complete set of func-\\ntions and services provided within its components. The technical architecture also in-cludes the procedures and rules that are required to perform the functions and provide theservices. The technical architecture also encompasses the data stores needed for eachcomponent to provide the services. \\nLet us now make another significant distinction. The architecture is not the set of tools\\nneeded to perform the functions and provide the services. When we refer to the data ex-traction function within one of the architectural components, we are simply mentioningthe function itself and the various tasks associated with that function. Also, we are relatingthe data store for the staging area to the data extraction function because extracted data ismoved to the staging area. Notice that there is no mention of any tools for performing thefunction. Where do the tools fit in? What are the tools for extracting the data? What aretools in relation to the architecture? Tools are the means to implement the architecture.\\nThat is why you must remember that architecture comes first and the tools follow. \\nY ou will be selecting the tools most appropriate for the architecture of your data ware-\\nhouse. Let us take a very simple, perhaps unrealistic, example. Suppose the only datasource for your data warehouse is just four tables from a single centralized relational data-base. If so, what is the extent and scope of the data source component? What is magnitudeof the data extraction function? They are extremely limited. Do you then need sophisticat-ed third-party tools for data extraction? Obviously not. Taking the other extreme position,suppose your data sources consist of databases and files from fifty or more legacy systemsrunning on multiple platforms at remote sites. In this case, your data source architecturalcomponent and the data extraction function have very broad and complex scope. Y ou cer-tainly need to augment your in-house effort with proper data extraction tools from vendors.\\nIn the remaining sections of this chapter, we will consider the technical architecture of\\nthe components. We will discuss and elaborate on the types of functions, services, proce-dures, and data stores that are relevant to each architectural component. These are guide-lines. Y ou have to take these guidelines and review and adapt them for establishing the ar-chitecture for your data warehouse. When you establish the architecture for your datawarehouse, you will prepare the architectural plan that will include all the components.The plan will also state in detail the extent and complexity of all the functions, services,procedures, and data stores related to each architectural component. The architectural planwill serve as the blueprint for the design and development. It will also serve as a masterchecklist for your tool selection.\\nLet us now move on to consider the technical architecture in each of the three major\\nareas of the data warehouse.\\nData Acquisition\\nThis area covers the entire process of extracting data from the data sources, moving all the\\nextracted data to the staging area, and preparing the data for loading into the data ware-house repository. The two major architectural components identified earlier as part of thisarea are source data and data staging. The functions and services in this area relate toTECHNICAL ARCHITECTURE 135', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cb4179cd-5ed1-4b2e-a8a6-02e51213d52f', embedding=None, metadata={'page_label': '157', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='these two architectural components. The variations in the data sources have a direct im-\\npact on the extent and scope of the functions and services. \\nWhat happens in this area is of great importance in the development of your data ware-\\nhouse. The processes of data extraction, transformation, and loading are time-consuming,human-intensive, and very important. Therefore, Chapter 12 treats these processes ingreat depth. However, at this time, we will deal with these in sufficient length for you toplace all the architectural components in proper perspective. Figure 7-4 summarizes thetechnical architecture for data acquisition.\\nData Flow\\nFlow. In the data acquisition area, the data flow begins at the data sources and pauses at\\nthe staging area. After transformation and integration, the data is ready for loading intothe data warehouse repository.\\nData Sources. For the majority of data warehouses, the primary data source consists of\\nthe enterprise’ s operational systems. Many of the operational systems at several enterpris-es are still legacy systems. Legacy data resides on hierarchical or network databases. Y ouhave to use the appropriate fourth generation language of the particular DBMS to extractdata from these databases. Some of the more recent operational systems run on theclient/server architecture. Usually, these systems are supported by relational DBMSs.Here you may use an SQL-based language for extracting data. \\nA fairly large number of companies have adopted ERP (enterprise resource planning)\\nsystems. ERP data sources provide an advantage in that the data from these sources is al-ready consolidated and integrated. There could, however, be a few drawbacks to using136 THE ARCHITECTURAL COMPONENTS\\nExternal Production   Source DataInternal   Archived   \\nData Staging Management & Control\\nMetadata\\nRelational DB \\nor Flat FilesIntermediary  \\nFlat FilesDATA  EXTRACTION\\nDATA  STAGINGDATA  TRANSFORMATION\\nFigure 7-4 Data acquisition: technical architecture.Archived  Internal      Production', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9751264b-0136-4d14-8441-d94a37ebd95f', embedding=None, metadata={'page_label': '158', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ERP data sources. Y ou will have to use the ERP vendor’ s proprietary tool for data extrac-\\ntion. Also, most of the ERP offerings contain very large numbers of source data tables.\\nFor including data from outside sources, you will have to create temporary files to hold\\nthe data received from the outside sources. After reformatting and rearranging the data el-ements, you will have to move the data to the staging area.\\nIntermediary Data Stores. As data gets extracted from the data sources, it moves\\nthrough temporary files. Sometimes, extracts of homogeneous data from several sourceapplications are pulled into separate temporary files and then merged into another tempo-rary file before moving it to the staging area.\\nThe opposite process is also common. From each application, one or two large flat\\nfiles are created and then divided into smaller files and merged appropriately before mov-ing the data to the staging area. Typically, the general practice is to use flat files to extractdata from operational systems. \\nStaging Area. This is the place where all the extracted data is put together and prepared\\nfor loading into the data warehouse. The staging area is like an assembly plant or a con-struction area. In this area, you examine each extracted file, review the business rules,perform the various data transformation functions, sort and merge data, resolve inconsis-tencies, and cleanse the data. When the data is finally prepared either for an enterprise-wide data warehouse or one of the conformed data marts, the data temporarily resides inthe staging area repository awaiting to be loaded into the data warehouse repository.\\nIn a large number of data warehouses, data in the staging area is kept in sequential or\\nflat files. These flat files, however, contain the fully integrated and cleansed data in appro-priate formats ready for loading. Typically, these files are in the formats that could beloaded by the utility tools of the data warehouse RDBMS. Now more and more stagingarea data repositories are becoming relational databases. The data in such staging areasare retained for longer periods. Although extracts for loading may be easily obtained fromrelational databases with proper indexes, creating and maintaining these relational data-bases involves overhead for index creation and data migration from the source systems. \\nThe staging area may contain data at the lowest grain to populate tables containing\\nbusiness measurements. It is also common for aggregated data to be kept in the stagingarea for loading. The other types of data kept in the staging area relate to business dimen-sions such as product, time, sales region, customer, and promotional schemes. \\nFunctions and Services. Please review the general list of functions and services\\ngiven in this section. The list relates to the data acquisition area and covers the functionsand services in three groups. This is a general list. It does not indicate the extent or com-plexity of each function or service. For the technical architecture of your data warehouse,you have to determine the content and complexity of each function or service.\\nList of Functions and Services\\nData Extraction\\n/L50539Select data sources and determine the types of filters to be applied to individual\\nsources\\n/L50539Generate automatic extract files from operational systems using replication and oth-\\ner techniquesTECHNICAL ARCHITECTURE 137', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e8daed94-dabe-45f9-8047-6e8efc90fbd1', embedding=None, metadata={'page_label': '159', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Create intermediary files to store selected data to be merged later\\n/L50539Transport extracted files from multiple platforms\\n/L50539Provide automated job control services for creating extract files\\n/L50539Reformat input from outside sources\\n/L50539Reformat input from departmental data files, databases, and spreadsheets\\n/L50539Generate common application code for data extraction\\n/L50539Resolve inconsistencies for common data elements from multiple sources \\nData Transformation\\n/L50539Map input data to data for data warehouse repository \\n/L50539Clean data, deduplicate, and merge/purge\\n/L50539Denormalize extracted data structures as required by the dimensional model of the\\ndata warehouse\\n/L50539Convert data types\\n/L50539Calculate and derive attribute values\\n/L50539Check for referential integrity\\n/L50539Aggregate data as needed\\n/L50539Resolve missing values\\n/L50539Consolidate and integrate data\\nData Staging\\n/L50539Provide backup and recovery for staging area repositories\\n/L50539Sort and merge files\\n/L50539Create files as input to make changes to dimension tables\\n/L50539If data staging storage is a relational database, create and populate database\\n/L50539Preserve audit trail to relate each data item in the data warehouse to input source \\n/L50539Resolve and create primary and foreign keys for load tables\\n/L50539Consolidate datasets and create flat files for loading through DBMS utilities\\n/L50539If staging area storage is a relational database, extract load files\\nData Storage\\nThis area covers the process of loading the data from the staging area into the data ware-\\nhouse repository. All functions for transforming and integrating the data are completed inthe data staging area. The prepared data in the data warehouse is like the finished productthat is ready to be stacked in an industrial warehouse. \\nEven before loading data into the data warehouse, metadata, which is another compo-\\nnent of the architecture, is already active. During the data extraction and data transforma-tion stages themselves, the metadata repository gets populated. Figure 7-5 shows a sum-marized view of the technical architecture for data storage.\\nData Flow\\nFlow. For data storage, the data flow begins at the data staging area. The transformed\\nand integrated data is moved from the staging area to the data warehouse repository. 138 THE ARCHITECTURAL COMPONENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d4219e68-4305-4450-a24b-5aeb9d4d20b5', embedding=None, metadata={'page_label': '160', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='If the data warehouse is an enterprise-wide data warehouse being built in a top-down\\nfashion, then there could be movements of data from the enterprise-wide data warehouserepository to the repositories of the dependent data marts. Alternatively, if the data ware-house is a conglomeration of conformed data marts being built in a bottom-up manner,then the data movements stop with the appropriate conformed data marts. \\nData Groups. Prepared data waiting in the data staging area fall into two groups. The\\nfirst group is the set of files or tables containing data for a full refresh. This group of datais usually meant for the initial loading of the data warehouse. Occasionally, some datawarehouse tables may be refreshed fully. \\nThe other group of data is the set of files or tables containing ongoing incremental\\nloads. Most of these relate to nightly loads. Some incremental loads of dimension datamay be performed at less frequent intervals.\\nThe Data Repository. Almost all of today’ s data warehouse databases are relational\\ndatabases. All the power, flexibility, and ease of use capabilities of the RDBMS becomeavailable for the processing of data. \\nFunctions and Services. The general list of functions and services given in this sec-\\ntion is for your guidance. The list relates to the data storage area and covers the broadfunctions and services. This is a general list. It does not indicate the extent or complexityof each function or service. For the technical architecture of your data warehouse, youhave to determine the content and complexity of each function or service.TECHNICAL ARCHITECTURE 139\\nData MartsData StorageManagement & Control\\nMetadata\\nRelational DB \\nE-R Model\\nRelational DB \\nDimensional ModelINCREMENTAL  LOAD\\nBACKUP / \\nRECOVERYFULL  REFRESH\\nDATA  ARCHIVALSECURITY\\nFigure 7-5 Data storage: technical architecture.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eeeb7102-a778-497a-a8e4-00589b57f10d', embedding=None, metadata={'page_label': '161', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='List of Functions and Services\\n/L50539Load data for full refreshes of data warehouse tables\\n/L50539Perform incremental loads at regular prescribed intervals\\n/L50539Support loading into multiple tables at the detailed and summarized levels\\n/L50539Optimize the loading process\\n/L50539Provide automated job control services for loading the data warehouse\\n/L50539Provide backup and recovery for the data warehouse database\\n/L50539Provide security\\n/L50539Monitor and fine-tune the database\\n/L50539Periodically archive data from the database according to preset conditions\\nInformation Delivery\\nThis area spans a broad spectrum of many different methods of making information avail-\\nable to users. For your users, the information delivery component is the data warehouse.They do not come into contact with the other components directly. For the users, thestrength of your data warehouse architecture is mainly concentrated in the robustness andflexibility of the information delivery component. \\nThe information delivery component makes it easy for the users to access the informa-\\ntion either directly from the enterprise-wide data warehouse, from the dependent datamarts, or from the set of conformed data marts. Most of the information access in a datawarehouse is through online queries and interactive analysis sessions. Nevertheless, yourdata warehouse will also be producing regular and ad hoc reports.\\nAlmost all modern data warehouses provide for online analytical processing (OLAP).\\nIn this case, the primary data warehouse feeds data to proprietary multidimensional data-bases (MDDBs) where summarized data is kept as multidimensional cubes of informa-tion. The users perform complex multidimensional analysis using the information cubesin the MDDBs. Refer to Figure 7-6 for a summarized view of the technical architecturefor information delivery.\\nData Flow\\nFlow. For information delivery, the data flow begins at the enterprise-wide data ware-\\nhouse and the dependent data marts when the design is based on the top-down technique.When the design follows the bottom-up method, the data flow starts at the set of con-formed data marts. Generally, data transformed into information flows to the user desk-tops during query sessions. Also, information printed on regular or ad hoc reports reachesthe users. Sometimes, the result sets from individual queries or reports are held in propri-etary data stores of the query or reporting tool vendors. The stored information may beput to faster repeated use.\\nIn many data warehouses, data also flows into specialized downstream decision support\\napplications such as executive information systems (EIS) and data mining. The other morecommon flow of information is to proprietary multidimensional databases for OLAP . \\nService Locations. In your information delivery component, you may provide query\\nservices from the user desktop, from an application server, or from the database itself.This will be one of the critical decisions for your architecture design. 140 THE ARCHITECTURAL COMPONENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7b461476-302f-4f25-8c8c-d6b9c0de3bb5', embedding=None, metadata={'page_label': '162', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='For producing regular or ad hoc reports, you may want to include a comprehensive re-\\nporting service. This service will allow users to create and run their own reports. It willalso provide for standard reports to be run at regular intervals.\\nData Stores. For information delivery, you may consider the following intermediary\\ndata stores:\\n/L50539Proprietary temporary stores to hold results of individual queries and reports for re-\\npeated use\\n/L50539Data stores for standard reporting\\n/L50539Proprietary multidimensional databases \\nFunctions and Services. Please review the general list of functions and services\\ngiven below and use it as a guide to establish the information delivery component of yourdata warehouse architecture. The list relates to information delivery and covers the broadfunctions and services. Again, this is a general list. It does not indicate the extent or com-plexity of each function or service. For the technical architecture of your data warehouse,you have to determine the content and complexity of each function or service.\\n/L50539Provide security to control information access\\n/L50539Monitor user access to improve service and for future enhancements\\n/L50539Allow users to browse data warehouse content\\n/L50539Simplify access by hiding internal complexities of data storage from usersTECHNICAL ARCHITECTURE 141\\nReport/QueryOLAP\\nData MiningInformation DeliveryManagement & Control\\nMetadata\\nQUERY  OPTIMIZATION\\nMultidimensional  \\nDatabase\\nTemporary Result \\nSets\\nStandard Reporting \\nData StoresQUERY GOVERNMENT\\nCONTENT  BROWSE\\nSECURITY  CONTROL\\nSELF-SERVICE  REPORT  \\nGENERATION\\nFigure 7-6 Information delivery: technical architecture.Information Delivery', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='056e7d99-19c2-438b-825e-7df560612379', embedding=None, metadata={'page_label': '163', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Automatically reformat queries for optimal execution\\n/L50539Enable queries to be aware of aggregate tables for faster results\\n/L50539Govern queries and control runaway queries\\n/L50539Provide self-service report generation for users, consisting of a variety of flexible\\noptions to create, schedule, and run reports \\n/L50539Store result sets of queries and reports for future use\\n/L50539Provide multiple levels of data granularity\\n/L50539Provide event triggers to monitor data loading\\n/L50539Make provision for the users to perform complex analysis through online analytical\\nprocessing (OLAP) \\n/L50539Enable data feeds to downstream, specialized decisions support systems such as EIS\\nand data mining\\nCHAPTER SUMMARY\\n/L50539Architecture is the structure that brings all the components together.\\n/L50539Data warehouse architecture consists of distinct components with the read-only data\\nrepository as the centerpiece.\\n/L50539The architectural components support the functioning of the data warehouse in the\\nthree major areas of data acquisition, data storage, and information delivery.\\n/L50539Data warehouse architecture is wide, complex, expansive, and has several distin-\\nguishing characteristics. \\n/L50539The architectural framework enables the flow of data from the data sources at one\\nend and the user’ s desktop at the other.\\n/L50539The technical architecture of a data warehouse is the complete set of functions and\\nservices provided within its components. It includes the procedures and rules need-ed to perform the functions and to provide the services. It encompasses the datastores needed for each component to provide the services.\\nREVIEW QUESTIONS\\n1. What is your understanding of data warehouse architecture? Describe in one or\\ntwo paragraphs. \\n2. What are the three major areas in the data warehouse? Is this a logical division? If\\nso, why do you think so? Relate the architectural components to the three majorareas. \\n3. Name four distinguishing characteristics of data warehouse architecture. Describe\\neach briefly.\\n4. Trace the flow of data through the data warehouse from beginning to end.5. For information delivery, what is the difference between top-down and bottom-up\\napproaches to data warehouse implementation?\\n6. In which architectural component does OLAP fit in? What is the function of\\nOLAP?142 THE ARCHITECTURAL COMPONENTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='948a34e4-ab4f-40ee-a70d-e5936906da7a', embedding=None, metadata={'page_label': '164', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7. Define technical architecture of the data warehouse. How does it relate to the indi-\\nvidual architectural components?\\n8. List five major functions and services in the data storage area.9. What are the types of storage repositories in the data staging area?\\n10. List four major functions and services for information delivery. Describe each\\nbriefly.\\nEXERCISES\\n1. Indicate if true or false:\\nA. Data warehouse architecture is just an overall guideline. It is not a blueprint for\\nthe data warehouse.\\nB. In a data warehouse, the metadata component is unique, with no truly matching\\ncomponent in operational systems.\\nC. Normally, data flows from the data warehouse repository to the data staging area.D. The management and control component does not relate to all operations in a\\ndata warehouse.\\nE. Technical architecture simply means the vendor tools.F . SQL-based languages are used to extract data from hierarchical databases.G. Sorts and merges of files are common in the staging area.H. MDDBs are generally relational databases.I. Sometimes, results of individual queries are held in temporary data stores for re-\\npeated use.\\nJ. Downstream specialized applications are fed directly from the source data com-\\nponent.\\n2. Y ou have been recently promoted to administrator for the data warehouse of a na-\\ntionwide automobile insurance company. Y ou are asked to prepare a checklist forselecting a proper vendor tool to help you with the data warehouse administration.Make a list of the functions in the management and control component of your datawarehouse architecture. Use this list to derive the tool-selection checklist.\\n3. As the senior analyst responsible for data staging, you are responsible for the design\\nof the data staging area. If your data warehouse gets input from several legacy sys-tems on multiple platforms, and also regular feeds from two external sources, howwill you organize your data staging area? Describe the data repositories you willhave for data staging.\\n4. Y ou are the data warehouse architect for a leading national department store chain.\\nThe data warehouse has been up and running for nearly a year. Now the manage-ment has decided to provide the power users with OLAP facilities. How will you al-ter the information delivery component of your data warehouse architecture? Makerealistic assumptions and proceed.\\n5. Y ou recently joined as the data extraction specialist on the data warehouse project\\nteam developing a conformed data mart for a local but progressive pharmacy. Makea detailed list of functions and services for data extraction, data transformation, anddata staging. EXERCISES 143', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bac47d44-13ce-410e-9373-97390a506231', embedding=None, metadata={'page_label': '165', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 8\\nINFRASTRUCTURE AS THE FOUNDATION\\nFOR DATA WAREHOUSING\\nCHAPTER OBJECTIVES\\n/L50539Understand the distinction between architecture and infrastructure\\n/L50539Find out how the data warehouse infrastructure supports its architecture\\n/L50539Gain an insight into the components of the physical infrastructure\\n/L50539Review hardware and operating systems for the data warehouse\\n/L50539Study parallel processing options as applicable to the data warehouse\\n/L50539Discuss the server options in detail\\n/L50539Learn how to select the DBMS\\n/L50539Review the types of tools needed for the data warehouse\\nWhat is data warehouse infrastructure in relation to its architecture? What is the distinc-\\ntion between architecture and infrastructure? In what ways are they different? Why do wehave to study the two separately?\\nIn the previous chapter, we discussed data warehouse architecture in detail. We looked\\nat the various architectural components and studied them by grouping them into the threemajor areas of the data warehouse, namely, data acquisition, data storage, and informationdelivery. Y ou learned the elements that composed the technical architecture of each archi-tectural component.\\nIn this chapter, let us find out what infrastructure means and what it includes. We will\\ndiscuss each part of the data warehouse infrastructure. Y ou will understand the signifi-cance of infrastructure and master the techniques for creating the proper infrastructure foryour data warehouse. \\nINFRASTRUCTURE SUPPORTING ARCHITECTURE\\nConsider the architectural components. For example, let us take the technical architecture\\nof the data staging component. This part of the technical architecture for your data ware-\\n145Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c53e84fb-ee1d-4a74-93b2-b997059856ca', embedding=None, metadata={'page_label': '166', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='house does a number of things. First of all, it indicates that there is a section of the archi-\\ntecture called data staging. Then it notes that this section of the architecture contains anarea where data is staged before it is loaded into the data warehouse repository. Next, itdenotes that this section of the architecture performs certain functions and provides spe-cific services in the data warehouse. Among others, the functions and services includedata transformation and data cleansing. \\nLet us now ask a few questions. Where exactly is the data staging area? What are the\\nspecific files and databases? How do the functions get performed? What enables the ser-vices to be provided? What is the underlying base? What is the foundational structure? In-frastructure is the foundation supporting the architecture. Figure 8-1 expresses this fact ina simple manner. \\nWhat are the various elements needed to support the architecture? The foundational in-\\nfrastructure includes many elements. First, it consists of the basic computing platform.The platform includes all the required hardware and the operating system. Next, the data-base management system (DBMS) is an important element of the infrastructure. All othertypes of software and tools are also part of the infrastructure. What about the people andthe procedures that make the architecture come alive? Are these also part of the infra-structure? In a sense, they are. \\nData warehouse infrastructure includes all the foundational elements that enable the ar-\\nchitecture to be implemented. In summary, the infrastructure includes several elementssuch as server hardware, operating system, network software, database software, the LANand WAN, vendor tools for every architectural component, people, procedures, and train-ing. \\nThe elements of the data warehouse infrastructure may be classified into two cate-\\ngories: operational infrastructure and physical infrastructure. This distinction is importantbecause elements in each category are different in their nature and features compared tothose in the other category. First, we will go over the elements that may be grouped as op-erational infrastructure. The physical infrastructure is much wider and more fundamental.146 INFRASTRUCTURE AS THE FOUNDATION FOR DATA WAREHOUSING\\nData\\nAcquisi -\\ntionData\\nStorageInformation\\nAccessData Warehouse Architecture\\nFigure 8-1 Infrastructure supporting architecture.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8976d1ba-6e76-4543-95b9-31eec70dc64a', embedding=None, metadata={'page_label': '167', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='After gaining a basic understanding of the elements of the physical architecture, we will\\nspend a large portion of this chapter examining specific elements in greater detail.\\nOperational Infrastructure\\nTo understand operational infrastructure, let us once again take the example of data staging.\\nOne part of foundational infrastructure refers to the computing hardware and the relatedsoftware. Y ou need the hardware and software to perform the data staging functions andrender the appropriate services. Y ou need software tools to perform data transformations.Y ou need software to create the output files. Y ou need disk hardware to place the data in thestaging area files. But what about the people involved in performing these functions? Whatabout the business rules and procedures for the data transformations? What about the man-agement software to monitor and administer the data transformation tasks?\\nOperational infrastructure to support each architectural component consists of \\n/L50539People\\n/L50539Procedures\\n/L50539Training\\n/L50539Management software\\nThese are not the people and procedures needed for developing the data warehouse.\\nThese are the ones needed to keep the data warehouse going. These elements are as essen-tial as the hardware and software that keep the data warehouse running. They support themanagement of the data warehouse and maintain its efficiency.\\nData warehouse developers pay a lot of attention to the hardware and system software\\nelements of the infrastructure. It is right to do so. But operational infrastructure is oftenneglected. Even though you may have the right hardware and software, your data ware-house needs the operational infrastructure in place for proper functioning. Without appro-priate operational infrastructure, your data warehouse is likely to just limp along andcease to be effective. Pay attention to the details of your operational infrastructure.\\nPhysical Infrastructure\\nLet us begin with a diagram. Figure 8-2 highlights the major elements of physical infra-\\nstructure. What do you see in the diagram? As you know, every system, including yourdata warehouse, must have an overall platform on which to reside. Essentially, the plat-form consists of the basic hardware components, the operating system with its utility soft-ware, the network, and the network software. Along with the overall platform is the set oftools that run on the selected platform to perform the various functions and services of in-dividual architectural components.\\nWe will examine the elements of physical infrastructure in the next few sections. Deci-\\nsions about the hardware top the list of decisions you have to make about the infrastruc-ture of your data warehouse. Hardware decisions are not easy. Y ou have to consider manyfactors. Y ou have to ensure that the selected hardware will support the entire data ware-house architecture.\\nPerhaps we can go back to our mainframe days and get some helpful hints. As newer\\nmodels of the corporate mainframes were announced and as we ran out of steam on theINFRASTRUCTURE SUPPORTING ARCHITECTURE 147', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3053857d-2b48-4bc1-b022-e5ecb1b56f7d', embedding=None, metadata={'page_label': '168', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='current configuration, we stuck to two principles. First, we leveraged as much of the exist-\\ning physical infrastructure as possible. Next, we kept the infrastructure as modular as pos-sible. When needs arose and when newer versions became available at cheaper prices, weunplugged an existing component and plugged in the replacement.\\nIn your data warehouse, try to adopt these two principles. Y ou already have the hard-\\nware and operating system components in your company supporting the current opera-tions. How much of this can you use for your data warehouse? How much extra capacityis available? How much disk space can be spared for the data warehouse repository? Findanswers to these questions.\\nApplying the modular approach, can you add more processors to the server hardware?\\nExplore if you can accommodate the data warehouse by adding more disk units. Take aninventory of individual hardware components. Check which of these components need tobe replaced with more potent versions. Also, make a list of the additional components thathave to be procured and plugged in. \\nHARDWARE AND OPERATING SYSTEMS\\nHardware and operating systems make up the computing environment for your data ware-\\nhouse. All the data extraction, transformation, integration, and staging jobs run on the se-lected hardware under the chosen operating system. When you transport the consolidatedand integrated data from the staging area to your data warehouse repository, you make useof the server hardware and the operating system software. When the queries are initiatedfrom the client workstations, the server hardware, in conjunction with the database soft-ware, executes the queries and produces the results. \\nHere are some general guidelines for hardware selection, not entirely specific to hard-\\nware for the data warehouse.\\nScalability. When your data warehouse grows in terms of the number of users, the\\nnumber of queries, and the complexity of the queries, ensure that your selected hardwarecould be scaled up.\\nSupport. Vendor support is crucial for hardware maintenance. Make sure that the sup-\\nport from the hardware vendor is at the highest possible level.148 INFRASTRUCTURE AS THE FOUNDATION FOR DATA WAREHOUSING\\nHardwareNetwork         \\nSoftware DBMSOperating      \\nSystemDATA \\nACQUISITION \\nTOOLSDATA \\nSTAGING \\nTOOLSINFO. \\nDELIVERY \\nTOOLS\\nCOMPUTING PLATFORM\\nFigure 8-2 Physical infrastructure.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5b91c30d-19d8-4c12-9f5a-34f3e4a46925', embedding=None, metadata={'page_label': '169', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='V endor Reference. It is important to check vendor references with other sites using\\nhardware from this vendor. Y ou do not want to be caught with your data warehouse beingdown because of hardware malfunctions when the CEO wants some critical analysis to becompleted.\\nV endor Stability. Check on the stability and staying power of the vendor.\\nNext let us quickly consider a few general criteria for the selection of the operating\\nsystem. First of all, the operating system must be compatible with the hardware. A list ofcriteria follows.\\nScalability. Again, scalability is first on the list because this is one common feature of\\nevery data warehouse. Data warehouses grow, and they grow very fast. Along with thehardware and database software, the operating system must be able to support the increasein the number of users and applications.\\nSecurity. When multiple client workstations access the server, the operating system\\nmust be able to protect each client and associated resources. The operating system mustprovide each client with a secure environment. \\nReliability. The operating system must be able to protect the environment from appli-\\ncation malfunctions.\\nAvailability. This is a corollary to reliability. The computing environment must contin-\\nue to be available after abnormal application terminations.\\nPreemptive Multitasking. The server hardware must be able to balance the allocation\\nof time and resources among the multiple tasks. Also, the operating system must be ableto let a higher priority task preempt or interrupt another task as and when needed.\\nUse multithreaded approach. The operating system must be able to serve multiple re-\\nquests concurrently by distributing threads to multiple processors in a multiprocessorhardware configuration. This feature is very important because multiprocessor configura-tions are architectures of choice in a data warehouse environment. \\nMemory protection. Again, in a data warehouse environment, large numbers of\\nqueries are common. That means that multiple queries will be executing concurrently. Amemory protection feature in an operating system prevents one task from violating thememory space of another.\\nHaving reviewed the requirements for hardware and operating systems in a data ware-\\nhouse environment, let us try to narrow down the choices. What are the possible options?Please go through the following list of three common options.\\nMainframes\\n/L50539Leftover hardware from legacy applications\\n/L50539Primarily designed for OLTP and not for decision support applications\\n/L50539Not cost-effective for data warehousing\\n/L50539Not easily scalable\\n/L50539Rarely used for data warehousing when too much spare resources are available for\\nsmaller data marts \\nOpen System Servers\\n/L50539UNIX servers, the choice medium for most data warehouses\\n/L50539Generally robust\\n/L50539Adapted for parallel processingHARDWARE AND OPERATING SYSTEMS 149', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='22508b54-8fa6-48ea-af9e-01589ad76464', embedding=None, metadata={'page_label': '170', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='NT Servers\\n/L50539Support medium-sized data warehouses\\n/L50539Limited parallel processing capabilities\\n/L50539Cost-effective for medium-sized and small data warehouses\\nPlatform Options\\nLet us now turn our attention to the computing platforms that are needed to perform the sev-\\neral functions of the various components of the data warehouse architecture. A computingplatform is the set of hardware components, the operating system, the network, and the net-work software. Whether it is a function of an OLTP system or a decision support system likethe data warehouse, the function has to be performed on a computing platform.\\nBefore we get into a deeper discussion of platform options, let us get back to the func-\\ntions and services of the architectural components in the three major areas. Here is a quicksummary recap:\\nData Acquisition: data extraction, data transformation, data cleansing, data integration,\\nand data staging. \\nData Storage: data loading, archiving, and data management.\\nInformation Delivery: report generation, query processing, and complex analysis.\\nWe will now discuss platform options in terms of the functions in these three areas.\\nWhere should each function be performed? On which platforms? How could you opti-mize the functions?\\nSingle Platform Option. This is the most straightforward and simplest option for im-\\nplementing the data warehouse architecture. In this option, all functions from the back-end data extraction to the front-end query processing are performed on a single comput-ing platform. This was perhaps the earliest approach, when developers were implementingdata warehouses on existing mainframes, minicomputers, or a single UNIX-based server. \\nBecause all operations in the data acquisition, data storage, and information delivery\\nareas take place on the same platform, this option hardly ever encounters any compatibili-ty or interface problems. The data flows smoothly from beginning to end without any plat-form-to-platform conversions. No middleware is needed. All tools work in a single com-puting environment. \\nIn many companies, legacy systems are still running on mainframes or minis. Some of\\nthese companies have migrated to UNIX-based servers and others have moved over toERP systems in client/server environments as part of the transition to address the Y2Kchallenge. In any case, most legacy systems still reside on mainframes, minis, or UNIX-based servers. What is the relationship of the legacy systems to the data warehouse? Re-member, the legacy systems contribute the major part of the data warehouse data. If thesecompanies wish to adopt a single-platform solution, that platform of choice has to be amainframe, mini, or a UNIX-based server. \\nIf the situation in your company warrants serious consideration of the single-platform\\noption, then analyze the implications before making a decision. The single-platform solu-tion appears to be an ideal option. If so, why are not many companies adopting this optionnow? Let us examine the reasons.150 INFRASTRUCTURE AS THE FOUNDATION FOR DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1963f4db-5f84-48ef-99a8-506d4338b91d', embedding=None, metadata={'page_label': '171', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Legacy Platform Stretched to Capacity. In many companies, the existing legacy\\ncomputing environment may have been around for a couple of decades and already fullystretched to capacity. The environment may be at a point where it can no longer be up-graded further to accommodate your data warehouse. \\nNonavailability of Tools. Software tools form a large part of the data warehouse infra-\\nstructure. Y ou will clearly grasp this fact from the last few subsections of this chapter.Most of the tools provided by the numerous data warehouse vendors do not support themainframe or minicomputer environment. Without the appropriate tools in the infrastruc-ture, your data warehouse will fall apart. \\nMultiple Legacy Platforms. Although we have surmised that the legacy mainframe or\\nminicomputer environment may be extended to include data warehousing, the practicalfact points to a different situation. In most corporations, a combination of a few main-frame systems, an assortment of minicomputer applications, and a smattering of the new-er PC-based systems exist side by side. The path most companies have taken is frommainframes to minis and then to PCs. Figure 8-3 highlights the typical configuration.\\nIf your corporation is one of the typical enterprises, what can you do about a single-\\nplatform solution? Not much. With such a conglomeration of disparate platforms, a sin-gle-platform option having your data warehouse alongside all the other applications is justnot tenable.\\nCompany’s Migration Policy. This is another important consideration. Y ou very well\\nknow the varied benefits of the client/server architecture for computing. Y ou are alsoHARDWARE AND OPERATING SYSTEMS 151\\nMINI\\nUNIXMAINFRAME\\nFigure 8-3 Multiple platforms in a typical corporation.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='baf3873c-9e65-444b-979c-33a073f67ec1', embedding=None, metadata={'page_label': '172', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='aware of the fact that every company is changing to embrace this new computing para-\\ndigm by moving the applications from the mainframe and minicomputer platforms. Inmost companies, the policy on the usage of information technology does not permit theperpetuation of the old platforms. If your company has a similar policy, then you will notbe permitted to add another significant system such as your data warehouse on the oldplatforms. \\nHybrid Option. After examining the legacy systems and the more modern applica-\\ntions in your corporation, it is most likely that you will decide that a single-platform ap-proach is not workable for your data warehouse. This is the conclusion most companiescome to. On the other hand, if your company falls in the category where the legacy plat-form will accommodate your data warehouse, then, by all means, take the approach of asingle-platform solution. Again, the single-platform solution, if feasible, is an easier solu-tion.\\nFor the rest of us who are not that fortunate, we have to consider other options. Let us\\nbegin with data extraction, the first major operation, and follow the flow of data until it isconsolidated into load images and waiting in the staging area. We will now step throughthe data flow and examine the platform options.\\nData Extraction. In any data warehouse, it is best to perform the data extraction func-\\ntion from each source system on its own computing platform. If your telephone sales dataresides in a minicomputer environment, create extract files on the mini-computer itself fortelephone sales. If your mail order application executes on the mainframe using an IMSdatabase, then create the extract files for mail orders on the mainframe platform. It israrely prudent to copy all the mail order database files to another platform and then do thedata extraction. \\nInitial Reformatting and Merging. After creating the raw data extracts from the vari-\\nous sources, the extracted files from each source are reformatted and merged into a small-er number of extract files. Verification of the extracted data against source system reportsand reconciliation of input and output record counts take place in this step. Just like theextraction step, it is best to do this step of initial merging of each set of source extracts onthe source platform itself. \\nPreliminary Data Cleansing. In this step, you verify the extracted data from each data\\nsource for any missing values in individual fields, supply default values, and perform ba-sic edits. This is another step for the computing platform of the source system itself. How-ever, in some data warehouses, this type of data cleansing happens after the data from allsources are reconciled and consolidated. In either case, the features and conditions of datafrom your source systems dictate when and where this step must be performed for yourdata warehouse.\\nTransformation and Consolidation. This step comprises all the major data transfor-\\nmation and integration functions. Usually, you will use transformation software tools forthis purpose. Where is the best place to perform this step? Obviously, not in any individ-ual legacy platform. Y ou perform this step on the platform where your staging area re-sides. 152 INFRASTRUCTURE AS THE FOUNDATION FOR DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f702099c-4e4d-4a5e-aea1-aa69ddecd1dc', embedding=None, metadata={'page_label': '173', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Validation and Final Quality Check. This step of final validation and quality check is\\na strong candidate for the staging area. Y ou will arrange for this step to happen on thatplatform. \\nCreation of Load Images. This step creates load images for individual database files\\nof the data warehouse repository. This step almost always occurs in the staging area and,therefore, on the platform where the staging area resides.\\nFigure 8-4 summarizes the data acquisition steps and the associated platforms. Y ou will\\nnotice the options for the steps. Relate this to your own corporate environment and deter-mine where the data acquisition steps must take place. \\nOptions for the Staging Area. In the discussion of the data acquisition steps, we\\nhave highlighted the optimal computing platform for each step. Y ou will notice that thekey steps happen in the staging area. This is the place where all data for the data ware-house come together and get prepared. What is the ideal platform for the staging area? Letus repeat that the platform most suitable for your staging area depends on the status ofyour source platforms. Nevertheless, let us explore the options for placing the staging areaand come up with general guidelines. These will help you decide. Figure 8-5 shows thedifferent options for the staging area. Please study the figure and follow the amplificationof the options given in the subsections below.\\nIn One of Legacy Platforms. If most of your legacy data sources are on the same plat-\\nform and if extra capacity is readily available, then consider keeping your data stagingarea in that legacy platform. In this option, you will save time and effort in moving thedata across platforms to the staging area.HARDWARE AND OPERATING SYSTEMS 153\\nMAINFRAME\\nMINIUNIXUNIX or \\nOTHER\\nSOURCE DATA PLATFORMS STAGING AREA PLATFORMData Extraction\\nInitial \\nReformatting/Merging\\nPreliminary Data  \\nCleansingPreliminary Data Cleansing\\nTransformation / \\nConsolidation\\nValidation / Quality \\nCheck\\nLoad Image Creation         \\nFigure 8-4 Platforms for data acquisition.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ccf7de7b-ed26-4ab7-9ed9-627e039a0797', embedding=None, metadata={'page_label': '174', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='On the Data Storage Platform. This is the platform on which the data warehouse\\nDBMS runs and the database exists. When you keep your data staging area on this plat-form, you will realize all the advantages for applying the load images to the database. Y oumay even be able to eliminate a few intermediary substeps and apply data directly to thedatabase from some of the consolidated files in the staging area.\\nOn a Separate Optimal Platform. Y ou may review your data source platforms, exam-\\nine the data warehouse storage platform, and then decide that none of these platforms arereally suitable for your staging area. It is likely that your environment needs complex datatransformations. It is possible that you need to work through your data thoroughly tocleanse and prepare it for your data warehouse. In such circumstances, you need a sepa-rate platform to stage your data before loading to the database. \\nHere are some distinct advantages of a separate platform for data staging:\\n/L50539Y ou can optimize the separate platform for complex data transformations and data\\ncleansing. What do we mean by this? Y ou can gear up the neutral platform with allthe necessary tools for data transformation, data cleansing, and data formatting.\\n/L50539While the extracted data is being transformed and cleansed in the data staging\\narea, you need to keep the entire data content and ensure that nothing is lost on theway. Y ou may want to think of some tracking file or table to contain tracking en-tries. A separate environment is most conducive for managing the movement ofdata.\\n/L50539We talked about the possibility of having specialized tools to manipulate the data in\\nthe staging area. If you have a separate computing environment for the staging area,154 INFRASTRUCTURE AS THE FOUNDATION FOR DATA WAREHOUSING\\nMINIUNIX\\nUNIX or \\nOTHER\\nSOURCE DATA   \\nPLATFORMSDATA STORAGE  \\nPLATFORMSTAGING AREASTAGING AREA\\nUNIX or \\nOTHERSTAGING AREA\\nSEPARATE  \\nPLATFORMOption  3 Option  2 Option  1\\nFigure 8-5 Platform options for the staging area.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bdbca67c-2f4e-4352-8ce3-9dea17bdb45c', embedding=None, metadata={'page_label': '175', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='you could easily have people specifically trained on these tools running the separate\\ncomputing equipment. \\nData Movement Considerations. On whichever computing platforms the individ-\\nual steps of data acquisition and data storage happen, data has to move across platforms.Depending on the source platforms in your company and the choice of the platform fordata staging and data storage, you have to provide for data transportation across differentplatforms. \\nPlease review the following options. Figure 8-6 summarizes the standard options. Y ou\\nmay find that a single approach alone is not sufficient. Do not hesitate to have a balancedcombination of the different approaches. In each data movement across two computingplatforms, choose the option that is most appropriate for that environment. Brief explana-tions of the standard options follow.\\nShared Disk. This method goes back to the mainframe days. Applications running in\\ndifferent partitions or regions were allowed to share data by placing the common data on ashared disk. Y ou may adapt this method to pass data from one step to another for data ac-quisition in your data warehouse. Y ou have to designate a disk storage area and set it up sothat each of the two platforms recognizes the disk storage area as its own.\\nMass Data Transmission. In this case, transmission of data across platforms takes\\nplace through data ports. Data ports are simply interplatform devices that enable massivequantities of data to be transported from one platform to the other. Each platform must beconfigured to handle the transfers through the ports. This option calls for special hard-HARDWARE AND OPERATING SYSTEMS 155\\nMAINFRAME\\nMINIUNIXUNIX or\\nOTHER\\nSOURCE PLATFORM TARGET PLATFORMDATA MOVEMENT\\nOption 1 -Shared Disk\\nOption 2 -Mass Transmission\\nOption 3 -Realtime Connection\\nOption 4 -Manual MethodsHigh Volume Data\\nFigure 8-6 Data movement options.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='09a08139-f6c1-4840-8762-921dfbb3a0f7', embedding=None, metadata={'page_label': '176', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ware, software, and network components. There must also be sufficient network band-\\nwidth to carry high data volumes.\\nReal-Time Connection. In this option, two platforms establish connection in real time\\nso that a program running on one platform may use the resources of the other platform. Aprogram on one platform can write to the disk storage on the other. Also, jobs running onone platform can schedule jobs and events on the other. With the widespread adoption ofTCP/IP , this option is very viable for your data warehouse.\\nManual Methods. Perhaps these are the options of last resort. Nevertheless, these op-\\ntions are straightforward and simple. A program on one platform writes to an externalmedium such as tape or disk. Another program on the receiving platform reads the datafrom the external medium.\\nC/S Architecture for the Data Warehouse. Although mainframe and minicom-\\nputer platforms were utilized in the early implementations of data warehouses, by andlarge, today’ s warehouses are built using the client/server architecture. Most of these aremultitiered, second-generation client/server architectures. Figure 8-7 shows a typicalclient/server architecture for a data warehouse implementation.\\nThe data warehouse DBMS executes on the data server component. The data reposito-\\nry of the data warehouse sits on this machine. This server component is a major compo-nent and we want to dedicate the next section for a detailed discussion of it. \\nAs data warehousing technologies have grown substantially, you will now observe a\\nproliferation of application server components in the middle tier. Y ou will find applicationservers for a number of purposes. Here are the important ones:156 INFRASTRUCTURE AS THE FOUNDATION FOR DATA WAREHOUSING\\nDESKTOP  \\nCLIENT\\nAPPLICATION  \\nSERVERS\\nDATABASE  \\nSERVERSERVICE TYPES\\nPresentation Logic\\nPresentation Service\\nMiddleware / Connectivity / Control /\\nMetadata Management / Web Access /Authentication / Query - Report Management / OLAP \\nDBMS\\nPrimary Data Repository\\nFigure 8-7 Client/server architecture for the data warehouse.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='339c00fb-ea5b-406c-a323-ad5cbe8e18aa', embedding=None, metadata={'page_label': '177', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539To run middleware and establish connectivity\\n/L50539To execute management and control software\\n/L50539To handle data access from the Web\\n/L50539To manage metadata\\n/L50539For authentication\\n/L50539As front end \\n/L50539For managing and running standard reports\\n/L50539For sophisticated query management\\n/L50539For OLAP applications\\nGenerally, the client workstations still handle the presentation logic and provide the\\npresentation services. Let us briefly address the significant considerations for the clientworkstations. \\nConsiderations for Client Workstations. When you are ready to consider the con-\\nfigurations for the workstation machines, you will quickly come to realize that you needto cater to a variety of user types. We are only considering the needs at the workstationwith regard to information delivery from the data warehouse. A casual user is perhaps sat-isfied with a machine that can run a Web browser to access HTML reports. A serious ana-lyst, on the other hand, needs a larger and more powerful workstation machine. The othertypes of users between these two extremes need a variety of services.\\nDo you then come up with a unique configuration for each user? That will not be prac-\\ntical. It is better to determine a minimum configuration on an appropriate platform thatwould support a standard set of information delivery tools in your data warehouse. Applythis configuration for most of your users. Here and there, add a few more functions asnecessary. For the power users, select another configuration that would support tools forcomplex analysis. Generally, this configuration for power users also supports OLAP .\\nThe factors for consideration when selecting the configurations for your users’ work-\\nstations are similar to the ones for any operating environment. However, the main consid-eration for workstations accessing the data warehouse is the support for the selected set oftools. This is the primary reason for the preference of one platform over another.\\nUse this checklist while considering workstations:\\n/L50539Workstation operating system\\n/L50539Processing power\\n/L50539Memory\\n/L50539Disk storage\\n/L50539Network and data transport\\n/L50539Tool support\\nOptions as the Data Warehouse Matures. After all this discussion of the com-\\nputing platforms for your data warehouse, you might reach the conclusion that the plat-form choice is fixed as soon as the initial choices are made. It is interesting to note that asthe data warehouse in each enterprise matures, the arrangement of the platforms alsoevolves. Data staging and data storage may start out on the same computing platform. Astime goes by and more of your users begin to depend on your data warehouse for strategicHARDWARE AND OPERATING SYSTEMS 157', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='53a82b5a-231a-446f-8c89-7383fec07fbb', embedding=None, metadata={'page_label': '178', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='decision making, you will find that the platform choices may have to be recast. Figure 8-8\\nshows you what to expect as your data warehouse matures.\\nOptions in Practice. Before we leave this section, it may be worthwhile to take a\\nlook at the types of data sources and target platforms in use at different enterprises. An in-dependent survey has produced some interesting findings. Figure 8-9 shows the approxi-mate percentage distribution for the first part of the survey about the principal datasources. In Figure 8-10, you will see the distribution of the answers to the question aboutthe platforms the respondents use for the data storage component of their data warehous-es.\\nServer Hardware\\nSelecting the server hardware is among the most important decisions your data warehouse\\nproject team is faced with. Probably, for most warehouses, server hardware selection canbe a “bet your bottom dollar” decision. Scalability and optimal query performance are thekey phrases. \\nY ou know that your data warehouse exists for one primary purpose—to provide infor-\\nmation to your users. Ad hoc, unpredictable, complex querying of the data warehouse isthe most common method for information delivery. If your server hardware does not sup-port faster query processing, the entire project is in jeopardy. \\nThe need to scale is driven by a few factors. As your data warehouse matures, you will\\nsee a steep increase in the number of users and in the number of queries. The load willsimply shoot up. Typically, the number of active users doubles in six months. Again, as158 INFRASTRUCTURE AS THE FOUNDATION FOR DATA WAREHOUSING\\nSTAGE 1          \\nINITIALSTAGE 2           \\nGROWINGSTAGE 3    \\nMATUREDDesktop Clients\\nAppln.     \\nServer\\nData \\nWarehouse / Data StagingDesktop \\nClients\\nAppln.     Server\\nData Warehouse / Data MartData Staging  / Develop-mentDesktop Clients\\nAppln.     Servers\\nData MartsData Staging \\nDevelop-ment \\nData Warehouse / Data Mart\\nFigure 8-8 Platform options as the data warehouse matures.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='297e0661-2f86-42aa-927b-c459d88899b9', embedding=None, metadata={'page_label': '179', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='your data warehouse matures, you will be increasing the content by including more busi-\\nness subject areas and adding more data marts. Corporate data warehouses start at approx-imately 200 GB and some shoot up to a terabyte within 18–24 months.\\nHardware options for scalability and complex query processing consists of four types\\nof parallel architecture. Initially, parallel architecture makes the most sense. Shouldn’t aquery complete faster if you increase the number of processors, each processor workingHARDWARE AND OPERATING SYSTEMS 159\\nFigure 8-9 Principal data sources.25%\\n20%35%20%Main-\\nframe \\nlegacy \\ndatabase \\nsystems\\nMain-\\nframe \\nVSAM and \\nother filesMisc. \\nincluding \\noutside \\nsources\\nOther \\nmain-\\nframe \\nsources\\nFigure 8-10 Target platforms for data storage component.60% 20%20%\\nUNIX-based \\nclient/server \\nwith relational \\nDBMSMainframe \\nenvironment \\nwith relational \\nDBMS\\nOther techno-\\nlogies including \\nNT-based \\nclient/server', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ffc03ad5-2111-4a34-a2d2-cb35cf504aa0', embedding=None, metadata={'page_label': '180', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='on parts of the query simultaneously? Can you not subdivide a large query into separate\\ntasks and spread the tasks among many processors? Parallel processing with multiplecomputing engines does provide a broad range of benefits, but no single architecture doeseverything right. \\nIn Chapter 3, we reviewed parallel processing as one of the significant trends in data\\nwarehousing. We also briefly looked at three more common architectures. In this section,let us summarize the current parallel processing hardware options. Y ou will gain sufficientinsight into the features, benefits, and limitations of each of these options. By doing so,you will be able contribute your understanding to your project team for selecting the prop-er server hardware. \\nSMP (Symmetric Multiprocessing). Refer to Figure 8-11. \\nFeatures:\\n/L50539This is a shared-everything architecture, the simplest parallel processing machine.\\n/L50539Each processor has full access to the shared memory through a common bus.\\n/L50539Communication between processors occurs through common memory.\\n/L50539Disk controllers are accessible to all processors.\\nBenefits:\\n/L50539This is a proven technology that has been used since the early 1970s.\\n/L50539Provides high concurrency. Y ou can run many concurrent queries.\\n/L50539Balances workload very well.\\n/L50539Gives scalable performance. Simply add more processors to the system bus.\\n/L50539Being a simple design, you can administer the server easily.160 INFRASTRUCTURE AS THE FOUNDATION FOR DATA WAREHOUSING\\nShared Disks Shared MemoryCommon BusProcessor Processor Processor Processor\\nFigure 8-11 Server hardware option: SMP .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d1fe85b4-d987-4b5c-82a6-5ba375fe2b8d', embedding=None, metadata={'page_label': '181', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Limitations:\\n/L50539Available memory may be limited.\\n/L50539May be limited by bandwidth for processor-to-processor communication, I/O, and\\nbus communication.\\n/L50539Availability is limited; like a single computer with many processors.\\nY ou may consider this option if the size of your data warehouse is expected to be\\naround a two or three hundred gigabytes and concurrency requirements are reasonable.\\nClusters. Refer to Figure 8-12. \\nFeatures:\\n/L50539Each node consists of one or more processors and associated memory.\\n/L50539Memory is not shared among the nodes; it is shared only within each node.\\n/L50539Communication occurs over a high-speed bus.\\n/L50539Each node has access to the common set of disks.\\n/L50539This architecture is a cluster of nodes. \\nBenefits:\\n/L50539This architecture provides high availability; all data is accessible even if one node\\nfails. \\n/L50539Preserves the concept of one database.\\n/L50539This option is good for incremental growth.HARDWARE AND OPERATING SYSTEMS 161\\nProcessor Processor\\nShared\\nMemoryProcessor Processor\\nShared\\nMemory\\nCommon High Speed Bus\\nShared Disks\\nFigure 8-12 Server hardware option: cluster.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='59d9d720-3cd7-4058-b406-23cff36cfa7e', embedding=None, metadata={'page_label': '182', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Limitations:\\n/L50539Bandwidth of the bus could limit the scalability of the system. \\n/L50539This option comes with a high operating system overhead.\\n/L50539Each node has a data cache; the architecture needs to maintain cache consistency\\nfor internode synchronization. A cache is “work area” holding currently used data;main memory is like a big file cabinet stretching across the entire room.\\nY ou may consider this option if your data warehouse is expected to grow in well-\\ndefined increments.\\nMPP (Massively Parallel Processing). Refer to Figure 8-13. \\nFeatures:\\n/L50539This is a shared-nothing architecture.\\n/L50539This architecture is more concerned with disk access than memory access.\\n/L50539Works well with an operating system that supports transparent disk access.\\n/L50539If a database table is located on a particular disk, access to that disk depends entire-\\nly on the processor that owns it.\\n/L50539Internode communication is by processor-to-processor connection.\\nBenefits:\\n/L50539This architecture is highly scalable.\\n/L50539The option provides fast access between nodes.\\n/L50539Any failure is local to the failed node; improves system availability.\\n/L50539Generally, the cost per node is low.\\nLimitations:\\n/L50539The architecture requires rigid data partitioning.\\n/L50539Data access is restricted.162 INFRASTRUCTURE AS THE FOUNDATION FOR DATA WAREHOUSING\\nProcessor Processor Processor Processor\\nMemoryMemory Memory Memory\\nDisk Disk Disk Disk\\nFigure 8-13 Server hardware option: MPP .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3030631b-cb80-4daa-b193-7989bbc3d583', embedding=None, metadata={'page_label': '183', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Workload balancing is limited.\\n/L50539Cache consistency must be maintained.\\nConsider this option if you are building a medium-sized or large data warehouse in the\\nrange of 400–500 GB. For larger warehouses in the terabyte range, look for special archi-tectural combinations.\\nccNUMA or NUMA (Cache-coherent Nonuniform Memory Architecture).\\nRefer to Figure 8-14. \\nFeatures:\\n/L50539This is the newest architecture; was developed in the early 1990s.\\n/L50539The NUMA architecture is like a big SMP broken into smaller SMPs that are easier\\nto build.\\n/L50539Hardware considers all memory units as one giant memory. The system has a single\\nreal memory address space over the entire machine; memory addresses begin with 1on the first node and continue on the following nodes. Each node contains a directo-ry of memory addresses within that node.\\n/L50539In this architecture, the amount of time needed to retrieve a memory value varies\\nbecause the first node may need the value that resides in the memory of the thirdnode. That is why this architecture is called nonuniform memory access architec-ture. \\nBenefits:\\n/L50539Provides maximum flexibility. \\n/L50539Overcomes the memory limitations of SMP .\\n/L50539Better scalability than SMP .HARDWARE AND OPERATING SYSTEMS 163\\nProcessor Processor\\nDisksMemoryProcessor Processor\\nDisksMemory\\nFigure 8-14 Server hardware option: NUMA.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3b3da1cd-c378-48f3-8736-3df512fed8fa', embedding=None, metadata={'page_label': '184', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539If you need to partition your data warehouse database and run these using a central-\\nized approach, you may want to consider this architecture. Y ou may also place yourOLAP data on the same server.\\nLimitations:\\n/L50539Programming NUMA architecture is more complex than even with MPP .\\n/L50539Software support for NUMA is fairly limited.\\n/L50539Technology is still maturing.\\nThis option is a more aggressive approach for you. Y ou may decide on a NUMA ma-\\nchine consisting of one or two SMP nodes, but if your company is inexperienced in hard-ware technology, this option may not be for you.\\nDATABASE SOFTWARE\\nExamine the features of the leading commercial RDBMSs. As data warehousing becomes\\nmore prevalent, you would expect to see data warehouse features being included in thesoftware products. That is exactly what the database vendors are doing. Data-warehouse-related add-ons are becoming part of the database offerings. The database software thatstarted out for use in operational OLTP systems is being enhanced to cater to decisionsupport systems. DBMSs have also been scaled up to support very large databases.\\nSome RDBMS products now include support for the data acquisition area of the data\\nwarehouse. Mass loading and retrieval of data from other database systems have becomeeasier. Some vendors have paid special attention to the data transformation function.Replication features have been reinforced to assist in bulk refreshes and incremental load-ing of the data warehouse. \\nBit-mapped indexes could be very effective in a data warehouse environment to index\\non fields that have a smaller number of distinct values. For example, in a database tablecontaining geographic regions, the number of distinct region codes is few. But frequently,queries involve selection by regions. In this case, retrieval by a bit-mapped index on theregion code values can be very fast. Vendors have strengthened this type of indexing. Wewill discuss bit-mapped indexing further in Chapter 18. \\nApart from these enhancements, the more important ones relate to load balancing and\\nquery performance. These two features are critical in a data warehouse. Y our data ware-house is query-centric. Everything that can be done to improve query performance is mostdesirable. The DBMS vendors are providing parallel processing features to improve queryperformance. Let us briefly review the parallel processing options within the DBMS thatcan take full advantage of parallel server hardware.\\nParallel Processing Options\\nParallel processing options in database software are intended only for machines with\\nmultiple processors. Most of the current database software can parallelize a large num-ber of operations. These operations include the following: mass loading of data, fulltable scans, queries with exclusion conditions, queries with grouping, selection with dis-tinct values, aggregation, sorting, creation of tables using subqueries, creating and re-building indexes, inserting rows into a table from other tables, enabling constraints, star164 INFRASTRUCTURE AS THE FOUNDATION FOR DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bf3a21b9-8e90-4d00-a6a7-7497c5b29dd4', embedding=None, metadata={'page_label': '185', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='transformation (an optimization technique when processing queries against a STAR\\nschema), and so on. Notice that this an impressive list of operations that the RDBMScan process in parallel. \\nLet us now examine what happens when a user initiates a query at the workstation.\\nEach session accesses the database through a server process. The query is sent to theDBMS and data retrieval takes place from the database. Data is retrieved and the resultsare sent back, all under the control of the dedicated server process. The query dispatchersoftware is responsible for splitting the work, distributing the units to be performedamong the pool of available query server processes, and balancing the load. Finally, theresults of the query processes are assembled and returned as a single, consolidated resultset.\\nInterquery Parallelization. In this method, several server processes handle multiple\\nrequests simultaneously. Multiple queries may be serviced based on your server configu-ration and the number of available processors. Y ou may successfully take advantage of thisfeature of the DBMS on SMP systems, thereby increasing the throughput and supportingmore concurrent users.\\nHowever, interquery parallelism is limited. Let us see what happens here. Multiple\\nqueries are processed concurrently, but each query is still being processed serially by asingle server process. Suppose a query consists of index read, data read, sort, and join op-erations; these operations are carried out in this order. Each operation must finish beforethe next one can begin. Parts of the same query do not execute in parallel. To overcomethis limitation, many DBMS vendors have come up with versions of their products to pro-vide intraquery parallelization. \\nIntraquery Parallelization. We will use Figure 8-15 for our discussion of intraquery\\nparallelization, so please take a quick look and follow along. This will greatly help you inmatching up your choice of server hardware with your selection of RDBMS.\\nLet us say a query from one of your users consists of an index read, a data read, a data\\njoin, and a data sort from the data warehouse database. A serial processing DBMS willprocess this query in the sequence of these base operations and produce the result set.However, while this query is executing on one processor in the SMP system, other queriescan execute in parallel. This method is the interquery parallelization discussed above. Thefirst group of operations in Figure 8-15 illustrates this method of execution. \\nUsing the intraquery parallelization technique, the DBMS splits the query into the\\nlower-level operations of index read, data read, data join, and data sort. Then each one ofthese basic operations is executed in parallel on a single processor. The final result set isthe consolidation of the intermediary results. Let us review three ways a DBMS can pro-vide intraquery parallelization, that is, parallelization of parts of the operations within thesame query itself.\\nHorizontal Parallelism. The data is partitioned across multiple disks. Parallel process-\\ning occurs within each single task in the query, for example, data read, which is performedon multiple processors concurrently on different sets of data to be read from multipledisks. After the first task is completed from all of the relevant parts of the partitioned data,the next task of that query is carried out, and then the next one after that task, and so on.The problem with this approach is the wait until all the needed data is read. Look at CaseA in Figure 8-15.DATABASE SOFTWARE 165', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f5378810-b5af-45de-ba87-91add4a249d8', embedding=None, metadata={'page_label': '186', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Vertical Parallelism. This kind of parallelism occurs among different tasks, not just a\\nsingle task in a query as in the case of horizontal parallelism. All component query opera-tions are executed in parallel, but in a pipelined manner. This assumes that the RDBMShas the capability to decompose the query into subtasks; each subtask has all the opera-tions of index read, data read, join, and sort. Then each subtask executes on the data in se-rial fashion. In this approach, the database records are ideally processed by one step andimmediately given to the next step for processing, thus avoiding wait times. Of course, inthis method, the DBMS must possess a very high level of sophistication in decomposingtasks. Now, please look at Case B in Figure 8-15. \\nHybrid Method. In this method, the query decomposer partitions the query both hori-\\nzontally and vertically. Naturally, this approach produces the best results. Y ou will realizethe greatest utilization of resources, optimal performance, and high scalability. Case C inFigure 8-15 illustrates this method.\\nSelection of the DBMS\\nOur discussions of the server hardware and the DBMS parallel processing options must have\\nconvinced you that selection of the DBMS is most crucial. Y ou must choose the server hard-ware with the appropriate parallel architecture. Y our choice of the DBMS must match withthe selected server hardware. These are critical decisions for your data warehouse. \\nWhile discussing how business requirements drive the design and development of the\\ndata warehouse in Chapter 6, we briefly mentioned how requirements influence the selec-166 INFRASTRUCTURE AS THE FOUNDATION FOR DATA WAREHOUSING\\nInter-query Parallelization\\nIndex Read Data Read Join Sort\\nIntra-query Parallelization\\nExecution Time\\nSerial  Processing\\nCASE A :    \\nHorizontal \\nPartitioning\\nCASE B :          \\nVertical   \\nPartitioning\\nCASE C :            \\nHybrid     \\nMethod\\nFigure 8-15 Intraquery parallelization by DBMS.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eaeff0e0-6387-4fdb-82ba-e64d8c9fd4c7', embedding=None, metadata={'page_label': '187', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='tion of the DBMS. Apart from the criteria that the selected DBMS must have load balanc-\\ning and parallel processing options, the other key features listed below must be consideredwhen selecting the DBMS for your data warehouse.\\nQuery governor— to anticipate and abort runaway queries\\nQuery optimizer— to parse and optimize user queries\\nQuery management— to balance the execution of different types of queries\\nLoad utility— for high-performance data loading, recovery, and restart\\nMetadata management— with an active data catalog or dictionary\\nScalability— in terms of both number of users and data volumes\\nExtensibility— having hybrid extensions to OLAP databases\\nPortability— across platforms\\nQuery tool APIs— for tools from leading vendors\\nAdministration— providing support for all DBA functions\\nCOLLECTION OF TOOLS\\nThink about an OLTP application, perhaps a checking account system in a commercial\\nbank. When you, as a developer, designed and deployed the application, how many third-party software tools did you use to develop such an application? Of course, do not countthe programming language or the database software. We mean other third-party vendortools for data modeling, GUI design software, and so on. Y ou probably used just a few, ifany at all. Similarly, when the bank teller uses the application, she or he probably uses nothird-party software tools. \\nBut a data warehouse environment is different. When you, as a member of the project\\nteam, develop the data warehouse, you will use third-party tools for different phases of thedevelopment. Y ou may use code-generators for preparing in-house software for data ex-traction. When the data warehouse is deployed, your users will be accessing informationthrough third-party query tools and creating reports with report writers. Software tools arevery significant parts of the infrastructure in a data warehouse environment. \\nSoftware tools are available for every architectural component of the data warehouse.\\nFigure 8-16 shows the tool groups that support the various functions and services in a datawarehouse. \\nSoftware tools are extremely important in a data warehouse. As you have seen from\\nthis figure, tools cover all the major functions. Data warehouse project teams write only asmall part of the software in-house needed to perform these functions. Because the datawarehouse tools are so important, we will discuss these again in later chapters: data ex-traction and transformation tools in Chapter 12, data quality tools in Chapter 13, andquery tools in Chapter 14. Also, Appendix C provides guidelines for evaluating vendor so-lutions. When you get to the point of selecting tools for your data warehouse project, thatlist could serve as a handy reference.\\nAt this stage, let us introduce the types of software tools that are generally required in a\\ndata warehouse environment. For each type, we will briefly discuss the purpose and func-tions. \\nBefore we get to the types of software tools, let us reiterate an important maxim thatCOLLECTION OF TOOLS 167', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6e713794-9664-48d6-a26e-f74476334b2e', embedding=None, metadata={'page_label': '188', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='was mentioned earlier in the previous chapter. In that chapter we discussed the architec-\\ntural components and studied the functions and services of individual components. Go tothe next subsection and read about that important principle again. \\nArchitecture First, Then Tools\\nThe title of this subsection simply means this: ignore the tools; design the architecture\\nfirst; then, and only then, choose the tools to match the functions and services stipulatedfor the architectural components. Do the architecture first; select the tools later. \\nWhy is this principle sacred? Why is it not advisable to just buy the set of tools and\\nthen use the tools to build and to deploy your data warehouse? This appears to be an easysolution. The salespersons of the tool vendors promise success. Why would this not workin the end? Let us take an example. \\nLet us begin to design your information delivery architectural component. First of all,\\nthe business requirements are the driving force. Y our largest group of users is the group ofpower users. They would be creating their own reports. They would run their own queries.These users would constantly perform complex analysis consisting of drilling down, slic-ing and dicing of data, and extensive visualization of result sets. Y ou know these users arepower users. They need the most sophisticated information delivery component. The func-tions and services of the information delivery component must be very involved and pow-erful. But you have not yet established the information delivery architectural component.\\nHold it right there. Let us now say that the salesperson from XYZ Report Writer, Inc.\\nhas convinced you that their report generation tool is all you need for information deliveryin your data warehouse. Two of your competitors use it in their data warehouses. Y ou buythe tool and are ready to install it. What would be the fate of your power users? What iswrong with this scenario? The information delivery tool was selected before the architec-168 INFRASTRUCTURE AS THE FOUNDATION FOR DATA WAREHOUSING\\n/G44/G61/G74/G61/G20/G53/G74/G6F/G72/G61/G67/G65 /G44/G61/G74/G61/G20/G41/G63/G71/G75/G69/G73/G69/G74/G69/G6F/G6E /G49/G6E/G66/G6F/G72/G6D/G61/G74/G69/G6F/G6E/G20/G44/G65/G6C/G69/G76/G65/G72/G79\\nData Loading\\nLoad Image CreationQuality AssuranceTransformationExtractionSource\\nSystems\\nData\\nWarehouse /\\nData Marts\\nStaging\\nArea/G4F/G4C/G41/G50\\n/G52/G65/G70/G6F/G72/G74/G20/G57/G72/G69/G74/G65/G72/G73\\n/G44/G53/G53/G20/G41/G70/G70/G73\\n/G44/G61/G74/G61\\n/G4D/G69/G6E/G69/G6E/G67/G41/G6C/G65/G72/G74\\n/G53/G79/G73/G74/G65/G6D/G73Data Modeling/G4D/G69/G64/G64/G6C/G65/G77/G61/G72/G65 /G61/G6E/G64/G20/G43/G6F/G6E/G6E/G65/G63/G74/G69/G76/G69/G74/G79/G44/G61/G74/G61/G20/G57/G61/G72/G65/G68/G6F/G75/G73/G65/G20/G4D/G61/G6E/G61/G67/G65/G6D/G65/G6E/G74\\nFigure 8-16    Tools for your data warehouse.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4e1624d3-98b9-4b01-ad3b-27b73eec1da2', embedding=None, metadata={'page_label': '189', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='tural component was established. The tool did not meet the requirements as would have\\nbeen reflected in the architecture.\\nNow let us move on to review the types of software tools for your data warehouse.\\nAs mentioned earlier, more details will be added in the later chapters. These chapterswill also elaborate on individual tool types. In the following subsections, we mention thebasic purposes and features of the type of tool indicated by the title of each subsection. \\nData Modeling\\n/L50539Enable developers to create and maintain data models for the source systems and\\nthe data warehouse target databases. If necessary, data models may be created forthe staging area.\\n/L50539Provide forward engineering capabilities to generate the database schema.\\n/L50539Provide reverse engineering capabilities to generate the data model from the data\\ndictionary entries of existing source databases.\\n/L50539Provide dimensional modeling capabilities to data designers for creating STAR\\nschemas.\\nData Extraction\\n/L50539Two primary extraction methods are available: bulk extraction for full refreshes and\\nchange-based replication for incremental loads. \\n/L50539Tool choices depend on the following factors: source system platforms and data-\\nbases, and available built-in extraction and duplication facilities in the source sys-tems.\\nData Transformation\\n/L50539Transform extracted data into appropriate formats and data structures.\\n/L50539Provide default values as specified.\\n/L50539Major features include field splitting, consolidation, standardization, and deduplica-\\ntion.\\nData Loading\\n/L50539Load transformed and consolidated data in the form of load images into the data\\nwarehouse repository.\\n/L50539Some loaders generate primary keys for the tables being loaded.\\n/L50539For load images available on the same RDBMS engine as the data warehouse, pre-\\ncoded procedures stored on the database itself may be used for loading.\\nData Quality\\n/L50539Assist in locating and correcting data errors.\\n/L50539May be used on the data in the staging area or on the source systems directly.\\n/L50539Help resolve data inconsistencies in load images.COLLECTION OF TOOLS 169', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6bb8ad6a-4e91-4711-bb79-fb7635ce1c97', embedding=None, metadata={'page_label': '190', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Queries and Reports\\n/L50539Allow users to produce canned, graphic-intensive, sophisticated reports.\\n/L50539Help users to formulate and run queries.\\n/L50539Two main classifications are report writers, report servers.\\nOnline Analytical Processing (OLAP)\\n/L50539Allow users to run complex dimensional queries.\\n/L50539Enable users to generate canned queries.\\n/L50539Two categories of online analytical processing are multidimensional online analyti-\\ncal processing (MOLAP) and relational online analytical processing (ROLAP).MOLAP works with proprietary multidimensional databases that receive data feedsfrom the main data warehouse. ROLAP provides online analytical processing capa-bilities from the relational database of the data warehouse itself. \\nAlert Systems\\n/L50539Highlight and get user’ s attention based on defined exceptions.\\n/L50539Provide alerts from the data warehouse database to support strategic decisions.\\n/L50539Three basic alert types are: from individual source systems, from integrated enter-\\nprise-wide data warehouses, and from individual data marts. \\nMiddleware and Connectivity\\n/L50539Transparent access to source systems in heterogeneous environments.\\n/L50539Transparent access to databases of different types on multiple platforms.\\n/L50539Tools are moderately expensive but prove to be invaluable for providing interoper-\\nability among the various data warehouse components. \\nData Warehouse Management\\n/L50539Assist data warehouse administrators in day-to-day management.\\n/L50539Some tools focus on the load process and track load histories.\\n/L50539Other tools track types and number of user queries. \\nCHAPTER SUMMARY\\n/L50539Infrastructure acts as the foundation supporting the data warehouse architecture.\\n/L50539Data warehouse infrastructure consists of operational infrastructure and physical in-\\nfrastructure.\\n/L50539Hardware and operating systems make up the computing environment for the data\\nwarehouse.\\n/L50539Several options exist for the computing platforms needed to implement the various\\narchitectural components.170 INFRASTRUCTURE AS THE FOUNDATION FOR DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='15fb54ea-4f8b-439a-b466-088c2a942aa5', embedding=None, metadata={'page_label': '191', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Selecting the server hardware is a key decision. Invariably, the choice is one of the\\nfour parallel server architectures.\\n/L50539Parallel processing options are critical in the DBMS. Current database software\\nproducts are able to perform interquery and intraquery parallelization.\\n/L50539Software tools are used in the data warehouse for data modeling, data extraction,\\ndata transformation, data loading, data quality assurance, queries and reports, andonline analytical processing (OLAP). Tools are also used as middleware, alert sys-tems, and for data warehouse administration. \\nREVIEW QUESTIONS\\n1. What is the composition of the operational infrastructure of the data warehouse?\\nWhy is operational infrastructure equally as important as the physical infrastruc-ture? \\n2. List the major components of the physical infrastructure. Write two or three sen-\\ntences to describe each component. \\n3. Briefly describe any six criteria you will use for selecting the operating system for\\nyour data warehouse. \\n4. What are the platform options for the staging area? Compare the options and men-\\ntion the advantages and disadvantages.\\n5. What are the four common methods for data movement within the data ware-\\nhouse? Explain any two of these methods.\\n6. Write two brief paragraphs on the considerations for client workstations.7. What are the four parallel server hardware options? List the features, benefits, and\\nlimitations of any one of these options.\\n8. How have the RDBMS vendors enhanced their products for data warehousing?\\nDescribe briefly in one or two paragraphs.\\n9. What is intraquery parallelization by the DBMS? What are the three methods?\\n10. List any six types of software tools used in the data warehouse. Pick any three\\ntypes from your list and describe the features and the purposes.\\nEXERCISES\\n1. Match the columns:\\n1. operational infrastructure A. shared-nothing architecture \\n2. preemptive multitasking B. provides high concurrency3. shared disk C. single memory address space4. MPP D. operating system feature5. SMP E. vertical parallelism6. interquery parallelization F . people, procedures, training7. intraquery parallelization G. easy administration8. NUMA H. choice data warehouse platform EXERCISES 171', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c1fa2aa7-f584-4fbf-b1e8-66ddd443b0e9', embedding=None, metadata={'page_label': '192', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9. UNIX-based system I. optimize for data transformation\\n10. data staging area J. data movement option\\n2. In your company, all the source systems reside on a single UNIX-based platform,\\nexcept one legacy system on a mainframe computer. Analyze the platform optionsfor your data warehouse. Would you consider the single-platform option? If so,why? If not, why not?\\n3. Y ou are the manager for the data warehouse project of a nationwide car rental com-\\npany. Y our data warehouse is expected to start out in the 500 GB range. Examinethe options for server hardware and write a justification for choosing one.\\n4. As the administrator of the proposed data warehouse for a hotel chain with a lead-\\ning presence in ten eastern states, write a proposal describing the criteria you willuse to select the RDBMS for your data warehouse. Make your assumptions clear.\\n5. Y ou are the Senior Analyst responsible for the tools in the data warehouse of a large\\nlocal bank with branches in only one state. Make a list of the types of tools you willprovide for use in your data warehouse. Include tools for developers and users. De-scribe the features you will be looking for in each tool type. 172 INFRASTRUCTURE AS THE FOUNDATION FOR DATA WAREHOUSING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='abe8f3c3-f7fb-48c6-96c0-3d374363f947', embedding=None, metadata={'page_label': '193', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 9\\nTHE SIGNIFICANT ROLE OF METADATA\\nCHAPTER OBJECTIVES\\n/L50539Find out why metadata is so important\\n/L50539Understand who needs metadata and what types they need\\n/L50539Review metadata types by the three functional areas \\n/L50539Discuss business metadata and technical metadata in detail\\n/L50539Examine all the requirements metadata must satisfy\\n/L50539Understand the challenges for metadata management\\n/L50539Study options for providing metadata \\nWe discussed metadata briefly in earlier chapters. In Chapter 2, we considered metada-\\nta as one of the major building blocks for a data warehouse. We grouped metadata into thethree types, namely, operational, extraction and transformation, and end-user metadata.While discussing the major data warehousing trends in Chapter 3, we reviewed the indus-try initiatives to standardize metadata.\\nThis chapter deals with the topic of metadata in sufficient depth. We will attempt to re-\\nmove the fuzzy feeling about the exact meaning, content, and characteristics of metadata.We will also get an appreciation for why metadata is vitally important. Further, we willlook for practical methods to provide effective metadata in a data warehouse environment.\\nWHY METADATA IS IMPORTANT\\nLet us begin with a positive assumption. Assume that your project team has successfully\\ncompleted the development of the first data mart. Everything was done according toschedule. Y our management is pleased that the team finished the project under budget andcomfortably before the deadline. All the results proved out in comprehensive testing. Y ourdata warehouse is ready to be deployed. This is the big day.\\n173Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8d27abf7-edf3-405f-8b49-9e0807c45df3', embedding=None, metadata={'page_label': '194', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='One of your prominent users is sitting at the workstation poised to compose and run\\nthe first query. Before he or she touches the keyboard, several important questions cometo mind.\\n/L50539Are there any predefined queries I can look at?\\n/L50539What are the various elements of data in the warehouse?\\n/L50539Is there information about unit sales and unit costs by product?\\n/L50539How can I browse and see what is available?\\n/L50539From where did they get the data for the warehouse? From which source systems?\\n/L50539How did they merge the data from the telephone orders system and the mail orders\\nsystem? \\n/L50539How old is the data in the warehouse?\\n/L50539When was the last time fresh data was brought in?\\n/L50539Are there any summaries by month and product?\\nThese questions and several more like them are very valid and pertinent. What are the\\nanswers? Where are the answers? Can your user see the answers? How easy is it for theuser to get to the answers?\\nMetadata in a data warehouse contains the answers to questions about the data in the\\ndata warehouse. Y ou keep the answers in a place called the metadata repository. Even ifyou ask just a few of data warehousing practitioners or if you read just a few of the bookson data warehousing, you will receive seemingly different definitions for metadata. Hereis a sample list of definitions:\\n/L50539Data about the data\\n/L50539Table of contents for the data\\n/L50539Catalog for the data\\n/L50539Data warehouse atlas\\n/L50539Data warehouse roadmap\\n/L50539Data warehouse directory\\n/L50539Glue that holds the data warehouse contents together\\n/L50539Tongs to handle the data\\n/L50539The nerve center\\nSo, what exactly is metadata? Which one of these definitions comes closest to the\\ntruth? Let us take a specific example. Assume your user wants to know about the table orentity called Customer in your data warehouse before running any queries on the cus-\\ntomer data. What is the information content about Customer in your metadata repository?\\nLet us review the metadata element for the Customer entity as shown in Figure 9-1.\\nWhat do you see in the figure? The metadata element describes the entity called\\nCustomer residing the data warehouse. It is not just a description. It tells you more. It\\ngives more than the explanation of the semantics and the syntax. Metatada describes allthe pertinent aspects of the data in the data warehouse fully and precisely. Pertinent towhom? Pertinent primarily to the users and also to you as developer and part of the pro-ject team. 174 THE SIGNIFICANT ROLE OF METADATA', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='01b5beaa-9910-4e2f-afb1-6279c720cd71', embedding=None, metadata={'page_label': '195', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='In this chapter, we will explore why metadata has a very significant role in the data\\nwarehouse. We will find out the reasons why and how metadata is vital to the users andthe developers. Without metadata, your data warehouse will simply be a disjointed sys-tem. If metadata is so significant, how best can you provide it? We will discuss someavailable options and make some valid suggestions. \\nA Critical Need in the Data Warehouse\\nLet us first examine the need for metadata in a slightly general way. We will get more spe-\\ncific in later sections. In broad terms, proper metadata is absolutely necessary for using,building, and administering your data warehouse.\\nFor Using the Data Warehouse. There is one big difference between a data ware-\\nhouse and any operational system such as an order processing application. The differenceis in the usage—the information access. In an order processing application, how do yourusers get information? Y ou provide them with GUI screens and predefined reports. Theyget information about pending or back orders through the relevant screens. They get infor-mation about the total orders for the day from specific daily reports. Y ou created thescreens and you formatted the reports for the users. Of course, these were designed basedon specifications from the users. Nevertheless, the users themselves do not create thescreen formats or lay out the reports every time they need information.\\nIn marked contrast, users themselves retrieve information from the data warehouse. By\\nand large, users themselves create ad hoc queries and run these against the data ware-house. They format their own reports. Because of this major difference, before they canWHY METADATA IS IMPORTANT 175\\nEntity Name: Customer\\nAlias Names: Account, Client\\nDefinition:  A person or an organization that purchases goods or services from \\nthe company.\\nRemarks: Customer entity includes regular, current, and past customers.\\nSource Systems: Finished Goods Orders, Maintenance Contracts, Online Sales.\\nCreate Date: January 15, 1999\\nLast Update Date: January 21, 2001\\nUpdate Cycle: Weekly\\nLast Full Refresh Date: December 29, 2000\\nFull Refresh Cycle: Every six months\\nData Quality Reviewed: January 25, 2001\\nLast Deduplication: January 10, 2001\\nPlanned Archival: Every six months\\nResponsible User: Jane Brown\\nFigure 9-1 Metadata element for Customer entity.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8a24f12b-5079-4ed6-a15d-154fe536d725', embedding=None, metadata={'page_label': '196', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='create and run their queries, users need to know about the data in the data warehouse.\\nThey need metadata.\\nIn our operational systems, however, we do not really have any easy and flexible meth-\\nods for knowing the nature of the contents of the database. In fact, there is no great needfor user-friendly interfaces to the database contents. The data dictionary or catalog ismeant for IT uses only. \\nThe situation for a data warehouse is totally different. Y our data warehouse users need\\nto receive maximum value from your data warehouse. They need sophisticated methodsfor browsing and examining the contents of the data warehouse. They need to know themeanings of the data items. Y ou have to prevent them from drawing wrong conclusionsfrom their analysis through their ignorance about the exact meanings. \\nEarlier data mart implementations were limited in scope to probably one subject area.\\nMostly, those data marts were used by small groups of users in single departments. Theusers of those data marts were able to get by with scanty metadata. Today’ s data ware-houses are much wider in scope and larger in size. Without adequate metadata support,users of these larger data warehouses are totally handicapped. \\nFor Building the Data Warehouse. Let us say you are the data extraction and trans-\\nformation expert on the project team. Y ou know data extraction methods very well. Y oucan work with data extraction tools. Y ou understand the general data transformation tech-niques. But, in order to apply your expertise, first you must know the source systems andtheir data structures. Y ou need to know the structures and the data content in the datawarehouse. Then you need to determine the mappings and the data transformations. Sofar, to perform your tasks in building the data extraction and data transformation compo-nent of the data warehouse, you need metadata about the source systems, source-to-targetmappings, and data transformation rules.\\nTry to wear a different hat. Y ou are now the DBA for the data warehouse database. Y ou\\nare responsible for the physical design of the database and for doing the initial loading.Y ou are also responsible for periodic incremental loads. There are more responsibilitiesfor you. Even ignoring all the other responsibilities for a moment, in order to perform justthe tasks of physical design and loading, you need metadata about a number of things. Y ouneed the layouts in the staging area. Y ou need metadata about the logical structure of thedata warehouse database. Y ou need metadata about the data refresh and load cycles. Thisis just the bare minimum information you need. \\nIf you consider every activity and every task for building the data warehouse, you will\\ncome to realize that metadata is an overall compelling necessity and a very significantcomponent in your data warehouse. Metadata is absolutely essential for building your datawarehouse.\\nFor Administering the Data Warehouse. Because of the complexities and enor-\\nmous sizes of modern data warehouses, it is impossible to administer the data warehousewithout substantial metadata. Figure 9-2 lists a series of questions relating to data ware-house administration. Please go through each question on the list carefully. Y ou cannot ad-minister your data warehouse without answers to these questions. Y our data warehousemetadata must address these issues.\\nWho Needs Metadata? Let us pause for a moment and consider who the people are\\nthat need metadata in a data warehouse environment. Please go through the columns in176 THE SIGNIFICANT ROLE OF METADATA', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='44490354-73b9-4a6f-87a2-8641b7e66703', embedding=None, metadata={'page_label': '197', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 9-3. This figure gives you an idea about who needs and uses metadata. We will\\nelaborate on this in later sections.\\nImagine a filing cabinet stuffed with documents without any folders and labels. With-\\nout metadata, your data warehouse is like such a filing cabinet. It is probably filled withinformation very useful for your users and for IT developers and administrators. But with-out any easy means to know what is there, the data warehouse is of very limited value. \\nMetadata is Like a Nerve Center. Various processes during the building and ad-\\nministering of the data warehouse generate parts of the data warehouse metadata. Parts ofmetadata generated by one process are used by another. In the data warehouse, metadataassumes a key position and enables communication among various processes. It acts likea nerve center in the data warehouse. Figure 9-4 shows the location of metadata within thedata warehouse. Use this figure to determine the metadata components that apply to yourdata warehouse environment. By examining each metadata component closely, you willalso perceive that the individual parts of the metadata are needed by two groups of people:(1) end-users, and (2) IT (developers and administrators). In the next two subsections, wewill review why metadata is critical for each of these two groups\\nWhy Metadata is Vital for End-Users\\nThe following would be a typical use of your data warehouse by a key user, say, a business\\nanalyst. The Marketing VP of your company has asked this business analyst to do a thor-WHY METADATA IS IMPORTANT 177\\nHow to handle data changes?\\nHow to include new sources?Where to cleanse the data? How to change \\nthe data cleansing methods?\\nHow to cleanse data after populating the \\nwarehouse?\\nHow to switch to  new data transformation \\ntechniques?\\nHow to audit the application of ongoing \\nchanges?\\nHow to add new external data sources?\\nHow to drop some external data sources?When mergers and acquisitions happen, how \\nto bring in new data to the warehouse?\\nHow to verify all external data on ongoing \\nbasis?How to add new summary tables?\\nHow to control runaway queries?How to expand storage?When to schedule platform upgrades?How to add new information delivery \\ntools for the users?\\nHow to continue ongoing training?How to maintain and enhance user \\nsupport function?\\nHow to monitor and improve ad hoc \\nquery performance?\\nWhen to schedule backups?How to perform disaster recovery drills?How to keep data definitions up-to-date?How to maintain the security system?How to monitor system load distribution? Data Extraction/Transformation/Loading\\nData from External Sources Data Warehouse \\nFigure 9-2 Data warehouse administration: questions and issues.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b4d4b4c0-25bb-457a-aba9-7c929ae93b7f', embedding=None, metadata={'page_label': '198', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='178 THE SIGNIFICANT ROLE OF METADATA\\nFigure 9-3 Who needs metadata?Databases, Tables,   \\nColumns, Server \\nPlatformsDatabases, Tables, \\nColumnsList of Predefined \\nQueries and Reports, \\nBusiness Views \\nData Structures, Data \\nDefinitions, Data \\nMapping, Cleansing \\nFunctions, \\nTransformation Rules Business Terms, Data \\nDefinitions, Data \\nMapping, Cleansing \\nFunctions, \\nTransformation Rules Business Terms, Data \\nDefinitions, Filters, \\nData Sources, \\nConversion , Data \\nOwners \\nProgram Code in \\nSQL, 3GL,4GL, \\nFront-end \\nApplications, SecurityQuery Toolsets, \\nDatabase Access for \\nComplex Analysis Authorization \\nRequests, \\nInformation Retrieval \\ninto Desktop \\nApplications such as \\nSpreadsheets Information Discovery\\nInformation AccessMeaning of DataIT Professionals Power Users Casual Users\\nFigure 9-4 Metadata acts as a nerve center.Extraction \\nToolSource    \\nSystems\\nCleansing \\nTool\\nTransfor-\\nmation \\nTool\\nData \\nLoad \\nFunction External \\nDataQuery \\nTool\\nReporting \\nTool\\nOLAP \\nTool\\nData \\nMining\\nAppli-cationsDATA \\nWAREHOUSE \\nMETADATA', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2c027b7b-e677-49aa-9f98-bae227b8e671', embedding=None, metadata={'page_label': '199', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ough analysis of a problem that recently surfaced. Because of the enormous sales potential\\nin the Midwest and Northeast regions, your company has opened five new stores in each re-gion. Although overall countrywide sales increased nicely for two months following theopening of the stores, after that the sales went back to the prior levels and remained flat. TheMarketing VP wants to know why, so that she can take appropriate action. \\nAs a user, the business analyst expects to find answers from the new data warehouse,\\nbut he does not know the details about the data in the data warehouse. Specifically, hedoes not know the answers to the following questions:\\n/L50539Are the sale units and dollars stored by individual transactions or as summary totals,\\nby product, for each day in each store?\\n/L50539Can sales be analyzed by product, promotion, store, and month?\\n/L50539Can current month sales be compared to sales in the same month last year?\\n/L50539Can sales be compared to targets?\\n/L50539How is profit margin calculated? What are the business rules?\\n/L50539What is the definition of a sales region? Which districts are included in each of the\\ntwo regions being analyzed?\\n/L50539Where did the sales come from? From which source systems?\\n/L50539How old are the sales numbers? How often do these numbers get updated?\\nIf the analyst is not sure of the nature of the data, he is likely to interpret the results of\\nthe analysis incorrectly. It is possible that the new stores are cannibalizing sales from theirown existing stores and that is why the overall sales remain flat. But the analyst may notfind the right reasons because of misinterpretation of the results. \\nThe analysis will be more effective if you provide adequate metadata to help as a pow-\\nerful roadmap of the data. If there is sufficient and proper metadata, the analyst does nothave to get assistance from IT every time he needs to run an analysis. Easily accessiblemetadata is crucial for end-users.\\nLet us take the analogy of an industrial warehouse storing items of merchandise sold\\nthrough catalog. The customer refers to the catalog to find the merchandise to be ordered.The customer uses the item number in the catalog to place the order. Also, the catalog in-dicates the color, size, and shape of the merchandise item. The customer calculates the to-tal amount to be paid from the price details in the catalog. In short, the catalog covers allthe items in the industrial warehouse, describes the items, and facilitates the placing of theorder. \\nIn a similar way, the user of your data warehouse is like the customer. A query for in-\\nformation from the user is like an order for items of merchandise in the industrial ware-house. Just as the customer needs the catalog to place an order, so does your user needmetadata to run a query on your data warehouse. \\nFigure 9-5 summarizes the vital need of metadata for end-users. The figure shows the\\ntypes of information metadata provides to the end-users and the purposes for which theyneed these types of information.\\nWhy Metadata is Essential for IT\\nDevelopment and deployment of your data warehouse is a joint effort between your IT\\nstaff and your user representatives. Nevertheless, because of the technical issues, IT is pri-WHY METADATA IS IMPORTANT 179', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e91a6fe4-0b35-4774-a8d5-e5eefaba5d3f', embedding=None, metadata={'page_label': '200', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='marily responsible for the design and ongoing administration of the data warehouse. For\\nperforming the responsibilities for design and administration, IT must have access toproper metadata.\\nThroughout the entire development process, metadata is essential for IT. Beginning with\\nthe data extraction and ending with information delivery, metadata is crucial for IT. As thedevelopment process moves through data extraction, data transformation, data integration,data cleansing, data staging, data storage, query and report design, design for OLAP , andother front-end systems, metadata is critical for IT to perform their development activities. \\nHere is a summary list of processes in which metadata is significant for IT:\\n/L50539Data extraction from sources\\n/L50539Data transformation\\n/L50539Data scrubbing\\n/L50539Data aggregation and summarization\\n/L50539Data staging\\n/L50539Data refreshment\\n/L50539Database design\\n/L50539Query and report design\\nFigure 9-6 summarizes the essential need for metadata for IT. The figure shows the\\ntypes of information metadata provides IT staff and the purposes for which they needthese types of information.180 THE SIGNIFICANT ROLE OF METADATA\\nMETADATA  VITAL  FOR  END-USERS\\nData content\\nSummary dataBusiness dimensionsBusiness metricsNavigation pathsSource systemsExternal dataData transformation rulesLast update datesData load/update cyclesQuery templatesReport formatsPredefined queries/reportsOLAP data\\nMETADATA \\nESSENTIAL \\nFOR IT\\nEND-USERS\\nFigure 9-5 Metadata vital for end-users.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7dea3774-fc34-47e5-91b0-d70fe6a403e7', embedding=None, metadata={'page_label': '201', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Automation of Warehousing Tasks\\nMaintaining metadata is no longer a form of glorified documentation. Traditionally, meta-\\ndata has been created and maintained as documentation about the data for each process.Now metadata is assuming a new active role. Let us see how this is happening. \\nAs you know, tools perform major functions in a data warehouse environment. For ex-\\nample, tools enable the extraction of data from designated sources. When you provide themapping algorithms, data transformation tools transform data elements to suit the targetdata structures. Y ou may specify valid values for data elements and the data quality toolswill use these values to ensure the integrity and validity of data. At the front end, tools em-power the users to browse the data content and gain access to the data warehouse. Thesetools generally fall into two categories: development tools for IT professionals, and infor-mation access tools for end-users.\\nWhen you, as a developer, use a tool for design and development, in that process, the\\ntool lets you to create and record a part of the data warehouse metadata. When you use an-other tool to perform another process in the design and development, this tool uses themetadata created by the first tool. When your end-user uses a query tool for informationaccess at the front end, that query tool uses metadata created by some of the back-endtools. What exactly is happening here with metadata? Metadata is no longer passive docu-mentation. Metadata takes part in the process. It aids in the automation of data warehouseprocesses.\\nLet us consider the back-end processes beginning with the defining of the data sources.\\nAs the data movement takes place from the data sources to the data warehouse databasethrough the data staging area, several processes occur. In a typical data warehouse, appro-WHY METADATA IS IMPORTANT 181\\nMETADATA  ESSENTIAL  FOR  IT\\nSource data structures\\nSource platformsData extraction methodsExternal dataData transformation rulesData cleansing rulesStaging area structuresDimensional modelsInitial loadsIncremental loadsData summarizationOLAP systemWeb-enablingQuery/report design\\nMETADATA \\nVITAL FOR \\nEND-USERS\\nIT  \\nPROFESSIONALS\\nFigure 9-6 Metadata essential for IT.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f7ff04a3-4fd0-4967-9e52-513ecae3015c', embedding=None, metadata={'page_label': '202', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='priate tools assist in these processes. Each tool records its own metadata as data move-\\nment takes place. The metadata recorded by one tool drives one or more processes thatfollow. This is how metadata assumes an active role and assists in the automation of datawarehouse processes. \\nHere is a list of back-end processes shown in the order in which they generally occur: \\n1. Source data structure definition\\n2. Data extraction 3. Initial reformatting/merging4. Preliminary data cleansing5. Data transformation and consolidation6. Validation and quality check7. Data warehouse structure definition8. Load image creation\\nFigure 9-7 shows each of these eight processes. The figure also indicates the metadata\\nrecorded by each process. Further, the figure points out how each process is able to usethe metadata recorded in the earlier processes.\\nMetadata is important in a data warehouse because it drives the processes. However,\\nour discussion above leads to the realization that each tool may record metadata in its ownproprietary format. Again, the metadata recorded by each tool may reside on the platformwhere the corresponding process runs. If this is the case, how can the metadata recorded182 THE SIGNIFICANT ROLE OF METADATA\\nFigure 9-7 Metadata drives data warehouse processes.Source\\nData\\nStructure\\nDefinitionSource system\\nplatforms, data\\nstructures\\nData\\nTransformation\\nand\\nConsolidationData transformation\\nrules, aggregationData\\nExtractionExtraction\\ntechniques,\\ninitial files and\\nstructures\\nValidation\\nand Quality\\nCheckQuality\\nverification\\nrulesInitial\\nReformatting\\n/ MergingSort/merge rules,\\nmerged files and\\nstructuresPreliminary\\nData\\nCleansingData cleansing\\nrules\\nData\\nWarehouse\\nStructure\\nDefinitionData models --\\nlogical/physicalLoad\\nImage\\nCreationKey structuring\\nrules, DBMS\\nconsiderations\\nProcess Associated Metadata', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1cf2d798-4d3e-45b7-81b5-0f7d8056f3f0', embedding=None, metadata={'page_label': '203', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='by one tool in a proprietary format drive the process for the next tool? This is a critical\\nquestion. This is where standardization of metadata comes into play. We will get to thediscussion on metadata standards at the end of the chapter.\\nEstablishing the Context of Information\\nImagine this scenario. One of your users wants to run a query to retrieve sales data for\\nthree products during the first seven days of April in the Southern Region. This user com-poses the query as follows:\\nProduct = Widget-1 or Widget-2 or Widget-3\\nRegion = ‘SOUTH’Period = 04-01-2000 to 04-07-2000 \\nThe result comes back:\\nSale Units Amount\\nWidget-1— 25,355 253,550Widget-2— 16,978 254,670Widget-3— 7,994 271,796\\nLet us examine the query and the results. In the specification for region, which territo-\\nries does region “SOUTH” include? Are these the territories your user is interested in?What is the context of the data item “SOUTH” in your data warehouse? Next, does thedata item 04-01-2000 denote April 1, 2000 or January 4, 2000? What is the conventionused for dates in your data warehouse?\\nLook at the result set. Are the numbers shown as sale units given in physical units of\\nthe products, or in some measure such as pounds or kilograms? What about the amountsshown in the result set? Are these amounts in dollars or in some other currency? This is apertinent question if your user is accessing your data warehouse from Europe.\\nFor the dates stored in your data warehouse, if the first two digits of the date format in-\\ndicate the month and the next two digits denote the date, then 04-01-2000 means April 1,2000. Only in this context is the interpretation correct. Similarly, context is important forthe interpretation of the other data elements. \\nHow can your user find out what exactly each data element in the query is and what the\\nresult set means? The answer is metadata. Metadata gives your user the meaning of eachdata element. Metadata establishes the context for the data elements. Data warehouseusers, developers, and administrators interpret each data element in the context estab-lished and recorded in metadata. \\nMETADATA TYPES BY FUNCTIONAL AREAS\\nSo far in this chapter, we have discussed several aspects of metadata in a data warehouse\\nenvironment. We have seen why metadata is a critical need for end-users as well as for ITprofessionals who are responsible for development and administration. We have estab-lished that metadata plays an active role in the automation of data warehouse processes.METADATA TYPES BY FUNCTIONAL AREAS 183', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6e119dad-6194-4fee-8267-7dc2ccd923e9', embedding=None, metadata={'page_label': '204', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='At this stage, we can increase our understanding further by grouping the various types of\\nmetadata. When you classify each type, your appreciation for each type will increase andyou can better understand the role of metadata within each group. \\nDifferent authors and data warehouse practitioners classify and group metadata in var-\\nious ways: some by usage, and some by who uses it. Let us look at a few ways in whichmetadata is being classified. In each line of the list shown below are the different methodsfor classification of metadata:\\n/L50539Administrative/End-user/Optimization\\n/L50539Development/Usage \\n/L50539In the data mart/At the workstation\\n/L50539Building/Maintaining/Managing/Using\\n/L50539Technical/Business\\n/L50539Back room/Front room\\n/L50539Internal/External\\nIn an earlier chapter, we considered a way of dividing the data warehouse environment\\nby means of the major functions. We can picture the data warehouse environment as beingfunctionally divided into the three areas of Data Acquisition, Data Storage, and Informa-\\ntion Delivery. All data warehouse processes occur in these three functional areas. As a de-\\nveloper, you design the processes in each of the three functional areas. Each of the toolsused for these processes creates and records metadata and may also use and be driven bythe metadata recorded by other tools.\\nFirst, let us group the metadata types by these three functional areas. Why? Because\\nevery data warehouse process occurs in one of just these three areas. Take into account allthe processes happening in each functional area and then put together all the processes inall the three functional areas. Y ou will get a complete set of the data warehouse processeswithout missing any one. Also, you will be able to compile a complete list of metadatatypes.\\nLet us move on to the classification of metadata types by the functional areas in the\\ndata warehouse:\\n1. Data acquisition\\n2. Data storage3. Information delivery\\nData Acquisition\\nIn this area, the data warehouse processes relate to the following functions:\\n/L50539Data extraction\\n/L50539Data transformation\\n/L50539Data cleansing\\n/L50539Data integration\\n/L50539Data staging184 THE SIGNIFICANT ROLE OF METADATA', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5cf795ee-39d4-414c-aa64-eb6be8f16637', embedding=None, metadata={'page_label': '205', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='As the processes take place, the appropriate tools record the metadata elements relating\\nto the processes. The tools record the metadata elements during the development phasesas well as while the data warehouse is in operation after deployment. \\nAs an IT professional and part of the data warehouse project team, you will be using\\ndevelopment tools that record metadata relating to this area. Also, some other tools youwill be using for other processes either in this area or in some other area may use themetadata recorded by other tools in this area. For example, when you use a query tool tocreate standard queries, you will be using metadata recorded by processes in the data ac-quisition area. As you will note, the query tool is meant for a process in a different area,namely, the information delivery area. \\nIT professionals will also be using metadata recorded by processes in the data acquisi-\\ntion area for administering and monitoring the ongoing functions of the data warehouseafter deployment. Y ou will use the metadata from this area to monitor ongoing data ex-traction and transformation. Y ou will make sure that the ongoing load images are createdproperly by referring to the metadata from this area.\\nThe users of your data warehouse will also be using the metadata recorded in the data\\nacquisition area. When a user wants to find the data sources for the data elements in his orher query, he or she will look up the metadata from the data acquisition area. Again, whenthe user wants to know how the profit margin has been calculated and stored in the datawarehouse, he or she will look up the derivation rules in the metadata recorded in the dataacquisition area. \\nFor metadata types recorded and used in the data acquisition area, please refer to Fig-\\nure 9-8. This figure summarizes the metadata types and the relevant data warehouseMETADATA TYPES BY FUNCTIONAL AREAS 185\\nDATA  ACQUISITION\\nPROCESSES\\nData Extraction, Data \\nTransformation, Data Cleansing, \\nData Integration, Data Staging\\nMETADATA TYPES\\nSource system platforms\\nSource system logical models\\nSource system physical models\\nSource structure definitionsData extraction methodsData transformation rulesData cleansing rulesSummarization rules\\nTarget logical models\\nTarget physical models\\nData structures in staging areaSource to target relationshipsExternal data structuresExternal data definitions\\nFigure 9-8 Data acquisition: metadata types.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4952853b-f47a-4421-9ca0-3b138210f7d1', embedding=None, metadata={'page_label': '206', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='processes. Try to relate these metadata types and processes to your data warehouse envi-\\nronment.\\nData Storage\\nIn this area, the data warehouse processes relate to the following functions:\\n/L50539Data loading\\n/L50539Data archiving\\n/L50539Data management\\nJust as in the other areas, as processes take place in the data storage functional area, the\\nappropriate tools record the metadata elements relating to the processes. The tools recordthe metadata elements during the development phases as well as while the data warehouseis in operation after deployment. \\nSimilar to metadata recorded by processes in the data acquisition area, metadata\\nrecorded by processes in the data storage area is used for development, administration,and by the users. Y ou will be using the metadata from this area for designing the full datarefreshes and the incremental data loads. The DBA will be using metadata for the process-es of backup, recovery, and tuning the database. For purging the data warehouse and forperiodic archiving of data, metadata from this area will be used for data warehouse ad-ministration. \\nWill the users be using metadata from the data storage functional area? To give you just\\none example, let us say one of your users wants to create a query breaking the total quar-terly sales down by sale districts. Before the user runs the query, he or she would like toknow when was the last time the data on district delineation was loaded. From where canthe user get the information about load dates of the district delineation? Metadata record-ed by the data loading process in the data storage functional area will give the user the lat-est load date for district delineation. \\nFor metadata types recorded and used in the data storage area, please refer to Figure\\n9-9. This figure summarizes the metadata types and the relevant data warehouseprocesses. See how the metadata types and the processes relate to your data warehouseenvironment.\\nInformation Delivery\\nIn this area, the data warehouse processes relate to the following functions:\\n/L50539Report generation\\n/L50539Query processing\\n/L50539Complex analysis\\nMostly, the processes in this area are meant for end-users. While using the processes,\\nend-users generally use metadata recorded in processes of the other two areas of data ac-quisition and data storage. When a user creates a query with the aid of a query processingtool, he or she can refer back to metadata recorded in the data acquisition and data storageareas and can look up the source data configurations, data structures, and data transforma-186 THE SIGNIFICANT ROLE OF METADATA', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f109132c-b56d-4e72-a6f2-03566827431b', embedding=None, metadata={'page_label': '207', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='tions from the metadata recorded in the data acquisition area. In the same way, from meta-\\ndata recorded in the data storage area, the user can find the date of the last full refresh andthe incremental loads for various tables in the data warehouse database.\\nGenerally, metadata recorded in the information delivery functional area relate to pre-\\ndefined queries, predefined reports, and input parameter definitions for queries and re-ports. Metadata recorded in this functional area also include information for OLAP . Thedevelopers and administrators are involved in these processes.\\nFor metadata types recorded and used in the information delivery area, see Figure 9-\\n10. This figure summarizes the metadata types and the relevant data warehouse processes.See how the metadata types and processes apply to your data warehouse environment.\\nMetadata types may also be classified as business metadata and technical metadata.\\nThis is another effective method of classifying metadata types because the nature and for-mat of metadata in one group are markedly different from those in the other group. Thenext two sections deal with this method of classification. \\nBUSINESS METADATA\\nBusiness metadata connects your business users to your data warehouse. Business users\\nneed to know what is available in the data warehouse from a perspective different fromthat of IT professionals like you. Business metadata is like a roadmap or an easy-to-useinformation directory showing the contents and how to get there. It is like a tour guide forexecutives and a route map for managers and business analysts.BUSINESS METADATA 187\\nINFORMATION  DELIVERY\\nPROCESSES\\nReport Generation, \\nQuery Processing,                                \\nComplex Analysis\\nMETADATA TYPES\\nSource systems\\nSource data definitionsSource structure definitionsData extraction rulesData transformation rules\\nData cleansing rules\\nSource-target mappingSummary dataTarget physical models\\nTarget data definitions in   \\nbusiness terms\\nData contentData navigation methods\\nQuery templates\\nPreformatted reportsPredefined queries/reportsOLAP content\\nFigure 9-9 Data storage: metadata types.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dc59205c-e3ff-4700-b6a7-b1431b8aa332', embedding=None, metadata={'page_label': '208', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Content Overview\\nFirst of all, business metadata must describe the contents in plain language giving infor-\\nmation in business terms. For example, the names of the data tables or individual data ele-ments must not be cryptic but be meaningful terms that business users are familiar with.The data item name calc_pr_sle is not acceptable. Y ou need to rename this as calculated-\\nprior-month-sale. \\nBusiness metadata is much less structured than technical metadata. A substantial por-\\ntion of business metadata originates from textual documents, spreadsheets, and even busi-ness rules and policies not written down completely. Even though much of business meta-data is from informal sources, it is as important as metadata from formal sources such asdata dictionary entries. All of the informal metadata must be captured, put in a standardform, and stored as business metadata in the data warehouse.\\nA large segment of business users do not have enough technical expertise to create\\ntheir own queries or format their own reports. They need to know what predefined queriesare available and what preformatted reports can be produced. They must be able to identi-fy the tables and columns in the data warehouse by referring to them by business names.Business metadata should, therefore, express all of this information in plain language.\\nExamples of Business Metadata\\nBusiness metadata focuses on providing support for the end-user at the workstation. It\\nmust make it easy for the end-users to understand what data is available in the data ware-188 THE SIGNIFICANT ROLE OF METADATA\\nINFORMATION  DELIVERY\\nPROCESSES\\nReport Generation, \\nQuery Processing,                                \\nComplex Analysis\\nMETADATA TYPES\\nSource systems\\nSource data definitionsSource structure definitionsData extraction rulesData transformation rules\\nData cleansing rules\\nSource-target mappingSummary dataTarget physical models\\nTarget data definitions in   \\nbusiness terms\\nData contentData navigation methods\\nQuery templates\\nPreformatted reportsPredefined queries/reportsOLAP content\\nFigure 9-10 Information delivery: metadata types.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='38950284-a039-467a-9e2d-0803d358a60e', embedding=None, metadata={'page_label': '209', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='house and how they can use it. Business metadata portrays the data warehouse purely\\nfrom the perspective of the end-users. It is like an external view of the data warehouse de-signed and composed in simple business terms that users can easily understand. \\nLet us try to better understand business metadata by looking at a list of examples:\\n/L50539Connectivity procedures\\n/L50539Security and access privileges\\n/L50539The overall structure of data in business terms\\n/L50539Source systems\\n/L50539Source-to-target mappings\\n/L50539Data transformation business rules\\n/L50539Summarization and derivations\\n/L50539Table names and business definitions\\n/L50539Attribute names and business definitions\\n/L50539Data ownership \\n/L50539Query and reporting tools\\n/L50539Predefined queries \\n/L50539Predefined reports\\n/L50539Report distribution information\\n/L50539Common information access routes\\n/L50539Rules for analysis using OLAP\\n/L50539Currency of OLAP data\\n/L50539Data warehouse refresh schedule\\nThe list is by no means all-inclusive, but it gives a good basis for you to make up a sim-\\nilar list for your data warehouse. Use the list as a guide to ensure that business metadata isprovided using business names and made easily understandable to your users.\\nContent Highlights\\nFrom the list of examples, let us highlight the contents of business metadata. What are all\\nthe various kinds of questions business metadata can answer? What types of informationcan the user get from business metadata? \\nLet us derive a list of questions business metadata can answer for the end-users. Al-\\nthough the following list does not include all possible questions by the users, it can be auseful reference: \\n/L50539How can I sign onto and connect with the data warehouse?\\n/L50539Which parts of the data warehouse can I access?\\n/L50539Can I see all the attributes from a specific table?\\n/L50539What are the definitions of the attributes I need in my query? \\n/L50539Are there any queries and reports already predefined to give the results I need?\\n/L50539Which source system did the data I want come from?\\n/L50539What default values were used for the data items retrieved by my query?\\n/L50539What types of aggregations are available for the metrics needed?BUSINESS METADATA 189', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a20783ca-32e5-4a3d-852b-9b734610059b', embedding=None, metadata={'page_label': '210', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539How is the value in the data item I need derived from other data items?\\n/L50539When was the last update for the data items in my query?\\n/L50539On which data items can I perform drill down analysis?\\n/L50539How old is the OLAP data? Should I wait for the next update?\\nWho Benefits?\\nBusiness metadata primarily benefits end-users. This is a general statement. Who specifi-\\ncally benefits from business metadata? How does business metadata serve specific mem-bers of the end-user community? Please look over the following list:\\n/L50539Managers \\n/L50539Business analysts\\n/L50539Power users\\n/L50539Regular users\\n/L50539Casual users\\n/L50539Senior managers/junior executives\\nTECHNICAL METADATA\\nTechnical metadata is meant for the IT staff responsible for the development and adminis-\\ntration of the data warehouse. The technical personnel need information to design eachprocess. These are processes in every functional area of the data warehouse. Y ou, as partof the technical group on the project team, must know the proposed structure and contentof the data warehouse. Different members on the project team need different kinds of in-formation from technical metadata. If business metadata is like a roadmap for the users touse the data warehouse, technical metadata is like a support guide for the IT professionalsto build, maintain, and administer the data warehouse.\\nContent Overview\\nIT staff working on the data warehouse project need technical metadata for different pur-\\nposes. If you are a data acquisition expert, your need for metadata is different from that ofthe information access developer on the team. As a whole, the technical staff on the pro-ject need to understand the data extraction, data transformation, and data cleansingprocesses. They have to know the output layouts from every extraction routine and mustunderstand the data transformation rules. \\nIT staff require technical metadata for three distinct purposes. First, IT personnel need\\ntechnical metadata for the initial development of the data warehouse. Let us say you areresponsible for design and development of the data transformation process. For this pur-pose, the metadata from the earlier process of data extraction can assist in your develop-ment effort.\\nSecond, technical metadata is absolutely essential for ongoing growth and maintenance\\nof the data warehouse. If you are responsible for making changes to some data structures,or even for a second release of the data warehouse, where will you find the information onthe contents and the various processes? Y ou need technical metadata. 190 THE SIGNIFICANT ROLE OF METADATA', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='253ace0b-91cf-4f89-9653-3cfeec953f28', embedding=None, metadata={'page_label': '211', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Technical metadata is also critical for the continuous administration of the production\\ndata warehouse. As an administrator, you have to monitor the ongoing data extractions.Y ou have to ensure that the incremental loads are completed correctly and on time. Y ourresponsibility may also include database backups and archiving of old data. Data ware-house administration is almost impossible without technical metadata.\\nExamples of Technical Metadata\\nTechnical metadata concentrates on support for the IT staff responsible for development,\\nmaintenance, and administration. Technical metadata is more structured than businessmetadata. Technical metadata is like an internal view of the data warehouse showing theinner details in technical terms. Here is a list of examples of technical metadata:\\n/L50539Data models of source systems\\n/L50539Record layouts of outside sources\\n/L50539Source-to-staging area mappings\\n/L50539Staging area-to-data warehouse mappings\\n/L50539Data extraction rules and schedules\\n/L50539Data transformation rules and versioning\\n/L50539Data aggregation rules\\n/L50539Data cleansing rules\\n/L50539Summarization and derivations\\n/L50539Data loading and refresh schedules and controls\\n/L50539Job dependencies\\n/L50539Program names and descriptions\\n/L50539Data warehouse data model\\n/L50539Database names\\n/L50539Table/view names\\n/L50539Column names and descriptions\\n/L50539Key attributes\\n/L50539Business rules for entities and relationships\\n/L50539Mapping between logical and physical models\\n/L50539Network/server information\\n/L50539Connectivity data\\n/L50539Data movement audit controls\\n/L50539Data purge and archival rules\\n/L50539Authority/access privileges\\n/L50539Data usage/timings\\n/L50539Query and report access patterns \\n/L50539Query and reporting tools\\nPlease review the list and come up with a comparable list for your data warehouse en-\\nvironment.TECHNICAL METADATA 191', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='939a53d4-b89f-423a-a7ff-370141abeca0', embedding=None, metadata={'page_label': '212', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Content Highlights\\nThe list of examples gives you an idea of the kinds of information technical metadata in a\\ndata warehouse environment must contain. Just as in the case of business metadata, let usderive a list of questions technical metadata can answer for developers and administrators.Please review the following list:\\n/L50539What databases and tables exist?\\n/L50539What are the columns for each table? \\n/L50539What are the keys and indexes?\\n/L50539What are the physical files?\\n/L50539Do the business descriptions correspond to the technical ones?\\n/L50539When was the last successful update?\\n/L50539What are the source systems and their data structures? \\n/L50539What are the data extraction rules for each data source?\\n/L50539What is source-to-target mapping for each data item in the data warehouse?\\n/L50539What are the data transformation rules?\\n/L50539What default values were used for the data items while cleaning up missing data?\\n/L50539What types of aggregations are available?\\n/L50539What are the derived fields and their rules for derivation?\\n/L50539When was the last update for the data items in my query?\\n/L50539What are the load and refresh schedules?\\n/L50539How often data is purged or archived? Which data items?\\n/L50539What is schedule for creating data for OLAP?\\n/L50539What query and report tools are available?\\nWho Benefits?\\nThe following list indicates the specific types of personnel who will benefit from techni-\\ncal metadata:\\n/L50539Project manager \\n/L50539Data warehouse administrator\\n/L50539Database administrator\\n/L50539Metadata manager\\n/L50539Data warehouse architect\\n/L50539Data acquisition developer\\n/L50539Data quality analyst\\n/L50539Business analyst\\n/L50539System administrator\\n/L50539Infrastructure specialist\\n/L50539Data modeler\\n/L50539Security architect192 THE SIGNIFICANT ROLE OF METADATA', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='17f20ff9-c35d-4ea7-9161-753b54f641ef', embedding=None, metadata={'page_label': '213', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='HOW TO PROVIDE METADATA\\nAs your data warehouse is being designed and built, metadata needs to be collected and\\nrecorded. As you know, metadata describes your data warehouse from various points ofview. Y ou look into the data warehouse through the metadata to find the data sources, tounderstand the data extractions and transformations, to determine how to navigatethrough the contents, and to retrieve information. Most of the data warehouse processesare performed with the aid of software tools. The same metadata or true copies of the rel-evant subsets must be available to every tool. \\nIn a recent study conducted by the Data Warehousing Institute, 86% of the respondents\\nfully recognized the significance of having a metadata management strategy. However,only 9% had implemented a metadata solution. Another 16% had a plan and had begun towork on the implementation. \\nIf most of the companies with data warehouses realize the enormous significance of\\nmetadata management, why are only a small percentage doing anything about it? Metada-ta management presents great challenges. The challenges are not in the capturing of meta-data through the use of the tools during data warehouse processes but lie in the integrationof the metadata from the various tools that create and maintain their own metadata.\\nWe will explore the challenges. How can you find options to overcome the challenges\\nand establish effective metadata management in your data warehouse environment? Whatis happening in the industry? While standards are being worked out in industry coalitions,are there interim options for you? First, let us establish the basic requirements for goodmetadata management. What are the requirements? Next, we will consider the sources formetadata before we examine the challenges.\\nMetadata Requirements\\nVery simply put, metadata must serve as a roadmap to the data warehouse for your users.\\nIt must also support IT in the development and administration of the data warehouse. Letus go beyond these simple statements and look at specifics of the requirements for meta-data management.\\nCapturing and Storing Data. The data dictionary in an operational system stores\\nthe structure and business rules as they are at the current time. For operational systems, itis not necessary to keep the history of the data dictionary entries. However, the history ofthe data in your data warehouse spans several years, typically five to ten in most datawarehouses. During this time, changes do occur in the source systems, data extractionmethods, data transformation algorithms, and in the structure and content of the datawarehouse database itself. Metadata in a data warehouse environment must, therefore,keep track of the revisions. As such, metadata management must provide means for cap-turing and storing metadata with proper versioning to indicate its time-variant feature. \\nVariety of Metadata Sources. Metadata for a data warehouse never comes from a\\nsingle source. CASE tools, the source operational systems, data extraction tools, datatransformation tools, the data dictionary definitions, and other sources all contribute tothe data warehouse metadata. Metadata management, therefore, must be open enough tocapture metadata from a large variety of sources.HOW TO PROVIDE METADATA 193', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f2f3cad2-71d5-4b0c-98e1-6dc966dbd18b', embedding=None, metadata={'page_label': '214', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Metadata Integration. We have looked at elements of business and technical meta-\\ndata. Y ou must be able to integrate and merge all these elements in a unified manner forthem to be meaningful to your end-users. Metadata from the data models of the sourcesystems must be integrated with metadata from the data models of the data warehousedatabases. The integration must continue further to the front-end tools used by the end-users. All these are difficult propositions and very challenging.\\nMetadata Standardization. If your data extraction tool and the data transformation\\ntool represent data structures, then both tools must record the metadata about the datastructures in the same standard way. The same metadata in different metadata stores ofdifferent tools must be represented in the same manner.\\nRippling Through of Revisions. Revisions will occur in metadata as data or busi-\\nness rules change. As the metadata revisions are tracked in one data warehouse process,the revisions must ripple throughout the data warehouse to the other processes.\\nKeeping Metadata Synchronized. Metadata about data structures, data elements,\\nevents, rules, and so on must be kept synchronized at all times throughout the data ware-house. \\nMetadata Exchange. While your end-users are using the front-end tools for infor-\\nmation access, they must be able to view the metadata recorded by back-end tools like thedata transformation tool. Free and easy exchange of metadata from one tool to anothermust be possible\\nSupport for End-Users. Metadata management must provide simple graphical and\\ntabular presentations to end-users, making it easy for them to browse through the metada-ta and understand the data in the data warehouse purely from a business perspective. \\nThe requirements listed are very valid for metadata management. Integration and stan-\\ndardization of metadata are great challenges. Nevertheless, before addressing these is-sues, you need to know the usual sources of metadata. The general list of metadatasources will help you establish a metadata management initiative for your data warehouse.\\nSources of Metadata\\nAs tools are used for the various data warehouse processes, metadata gets recorded as a\\nbyproduct. For example, when a data transformation tool is used, the metadata on thesource-to-target mappings get recorded as a byproduct of the process carried out with thattool. Let us look at all the usual sources of metadata without any reference to individualprocesses.\\nSource Systems\\n/L50539Data models of operational systems (manual or with CASE tools)\\n/L50539Definitions of data elements from system documentation\\n/L50539COBOL copybooks and control block specification\\n/L50539Physical file layouts and field definitions\\n/L50539Program specifications194 THE SIGNIFICANT ROLE OF METADATA', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b5d2fc3d-1231-40bc-9263-1392a09c9e60', embedding=None, metadata={'page_label': '215', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539File layouts and field definitions for data from outside sources\\n/L50539Other sources such as spreadsheets and manual lists\\nData Extraction\\n/L50539Data on source platforms and connectivity\\n/L50539Layouts and definitions of selected data sources\\n/L50539Definitions of fields selected for extraction\\n/L50539Criteria for merging into initial extract files on each platform\\n/L50539Rules for standardizing field types and lengths\\n/L50539Data extraction schedules\\n/L50539Extraction methods for incremental changes\\n/L50539Data extraction job streams\\nData Transformation and Cleansing\\n/L50539Specifications for mapping extracted files to data staging files\\n/L50539Conversion rules for individual files\\n/L50539Default values for fields with missing values\\n/L50539Business rules for validity checking\\n/L50539Sorting and resequencing arrangements\\n/L50539Audit trail for the movement from data extraction to data staging\\nData Loading\\n/L50539Specifications for mapping data staging files to load images\\n/L50539Rules for assigning keys for each file\\n/L50539Audit trail for the movement from data staging to load images\\n/L50539Schedules for full refreshes\\n/L50539Schedules for incremental loads\\n/L50539Data loading job streams\\nData Storage\\n/L50539Data models for centralized data warehouse and dependent data marts\\n/L50539Subject area groupings of tables\\n/L50539Data models for conformed data marts\\n/L50539Physical files \\n/L50539Table and column definitions\\n/L50539Business rules for validity checking\\nInformation Delivery\\n/L50539List of query and report tools\\n/L50539List of predefined queries and reportsHOW TO PROVIDE METADATA 195', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a2237e89-3e37-477b-975c-67e54febd3de', embedding=None, metadata={'page_label': '216', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Data model for special databases for OLAP\\n/L50539Schedules for retrieving data for OLAP\\nChallenges for Metadata Management\\nAlthough metadata is so vital in a data warehouse enrivonment, seamlessly integrating all\\nthe parts of metadata is a formidable task. Industry-wide standardization is far from beinga reality. Metadata created by a process at one end cannot be viewed through a tool used atanother end without going through convoluted transformations. These challenges forcemany data warehouse developers to abandon the requirements for proper metadata man-agement. \\nHere are the major challenges to be addressed while providing metadata:\\n/L50539Each software tool has its own propriety metadata. If you are using several tools in\\nyour data warehouse, how can you reconcile the formats?\\n/L50539No industry-wide accepted standards exist for metadata formats.\\n/L50539There are conflicting claims on the advantages of a centralized metadata repository\\nas opposed to a collection of fragmented metadata stores.\\n/L50539There are no easy and accepted methods of passing metadata along the processes as\\ndata moves from the source systems to the staging area and thereafter to the datawarehouse storage.\\n/L50539Preserving version control of metadata uniformly throughout the data warehouse is\\ntedious and difficult.\\n/L50539In a large data warehouse with numerous source systems, unifying the metadata re-\\nlating to the data sources can be an enormous task. Y ou have to deal with conflictingstandards, formats, data naming conventions, data definitions, attributes, values,business rules, and units of measure. Y ou have to resolve indiscriminate use of alias-es and compensate for inadequate data validation rules.\\nMetadata Repository\\nThink of a metadata repository as a general-purpose information directory or cataloguing\\ndevice to classify, store, and manage metadata. As we have seen earlier, business metada-ta and technical metadata serve different purposes. The end-users need the business meta-data; data warehouse developers and administrators require the technical metadata. Thestructures of these two categories of metadata also vary. Therefore, the metadata reposito-ry can be thought of as two distinct information directories, one to store business metada-ta and the other to store technical metadata. This division may also be logical within a sin-gle physical repository. \\nFigure 9-11 shows the typical contents in a metadata repository. Notice the division be-\\ntween business and technical metadata. Did you also notice another component called theinformation navigator? This component is implemented in different ways in commercialofferings. The functions of the information navigator include the following:\\nInterface from query tools. This function attaches data warehouse data to third-party\\nquery tools so that metadata definitions inside the technical metadata may beviewed from these tools.196 THE SIGNIFICANT ROLE OF METADATA', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='783b27af-5665-4236-86ce-9f12cda2a7c7', embedding=None, metadata={'page_label': '217', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Drill-down for details. The user of metadata can drill down and proceed from one lev-\\nel of metadata to a lower level for more information. For example, you can first getthe definition of a data table, then go to the next level for seeing all attributes, andgo further to get the details of individual attributes. \\nReview predefined queries and reports. The user is able to review predefined queries\\nand reports, and launch the selected ones with proper parameters.\\nA centralized metadata repository accessible from all parts of the data warehouse for\\nyour end-users, developers, and administrators appears to be an ideal solution for metadatamanagement. But for a centralized metadata repository to be the best solution, the reposi-tory must meet some basic requirements. Let us quickly review these requirements. It is noteasy to find a repository tool that satisfies every one of the requirements listed below.\\nFlexible organization. Allow the data administrator to classify and organize metadata\\ninto logical categories and subcategories, and assign specific components of meta-data to the classifications. \\nHistorical. Use versioning to maintain the historical perspective of the metadata.\\nIntegrated. Store business and technical metadata in formats meaningful to all types\\nof users.\\nGood compartmentalization. Able to separate and store logical and physical database\\nmodels.HOW TO PROVIDE METADATA 197\\nMETADATA  REPOSITORY\\nInformation  Navigator\\nTechnical MetadataBusiness  Metadata\\nSource systems data models, structures of external data sources, staging area file \\nlayouts, target  warehouse data models, source-staging area mappings, staging area-\\nwarehouse mappings, data extraction rules, data transformation rules, data cleansing \\nrules, data aggregation rules, data loading and refreshing rules, source system   \\nplatforms, data warehouse platform, purge/archival rules, backup/recovery, security  Source systems, source-target mappings, data transformation business rules, \\nsummary datasets, warehouse tables and columns in business terminology, query \\nand reporting tools, predefined queries, preformatted reports, data load and refresh    \\nschedules, support contact, OLAP data, access authorizationsNavigation routes through warehouse content, browsing of warehouse tables and \\nattributes, query composition, report formatting, drill-down and roll-up, report \\ngeneration and distribution, temporary storage of results\\nFigure 9-11 Metadata repository.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f0d83a2c-a645-4db4-9b1b-4aa645a22225', embedding=None, metadata={'page_label': '218', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Analysis and look-up capabilities. Capable of browsing all parts of metadata and also\\nnavigating through the relationships.\\nCustomizable. Able to create customized views of metadata for individual groups of\\nusers and to include new metadata objects as necessary.\\nMaintain descriptions and definitions. View metadata in both business and technical\\nterms.\\nStandardization of naming conventions. Flexibility to adopt any type of naming con-\\nvention and standardize throughout the metadata repository.\\nSynchronization. Keep metadata synchronized within all parts of the data warehouse\\nenvironment and with the related external systems.\\nOpen. Support metadata exchange between processes via industry-standard interfaces\\nand be compatible with a large variety of tools. \\nSelection of a suitable metadata repository product is one of the key decisions the pro-\\nject team must make. Use the above list of criteria as a guide while evaluating repositorytools for your data warehouse.\\nMetadata Integration and Standards\\nFor a free interchange of metadata within the data warehouse between processes performed\\nwith the aid of software tools, the need for standardization is obvious. Our discussions sofar must have convinced you of this dire need. As mentioned in Chapter 3, the Meta DataCoalition and the Object Management Group have both been working on standards formetadata. The Meta Data Coalition has accepted a standard known as the Open InformationModel (OIM). The Object Management Group has released the Common WarehouseMetamodel (CWM) as its standard. The two bodies have declared that they are working to-gether to fuse the standards so that there could be a single industry-wide standard. \\nY ou need to be aware of these efforts towards the worthwhile goal of metadata stan-\\ndards. Also, please note the following highlights of these initiatives as they relate to datawarehouse metadata:\\n/L50539The standard model provides metadata concepts for database schema management,\\ndesign, and reuse in a data warehouse environment. It includes both logical andphysical database concepts.\\n/L50539The model includes details of data transformations applicable to populating data\\nwarehouses.\\n/L50539The model can be extended to include OLAP-specific metadata types capturing de-\\nscriptions of data cubes.\\n/L50539The standard model contains details for specifying source and target schemas and\\ndata transformations between those regularly found in the data acquisition process-es in the data warehouse environment. This type of metadata can be used to supporttransformation design, impact analysis (which transformations are affected by agiven schema change), and data lineage (which data sources and transformationswere used to produce given data in the data warehouse).\\n/L50539The transformation component of the standard model captures information about\\ncompound data transformation scripts. Individual transformations have relation-198 THE SIGNIFICANT ROLE OF METADATA', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='73c54102-5980-4ae9-ba46-9455d2f7c529', embedding=None, metadata={'page_label': '219', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ships to the sources and targets of the transformation. Some transformation seman-\\ntics may be captured by constraints and by code–decode sets for table-driven map-pings. \\nImplementation Options\\nEnough has been said about the absolute necessity of metadata in a data warehouse envi-\\nronment. At the same time, we have noted the need for integration and standards for meta-data. Associated with these two facts is the reality of the lack of universally acceptedmetadata standards. Therefore, in a typical data warehouse environment where multipletools from different vendors are used, what are the options for implementing metadatamanagement? In this section, we will explore a few random options. We have to hope,however, that the goal of universal standards will be met soon.\\nPlease review the following options and consider the ones most appropriate for your\\ndata warehouse environment.\\n/L50539Select and use a metadata repository product with its business information directory\\ncomponent. Y our information access and data acquisition tools that are compatiblewith the repository product will seamlessly interface with it. For the other tools thatare not compatible, you will have to explore other methods of integration.\\n/L50539In the opinion of some data warehouse consultants, a single centralized repository is\\na restrictive approach jeopardizing the autonomy of individual processes. Althougha centralized repository enables sharing of metadata, it cannot be easily adminis-tered in a large data warehouse. In the decentralized approach, metadata is spreadacross different parts of the architecture with several private and unique metadatastores. Metadata interchange could be a problem.\\n/L50539Some developers have come up with their own solutions. They come up with a set of\\nprocedures for the standard usage of each tool in the development environment andprovide a table of contents.\\n/L50539Other developers create their own database to gather and store metadata and publish\\nit on the company’ s intranet.\\n/L50539Some adopt clever methods of integration of information access and analysis tools.\\nThey provide side-by-side display of metadata by one tool and display of the realdata by another tool. Sometimes, the help texts in the query tools may be populatedwith the metadata exported from a central repository. \\nAs you know, the current trend is to use Web technology for reporting and OLAP func-\\ntions. The company’ s intranet is widely used as the means for information delivery. Figure9-12 shows how this paradigm shift changes the way metadata may be accessed. Businessusers can use their Web browsers to access metadata and navigate through the data ware-house and any data marts.\\nFrom the outset, pay special attention to metadata for your data warehouse environ-\\nment. Prepare a metadata initiative to answer the following questions:\\nWhat are the goals for metadata in your enterprise?\\nWhat metadata is required to meet the goals?What are the sources for metadata in your environment?HOW TO PROVIDE METADATA 199', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='19545fc7-8da6-47e8-a541-967d1a789626', embedding=None, metadata={'page_label': '220', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Who will maintain it?\\nHow will they maintain it?What are the metadata standards?How will metadata be used? By whom?What metadata tools will be needed?\\nSet your goals for metadata in your environment and follow through.\\nCHAPTER SUMMARY\\n/L50539Metadata is a critical need for using, building, and administering the data warehouse. \\n/L50539For end-users, metadata is like a roadmap to the data warehouse contents.\\n/L50539For IT professionals, metadata supports development and administration functions.\\n/L50539Metadata has an active role in the data warehouse and assists in the automation of\\nthe processes. \\n/L50539Metadata types may be classified by the three functional areas of the data ware-\\nhouse, namely, data acquisition, data storage, and information delivery. The typesare linked to the processes that take places in these three areas.\\n/L50539Business metadata connects the business users to the data warehouse. Technical\\nmetadata is meant for the IT staff responsible for development and administration.\\n/L50539Effective metadata must meet a number of requirements. Metadata management is\\ndifficult; many challenges need to be faced. 200 THE SIGNIFICANT ROLE OF METADATA\\nWarehouse data\\nMetadata RepositoryODBC\\nJDBCAPI\\nCGI \\nGateway\\nFigure 9-12 Metadata: web-based access.Web ClientWeb Client\\nBrowserBrowser\\nWeb Server', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0775379b-5ecf-412f-8641-e99411cf0215', embedding=None, metadata={'page_label': '221', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Universal metadata standardization is still an elusive goal. Lack of standardization\\ninhibits seamless passing of metadata from one tool to another.\\n/L50539A metadata repository is like a general-purpose information directory that includes\\nseveral enhancing functions.\\n/L50539One metadata implementation option includes the use of a commercial metadata\\nrepository. There are other possible home-grown options. \\nREVIEW QUESTIONS\\n1. Why do you think metadata is important in a data warehouse environment? Give a\\ngeneral explanation in one or two paragraphs. \\n2. Explain how metadata is critical for data warehouse development and administra-\\ntion.\\n3. Examine the concept that metadata is like a nerve center. Describe how the con-\\ncept applies to the data warehouse environment.\\n4. List and describe three major reasons why metadata is vital for end-users.5. Why is metadata essential for IT? List six processes in which metadata is signifi-\\ncant for IT and explain why.\\n6. Pick three processes in which metadata assists in the automation of these process-\\nes. Show how metadata plays an active role in these processes.\\n7. What is meant by establishing the context of information? Briefly explain with an\\nexample how metadata establishes the context of information in a data warehouse.\\n8. List four metadata types used in each of the three areas of data acquisition, data\\nstorage, and information delivery. \\n9. List any ten examples of business metadata.\\n10. List four major requirements that metadata must satisfy. Describe each of these\\nfour requirements.\\nEXERCISES\\n1. Indicate if true or false:\\nA. The importance of metadata is the same in a data warehouse as it is in an opera-\\ntional system.\\nB. Metadata is needed by IT for data warehouse administration.C. Technical metadata is usually less structured than business metadata.D. Maintaining metadata in a modern data warehouse is just for documentation.E. Metadata provides information on predefined queries.F . Business metadata comes from sources more varied than those for technical\\nmetadata.\\nG. Technical metadata is shared between business users and IT staff.H. A metadata repository is like a general purpose directory tool.EXERCISES 201', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cb825da2-68ec-4b96-a8bb-f8f8f5713cf7', embedding=None, metadata={'page_label': '222', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='I. Metadata standards facilitate metadata interchange among tools.\\nJ. Business metadata is only for business users; business metadata cannot be un-\\nderstood or used by IT staff.\\n2. As the project manager for the development of the data warehouse for a domestic\\nsoft drinks manufacturer, your assignment is to write a proposal for providing meta-data. Consider the options and come up with what you think is needed and how youplan to implement a metadata strategy.\\n3. As the data warehouse administrator, describe all the types of metadata you would\\nneed for performing your job. Explain how these types would assist you.\\n4. Y ou are responsible for training the data warehouse end-users. Write a short proce-\\ndure for your casual end-users to use the business metadata and run queries. De-scribe the procedure in user terms without using the word metadata.\\n5. As the data acquisition specialist, what types of metadata can help you? Choose one\\nof the data acquisition processes and explain the role of metadata in that process.202 THE SIGNIFICANT ROLE OF METADATA', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c4d2bc87-b71a-4656-95f1-7131081de747', embedding=None, metadata={'page_label': '223', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 10\\nPRINCIPLES OF \\nDIMENSIONAL MODELING\\nCHAPTER OBJECTIVES\\n/L50539Clearly understand how the requirements definition determines data design\\n/L50539Introduce dimensional modeling and contrast it with entity-relationship modeling\\n/L50539Review the basics of the STAR schema\\n/L50539Find out what is inside the fact table and inside the dimension tables\\n/L50539Determine the advantages of the STAR schema for data warehouses\\nFROM REQUIREMENTS TO DATA DESIGN\\nThe requirements definition completely drives the data design for the data warehouse.\\nData design consists of putting together the data structures. A group of data elementsform a data structure. Logical data design includes determination of the various data el-ements that are needed and combination of the data elements into structures of data.Logical data design also includes establishing the relationships among the data struc-tures.\\nLet us look at Figure 10-1. Notice how the phases start with requirements gathering.\\nThe results of the requirements gathering phase is documented in detail in the require-ments definition document. An essential component of this document is the set of infor-mation package diagrams. Remember that these are information matrices showing themetrics, business dimensions, and the hierarchies within individual business dimensions.\\nThe information package diagrams form the basis for the logical data design for the\\ndata warehouse. The data design process results in a dimensional data model. \\n203Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3c46974f-0354-46cb-aaca-29d110263a31', embedding=None, metadata={'page_label': '224', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Design Decisions\\nBefore we proceed with designing the dimensional data model, let us quickly review some\\nof the design decisions you have to make:\\nChoosing the process. Selecting the subjects from the information packages for the\\nfirst set of logical structures to be designed.\\nChoosing the grain. Determining the level of detail for the data in the data structures.\\nIdentifying and conforming the dimensions. Choosing the business dimensions\\n(such as product, market, time, etc.) to be included in the first set of structures andmaking sure that each particular data element in every business dimension is con-formed to one another. \\nChoosing the facts. Selecting the metrics or units of measurements (such as product\\nsale units, dollar sales, dollar revenue, etc.) to be included in the first set of structures.\\nChoosing the duration of the database. Determining how far back in time you\\nshould go for historical data.\\nDimensional Modeling Basics\\nDimensional modeling gets its name from the business dimensions we need to incorpo-\\nrate into the logical data model. It is a logical design technique to structure the businessdimensions and the metrics that are analyzed along these dimensions. This modeling tech-nique is intuitive for that purpose. The model has also proved to provide high performancefor queries and analysis.204 PRINCIPLES OF DIMENSIONAL MODELING\\nRequirements\\nGathering\\nData\\nDesignRequirements\\nDefinition\\nDocument\\nInformation\\nPackages\\nDimen -\\nsional\\nModel………\\n………\\n………\\n………\\n……………\\n……\\nFigure 10-1 From requirements to data design.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fe9095f5-29a1-4320-8e38-f539d0053641', embedding=None, metadata={'page_label': '225', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The multidimensional information package diagram we have discussed is the founda-\\ntion for the dimensional model. Therefore, the dimensional model consists of the specificdata structures needed to represent the business dimensions. These data structures alsocontain the metrics or facts. \\nIn Chapter 5, we discussed information package diagrams in sufficient detail. We\\nspecifically looked at an information package diagram for automaker sales. Please goback and review Figure 5-5 in that chapter. What do you see? In the bottom section of thediagram, you observe the list of measurements or metrics that the automaker wants to usefor analysis. Next, look at the column headings. These are the business dimensions alongwhich the automaker wants to analyze the measurements or metrics. Under each columnheading you see the dimension hierarchies and categories within that business dimension.What you see under each column heading are the attributes relating to that business di-mension.\\nReviewing the information package diagram for automaker sales, we notice three types\\nof data entities: (1) measurements or metrics, (2) business dimensions, and (3) attributesfor each business dimension. So when we put together the dimensional model to representthe information contained in the automaker sales information package, we need to comeup with data structures to represent these three types of data entities. Let us discuss howwe can do this.\\nFirst, let us work with the measurements or metrics seen at the bottom of the informa-\\ntion package diagram. These are the facts for analysis. In the automaker sales diagram, thefacts are as follows:\\nActual sale price\\nMSRP sale priceOptions priceFull priceDealer add-onsDealer creditsDealer invoiceAmount of downpaymentManufacturer proceedsAmount financed\\nEach of these data items is a measurement or fact. Actual sale price is a fact about what\\nthe actual price was for the sale. Full price is a fact about what the full price was relatingto the sale. As we review each of these factual items, we find that we can group all ofthese into a single data structure. In relational database terminology, you may call the datastructure a relational table. So the metrics or facts from the information package diagramwill form the fact table. For the automaker sales analysis this fact table would be the au-tomaker sales fact table.\\nLook at Figure 10-2 showing how the fact table is formed. The fact table gets its name\\nfrom the subject for analysis; in this case, it is automaker sales. Each fact item or mea-surement goes into the fact table as an attribute for automaker sales.\\nWe have determined one of the data structures to be included in the dimensional model\\nfor automaker sales and derived the fact table from the information package diagram. LetFROM REQUIREMENTS TO DATA DESIGN 205', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b4638983-2e40-41a4-b648-11cfe26ecf50', embedding=None, metadata={'page_label': '226', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='us now move on to the other sections of the information package diagram, taking the busi-\\nness dimensions one by one. Look at the product business dimension in Figure 5-5.\\nThe product business dimension is used when we want to analyze the facts by prod-\\nucts. Sometimes our analysis could be a breakdown by individual models. Another analy-sis could be at a higher level by product lines. Y et another analysis could be at even a high-er level by product categories. The list of data items relating to the product dimension areas follows:\\nModel name\\nModel yearPackage stylingProduct lineProduct categoryExterior colorInterior colorFirst model year\\nWhat can we do with all these data items in our dimensional model? All of these relate\\nto the product in some way. We can, therefore, group all of these data items in one datastructure or one relational table. We can call this table the product dimension table. Thedata items in the above list would all be attributes in this table.\\nLooking further into the information package diagram, we note the other business di-206 PRINCIPLES OF DIMENSIONAL MODELING\\nFacts : Actual Sale Price, MSRP Sale Price, Options Price, Full Price, Dealer   \\nAdd-ons, Dealer Credits, Dealer Invoice, Down Payment, Proceeds, FinanceTime ProductPayment \\nMethodCustomer \\nDemo-\\ngraphics\\nYearDimensions\\nQuarter\\nMonth\\nDate\\nDay of \\nWeek\\nDay of \\nMonth\\nSeason\\nHoliday \\nFlagModel \\nName\\nModel \\nYear\\nPackage \\nStyling\\nProduct \\nLine\\nProduct \\nCategory\\nExterior \\nColor\\nInterior \\nColor\\nFirst YearFinance \\nType\\nTerm \\n(Months)\\nInterest \\nRate\\nAgentDealer\\nAge\\nGender\\nIncome \\nRange\\nMarital \\nStatus\\nHouse-\\nhold Size\\nVehicles \\nOwned\\nHome \\nValue\\nOwn or \\nRentDealer \\nName\\nCity\\nState\\nSingle \\nBrand Flag\\nDate First \\nOperationActual Sale Price        \\nMSRP Sale Price \\nOptions Price    Full \\nPrice      Dealer  \\nAdd-ons Dealer \\nCredits Dealer \\nInvoice Down \\nPayment Proceeds \\nFinanceAutomaker \\nSales\\nFact Table\\nFigure 10-2 Formation of the automaker sales fact table.Actual Sale Price\\nMSRP Sale Price\\nOptions Price\\nFull Price\\nDealer Add-ons\\nDealer Credits\\nDealer Invoice\\nDown Payment\\nProceeds\\nFinance', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='befdcce8-03bc-42e3-9f7a-921555e0ca63', embedding=None, metadata={'page_label': '227', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='mensions shown as column headings. In the case of the automaker sales information\\npackage diagram, these other business dimensions are dealer, customer demographics,payment method, and time. Just as we formed the product dimension table, we can formthe remaining dimension tables of dealer, customer demographics, payment method, andtime. The data items shown within each column would then be the attributes for each cor-responding dimension table.\\nFigure 10-3 puts all of this together. It shows how the various dimension tables are\\nformed from the information package diagram. Look at the figure closely and see howeach dimension table is formed.\\nSo far we have formed the fact table and the dimension tables. How should these tables\\nbe arranged in the dimensional model? What are the relationships and how should wemark the relationships in the model? The dimensional model should primarily facilitatequeries and analyses. What would be the types of queries and analyses? These would bequeries and analyses where the metrics inside the fact table are analyzed across one ormore dimensions using the dimension table attributes.\\nLet us examine a typical query against the automaker sales data. How much sales pro-\\nceeds did the Jeep Cherokee, Y ear 2000 Model with standard options, generate in January2000 at Big Sam Auto dealership for buyers who own their homes and who took 3-year leas-es, financed by Daimler-Chrysler Financing? We are analyzing actual sale price, MSRPsale price, and full price. We are analyzing these facts along attributes in the various di-mension tables. The attributes in the dimension tables act as constraints and filters in ourFROM REQUIREMENTS TO DATA DESIGN 207\\nFacts : Actual Sale Price, MSRP Sale Price, Options Price, Full Price, Dealer   \\nAdd-ons, Dealer Credits, Dealer Invoice, Down Payment, Proceeds, FinanceTime ProductPayment \\nMethodCustomer \\nDemo-\\ngraphics\\nYear\\nQuarter\\nMonth\\nDate\\nDay of \\nWeek\\nDay of \\nMonth\\nSeason\\nHoliday \\nFlagModel \\nName\\nModel \\nYear\\nPackage \\nStyling\\nProduct \\nLine\\nProduct \\nCategory\\nExterior \\nColor\\nInterior \\nColor\\nFirst YearFinance \\nType\\nTerm \\n(Months)\\nInterest \\nRate\\nAgentDealer\\nAge\\nGender\\nIncome \\nRange\\nMarital \\nStatus\\nHouse-\\nhold Size\\nVehicles \\nOwned\\nHome \\nValue\\nOwn or \\nRentDealer \\nName\\nCity\\nState\\nSingle \\nBrand Flag\\nDate First \\nOperationModel Name       \\nModel Year         \\nPackage Styling    \\nProduct Line         \\nProduct Category \\nExterior Color    \\nInterior Color        \\nFirst Year  Dimension  Tables  \\nYear                      \\nQuarter …………..Product  \\nTime  \\nFinance Type    \\nTerm ………….....\\nAge                   \\nGender …………..Payment \\nMethod  \\nCustomer \\nDemo-graphics  Dealer Name          \\nCity …………..Dealer \\nFigure 10-3 Formation of the automaker dimension tables.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b4e76108-b3d1-4cf9-9ad4-fde333fc55f1', embedding=None, metadata={'page_label': '228', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='queries. We also find that any or all of the attributes of each dimension table can participate\\nin a query. Further, each dimension table has an equal chance to be part of a query. \\nBefore we decide how to arrange the fact and dimension tables in our dimensional\\nmodel and mark the relationships, let us go over what the dimensional model needs toachieve and what its purposes are. Here are some of the criteria for combining the tablesinto a dimensional model.\\n/L50539The model should provide the best data access. \\n/L50539The whole model must be query-centric. \\n/L50539It must be optimized for queries and analyses.\\n/L50539The model must show that the dimension tables interact with the fact table.\\n/L50539It should also be structured in such a way that every dimension can interact equally\\nwith the fact table.\\n/L50539The model should allow drilling down or rolling up along dimension hierarchies.\\nWith these requirements, we find that a dimensional model with the fact table in the\\nmiddle and the dimension tables arranged around the fact table satisfies the conditions. Inthis arrangement, each of the dimension tables has a direct relationship with the fact tablein the middle. This is necessary because every dimension table with its attributes musthave an even chance of participating in a query to analyze the attributes in the fact table.\\nSuch an arrangement in the dimensional model looks like a star formation, with the\\nfact table at the core of the star and the dimension tables along the spikes of the star. Thedimensional model is therefore called a STAR schema. \\nLet us examine the STAR schema for the automaker sales as shown in Figure 10-4. The\\nsales fact table is in the center. Around this fact table are the dimension tables of product,208 PRINCIPLES OF DIMENSIONAL MODELING\\nAUTO\\nSALESDEALERPRODUCT\\nTIME\\nPAYMENT\\nMETHODCUSTOMER\\nDEMO -\\nGRAPHICS\\nFigure 10-4 STAR schema for automaker sales.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dd4beaf2-cce8-4fac-bbf1-eb6c222a076d', embedding=None, metadata={'page_label': '229', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='dealer, customer demographics, payment method, and time. Each dimension table is relat-\\ned to the fact table in a one-to-many relationship. In other words, for one row in the prod-uct dimension table, there are one or more related rows in the fact table. \\nE-R Modeling Versus Dimensional Modeling\\nWe are familiar with data modeling for operational or OLTP systems. We adopt the Enti-\\nty-Relationship (E-R) modeling technique to create the data models for these systems.Figure 10-5 lists the characteristics of OLTP systems and shows why E-R modeling issuitable for OLTP systems. \\nWe have so far discussed the basics of the dimensional model and find that this model\\nis most suitable for modeling the data for the data warehouse. Let us recapitulate the char-acteristics of the data warehouse information and review how dimensional modeling issuitable for this purpose. Let us study Figure 10-6.\\nUse of CASE Tools\\nMany case tools are available for data modeling. In Chapter 8, we introduced these tools\\nand their features. Y ou can use these tools for creating the logical schema and the physicalschema for specific target database management systems (DBMSs). \\nY ou can use a case tool to define the tables, the attributes, and the relationships. Y ou\\ncan assign the primary keys and indicate the foreign keys. Y ou can form the entity-rela-tionship diagrams. All of this is done very easily using graphical user interfaces and pow-erful drag-and-drop facilities. After creating an initial model, you may add fields, deletefields, change field characteristics, create new relationships, and make any number of re-visions with utmost ease. \\nAnother very useful function found in the case tools is the ability to forward-engineerFROM REQUIREMENTS TO DATA DESIGN 209\\n/G75OLTP systems capture details of events or transactions\\n/G75OLTP systems focus on individual events\\n/G75An OLTP system is a window into micro-level transactions\\n/G75Picture at detail level necessary to run the business \\n/G75Suitable only for questions at transaction level\\n/G75Data consistency, non-redundancy, and efficient data \\nstorage critical \\nEntity-Relationship Modeling\\nRemoves data redundancy              \\nEnsures data consistency                  Expresses microscopic \\nrelationships        \\nFigure 10-5 E-R modeling for OLTP systems.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7286d77f-32e7-41a8-ad18-d3bbd059d7cf', embedding=None, metadata={'page_label': '230', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='the model and generate the schema for the target database system you need to work with.\\nForward-engineering is easily done with these case tools.\\nFor modeling the data warehouse, we are interested in the dimensional modeling tech-\\nnique. Most of the existing vendors have expanded their modeling case tools to include di-mensional modeling. Y ou can create fact tables, dimension tables, and establish the rela-tionships between each dimension table and the fact table. The result is a STAR schemafor your model. Again, you can forward-engineer the dimensional STAR model into a re-lational schema for your chosen database management system. \\nTHE STAR SCHEMA\\nNow that you have been introduced to the STAR schema, let us take a simple example and\\nexamine its characteristics. Creating the STAR schema is the fundamental data designtechnique for the data warehouse. It is necessary to gain a good grasp of this technique.\\nReview of a Simple STAR Schema\\nWe will take a simple STAR schema designed for order analysis. Assume this to be the\\nschema for a manufacturing company and that the marketing department is interested indetermining how they are doing with the orders received by the company.\\nFigure 10-7 shows this simple STAR schema. It consists of the orders fact table shown\\nin the middle of schema diagram. Surrounding the fact table are the four dimension tablesof customer, salesperson, order date, and product. Let us begin to examine this STARschema. Look at the structure from the point of view of the marketing department. Theusers in this department will analyze the orders using dollar amounts, cost, profit margin,and sold quantity. This information is found in the fact table of the structure. The users210 PRINCIPLES OF DIMENSIONAL MODELING\\n/G75DW meant to answer questions on overall process\\n/G75DW focus is on how managers view the business\\n/G75DW reveals business trends\\n/G75Information is centered around a business process \\n/G75Answers show how the business measures the process\\n/G75The measures to be studied in many ways along several \\nbusiness dimensions \\nDimensional Modeling\\nCaptures critical measures              \\nViews along dimensions                  Intuitive to business users        \\nFigure 10-6 Dimensional modeling for the data warehouse.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='80e8a5cc-4497-4601-84be-9c3f2e63589d', embedding=None, metadata={'page_label': '231', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='will analyze these measurements by breaking down the numbers in combinations by cus-\\ntomer, salesperson, date, and product. All these dimensions along which the users will an-alyze are found in the structure. The STAR schema structure is a structure that can be eas-ily understood by the users and with which they can comfortably work. The structuremirrors how the users normally view their critical measures along their business dimen-sions. \\nWhen you look at the order dollars, the STAR schema structure intuitively answers the\\nquestions of what, when, by whom, and to whom. From the STAR schema, the users caneasily visualize the answers to these questions: For a given amount of dollars, what wasthe product sold? Who was the customer? Which salesperson brought the order? Whenwas the order placed?\\nWhen a query is made against the data warehouse, the results of the query are pro-\\nduced by combining or joining one of more dimension tables with the fact table. The joinsare between the fact table and individual dimension tables. The relationship of a particularrow in the fact table is with the rows in each dimension table. These individual relation-ships are clearly shown as the spikes of the STAR schema.\\nTake a simple query against the STAR schema. Let us say that the marketing depart-\\nment wants the quantity sold and order dollars for product bigpart-1, relating to cus-\\ntomers in the state of Maine, obtained by salesperson Jane Doe, during the month of June.\\nFigure 10-8 shows how this query is formulated from the STAR schema. Constraints andfilters for queries are easily understood by looking at the STAR schema. \\nA common type of analysis is the drilling down of summary numbers to get at the de-\\ntails at the lower levels. Let us say that the marketing department has initiated a specificanalysis by placing the following query: Show me the total quantity sold of product brandbig parts to customers in the Northeast Region for year 1999. In the next step of the\\nanalysis, the marketing department now wants to drill down to the level of quarters in\\n1999 for the Northeast Region for the same product brand, big parts. Next, the analysis\\ngoes down to the level of individual products in that brand. Finally, the analysis goes tothe level of details by individual states in the Northeast Region. The users can easily dis-THE STAR SCHEMA 211\\nFigure 10-7 Simple STAR schema for orders analysis.Product\\nProduct Name\\nSKU\\nBrand\\nOrder Measures\\nOrder Dollars\\nCost\\nMargin Dollars\\nQuantity Sold\\nOrder Date\\nDate\\nMonth\\nQuarter\\nYearCustomer\\nCustomer Name\\nCustomer Code\\nBilling Address\\nShipping Address\\nSalesperson\\nSalesperson Name\\nTerritory Name\\nRegion Name', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2017d28b-bf9d-4624-b71d-e54e3b062712', embedding=None, metadata={'page_label': '232', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='cern all of this drill-down analysis by reviewing the STAR schema. Refer to Figure 10-9\\nto see how the drill-down is derived from the STAR schema.\\nInside a Dimension Table\\nWe have seen that a key component of the STAR schema is the set of dimension tables.\\nThese dimension tables represent the business dimensions along which the metrics are an-alyzed. Let us look inside a dimension table and study its characteristics. Please see Fig-ure 10-10 and review the following observations.\\nDimension table key. Primary key of the dimension table uniquely identifies each row\\nin the table.\\nTable is wide. Typically, a dimension table has many columns or attributes. It is not un-\\ncommon for some dimension tables to have more than fifty attributes. Therefore, wesay that the dimension table is wide. If you lay it out as a table with columns androws, the table is spread out horizontally.\\nTextual attributes. In the dimension table you will seldom find any numerical values\\nused for calculations. The attributes in a dimension table are of textual format.212 PRINCIPLES OF DIMENSIONAL MODELING\\nFigure 10-8 Understanding a query from the STAR schema.Product Name\\n= bigpart-1State = Maine\\nMonth = JuneSalesperson Name\\n= Jane DoeProduct\\nProduct Name\\nSKU\\nBrand\\nOrder Measures\\nOrder Dollars\\nCost\\nMargin Dollars\\nQuantity Sold\\nOrder Date\\nDate\\nMonth\\nQuarter\\nYearCustomer\\nCustomer Name\\nCustomer Code\\nBilling Address\\nShipping Address\\nSalesperson\\nSalesperson Name\\nTerritory Name\\nRegion Name', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='70169db3-74b1-46eb-9ad6-3987fcd7a1b3', embedding=None, metadata={'page_label': '233', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='THE STAR SCHEMA 213\\n\\x0cDimension table key\\n\\x0cLarge number of attributes (wide)\\n\\x0cTextual attributes\\n\\x0cAttributes not directly related\\n\\x0cFlattened out, not normalized\\n\\x0cAbility to drill down / roll up\\n\\x0cMultiple hierarchies\\n\\x0cLess number of records\\nFigure 10-10 Inside a dimension table.Figure 10-9 Understanding drill-down analysis from the STAR schema.\\nBrand=big parts\\nYear=1999\\nRegion Name \\n= North East1999 1st Qtr.     \\n1999 2nd Qtr.    1999 3rd Qtr.      \\n1999 4th Qtr.Brand=big parts\\nRegion Name = North EastProduct=bigpart1 \\nProduct=bigpart2  \\n………………..\\n1999 1st Qtr.     \\n1999 2nd Qtr.    1999 3rd Qtr.      \\n1999 4th Qtr.\\nRegion Name = North EastState=Maine \\nState=New York   \\n………………. Product=bigpart1 \\nProduct=bigpart2  \\n………………..\\n1999 1st Qtr.     \\n1999 2nd Qtr.    1999 3rd Qtr.      \\n1999 4th Qtr.STEP 1STEP 4STEP 3\\nSTEP 2DRILL DOWN STEPSProduct\\nProduct Name\\nSKU\\nBrandOrder Measures\\nOrder Dollars\\nCost\\nMargin Dollars\\nQuantity SoldOrder Date\\nDate\\nMonth\\nQuarter\\nYearCustomer\\nCustomer Name\\nCustomer Code\\nBilling Address\\nShipping Address\\nSalesperson\\nSalesperson Name\\nTerritory Name\\nRegion Name\\nCustomer\\ncumstomer_key\\nname\\ncustomer_id\\nbilling_address\\nbilling_city\\nbilling_state\\nbilling_zip\\nshipping_address', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1b5ce0a7-c221-45e7-827f-1fdb59848624', embedding=None, metadata={'page_label': '234', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='These attributes represent the textual descriptions of the components within the\\nbusiness dimensions. Users will compose their queries using these descriptors.\\nAttributes not directly related. Frequently you will find that some of the attributes in\\na dimension table are not directly related to the other attributes in the table. For ex-ample, package size is not directly related to product brand; nevertheless, packagesize and product brand could both be attributes of the product dimension table.\\nNot normalized. The attributes in a dimension table are used over and over again in\\nqueries. An attribute is taken as a constraint in a query and applied directly to themetrics in the fact table. For efficient query performance, it is best if the query picksup an attribute from the dimension table and goes directly to the fact table and notthrough other intermediary tables. If you normalize the dimension table, you will becreating such intermediary tables and that will not be efficient. Therefore, a dimen-sion table is flattened out, not normalized. \\nDrilling down, rolling up. The attributes in a dimension table provide the ability to get\\nto the details from higher levels of aggregation to lower levels of details. For exam-ple, the three attributes zip, city, and state form a hierarchy. Y ou may get the totalsales by state, then drill down to total sales by city, and then by zip. Going the otherway, you may first get the totals by zip, and then roll up to totals by city and state.\\nMultiple hierarchies. In the example of the customer dimension, there is a single hier-\\narchy going up from individual customer to zip, city, and state. But dimension tablesoften provide for multiple hierarchies, so that drilling down may be performedalong any of the multiple hierarchies. Take for example a product dimension tablefor a department store. In this business, the marketing department may have its wayof classifying the products into product categories and product departments. On theother hand, the accounting department may group the products differently into cate-gories and product departments. So in this case, the product dimension table willhave the attributes of marketing–product–category, marketing–product–department,finance–product–category, and finance–product–department.\\nFewer number of records. A dimension table typically has fewer number of records or\\nrows than the fact table. A product dimension table for an automaker may have just500 rows. On the other hand, the fact table may contain millions of rows.\\nInside the Fact Table\\nLet us now get into a fact table and examine the components. Remember this is where we\\nkeep the measurements. We may keep the details at the lowest possible level. In the de-partment store fact table for sales analysis, we may keep the units sold by individual trans-actions at the cashier’ s checkout. Some fact tables may just contain summary data. Theseare called aggregate fact tables. Figure 10-11 lists the characteristics of a fact table. Let usreview these characteristics.\\nConcatenated Key. A row in the fact table relates to a combination of rows from all\\nthe dimension tables. In this example of a fact table, you find quantity ordered as anattribute. Let us say the dimension tables are product, time, customer, and sales rep-resentative. For these dimension tables, assume that the lowest level in the dimen-sion hierarchies are individual product, a calendar date, a specific customer, and asingle sales representative. Then a single row in the fact table must relate to a partic-214 PRINCIPLES OF DIMENSIONAL MODELING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='50e792ad-d91b-454a-8d0a-b30c9e392c50', embedding=None, metadata={'page_label': '235', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ular product, a specific calendar date, a specific customer, and an individual sales\\nrepresentative. This means the row in the fact table must be identified by the prima-ry keys of these four dimension tables. Thus, the primary key of the fact table mustbe the concatenation of the primary keys of all the dimension tables. \\nData Grain. This is an important characteristic of the fact table. As we know, the\\ndata grain is the level of detail for the measurements or metrics. In this example, themetrics are at the detailed level. The quantity ordered relates to the quantity of aparticular product on a single order, on a certain date, for a specific customer, andprocured by a specific sales representative. If we keep the quantity ordered as thequantity of a specific product for each month, then the data grain is different and isat a higher level. \\nFully Additive Measures. Let us look at the attributes order_dollars, extended_cost,\\nand quantity_ordered. Each of these relates to a particular product on a certain date\\nfor a specific customer procured by an individual sales representative. In a certainquery, let us say that the user wants the totals for the particular product on a certaindate, not for a specific customer, but for customers in a particular state. Then weneed to find all the rows in the fact table relating to all the customers in that stateand add the order_dollars, extended_cost, and quantity_ordered to come up with\\nthe totals. The values of these attributes may be summed up by simple addition.Such measures are known as fully additive measures. Aggregation of fully additivemeasures is done by simple addition. When we run queries to aggregate measures inthe fact table, we will have to make sure that these measures are fully additive. Oth-erwise, the aggregated numbers may not show the correct totals. \\nSemiadditive Measures. Consider the margin_dollars attribute in the fact table. For\\nexample, if the order_dollars is 120 and extended_cost is 100, the margin_percent-\\nageis 20. This is a calculated metric derived from the order_dollars and extended_\\ncost. If you are aggregating the numbers from rows in the fact table relating to all\\nthe customers in a particular state, you cannot add up the margin_percentages from\\nall these rows and come up with the aggregated number. Derived attributes such asTHE STAR SCHEMA 215\\nFigure 10-11 Inside a fact table.\\x0cConcatenated fact table key\\n\\x0cGrain or level of data identified\\n\\x0cFully additive measures\\n\\x0cSemi -additive measures\\n\\x0cLarge number of records\\n\\x0cOnly a few attributes\\n\\x0cSparsity of data\\n\\x0cDegenerate dimensions\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4dd9fbae-c670-499d-ada5-7754b542b2ba', embedding=None, metadata={'page_label': '236', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='margin_percentage are not additive. They are known as semiadditive measures.\\nDistinguish semiadditive measures from fully additive measures when you performaggregations in queries.\\nTable Deep, Not Wide. Typically a fact table contains fewer attributes than a dimen-\\nsion table. Usually, there are about 10 attributes or less. But the number of recordsin a fact table is very large in comparison. Take a very simplistic example of 3 prod-ucts, 5 customers, 30 days, and 10 sales representatives represented as rows in thedimension tables. Even in this example, the number of fact table rows will be 4500,very large in comparison with the dimension table rows. If you lay the fact table outas a two-dimensional table, you will note that the fact table is narrow with a smallnumber of columns, but very deep with a large number of rows. \\nSparse Data. We have said that a single row in the fact table relates to a particular\\nproduct, a specific calendar date, a specific customer, and an individual sales repre-sentative. In other words, for a particular product, a specific calendar date, a specif-ic customer, and an individual sales representative, there is a corresponding row inthe fact table. What happens when the date represents a closed holiday and no or-ders are received and processed? The fact table rows for such dates will not havevalues for the measures. Also, there could be other combinations of dimension tableattributes, values for which the fact table rows will have null measures. Do we needto keep such rows with null measures in the fact table? There is no need for this.Therefore, it is important to realize this type of sparse data and understand that thefact table could have gaps. \\nDegenerate Dimensions. Look closely at the example of the fact table. Y ou find the\\nattributes of order_number and order_line. These are not measures or metrics or\\nfacts. Then why are these attributes in the fact table? When you pick up attributesfor the dimension tables and the fact tables from operational systems, you will beleft with some data elements in the operational systems that are neither facts norstrictly dimension attributes. Examples of such attributes are reference numbers likeorder numbers, invoice numbers, order line numbers, and so on. These attributes areuseful in some types of analyses. For example, you may be looking for averagenumber of products per order. Then you will have to relate the products to the ordernumber to calculate the average. Attributes such as order_number and order_line in\\nthe example are called degenerate dimensions and these are kept as attributes of thefact table.\\nThe Factless Fact Table\\nApart from the concatenated primary key, a fact table contains facts or measures. Let us\\nsay we are building a fact table to track the attendance of students. For analyzing studentattendance, the possible dimensions are student, course, date, room, and professor. The at-tendance may be affected by any of these dimensions. When you want to mark the atten-dance relating to a particular course, date, room, and professor, what is the measurementyou come up for recording the event? In the fact table row, the attendance will be indicat-ed with the number one. Every fact table row will contain the number one as attendance.\\nIf so, why bother to record the number one in every fact table row? There is no need to do\\nthis. The very presence of a corresponding fact table row could indicate the attendance.This type of situation arises when the fact table represents events. Such fact tables really216 PRINCIPLES OF DIMENSIONAL MODELING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='10088970-1ce9-4bbc-9868-d8650751e5a1', embedding=None, metadata={'page_label': '237', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='do not need to contain facts. They are “factless” fact tables. Figure 10-12 shows a typical\\nfactless fact table.\\nData Granularity\\nBy now, we know that granularity represents the level of detail in the fact table. If the fact\\ntable is at the lowest grain, then the facts or metrics are at the lowest possible level atwhich they could be captured from the operational systems. What are the advantages ofkeeping the fact table at the lowest grain? What is the trade-off?\\nWhen you keep the fact table at the lowest grain, the users could drill down to the low-\\nest level of detail from the data warehouse without the need to go to the operational sys-tems themselves. Base level fact tables must be at the natural lowest levels of all corre-sponding dimensions. By doing this, queries for drill-down and roll-up can be performedefficiently. \\nWhat then are the natural lowest levels of the corresponding dimensions? In the exam-\\nple with the dimensions of product, date, customer, and sales representative, the naturallowest levels are an individual product, a specific individual date, an individual customer,and an individual sales representative, respectively. So, in this case, a single row in thefact table should contain measurements at the lowest level for an individual product, or-dered on a specific date, relating to an individual customer, and procured by an individualsales representative.\\nLet us say we want to add a new attribute of district in the sales representative dimen-\\nsion. This change will not warrant any changes in the fact table rows because these are al-ready at the lowest level of individual sales representative. This is a “graceful” change be-cause all the old queries will continue to run without any changes. Similarly, let us assumewe want to add a new dimension of promotion. Now you will have to recast the fact tablerows to include promotion dimensions. Still, the fact table grain will be at the lowest lev-THE STAR SCHEMA 217\\nMeasures or facts are represented in a fact table. However, there are \\nbusiness events or coverage that could be represented in a fact table, although no measures or facts are associated with these.\\nDate Key\\nCourse KeyProfessor KeyStudent KeyRoom KeyDate Dimension\\nCourse Dimension\\nStudent DimensionProfessor Dimension\\nRoom Dimension\\nFigure 10-12 Factless fact table.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d27cc688-7a6a-4ff1-8405-b28832b71b03', embedding=None, metadata={'page_label': '238', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='el. Even here, the old queries will still run without any changes. This is also a “graceful”\\nchange. Fact tables at the lowest grain facilitate “graceful” extensions. \\nBut we have to pay the price in terms of storage and maintenance for the fact table at\\nthe lowest grain. Lowest grain necessarily means large numbers of fact table rows. Inpractice, however, we build aggregate fact tables to support queries looking for summarynumbers. \\nThere are two more advantages of granular fact tables. Granular fact tables serve as\\nnatural destinations for current operational data that may be extracted frequently from op-erational systems. Further, the more recent data mining applications need details at thelowest grain. Data warehouses feed data into data mining applications.\\nSTAR SCHEMA KEYS\\nFigure 10-13 illustrates how the keys are formed for the dimension and fact tables.\\nPrimary Keys\\nEach row in a dimension table is identified by a unique value of an attribute designated as\\nthe primary key of the dimension. In a product dimension table, the primary key identifieseach product uniquely. In the customer dimension table, the customer number identifieseach customer uniquely. Similarly, in the sales representative dimension table, the socialsecurity number of the sales representative identifies each sales representative.\\nWe have picked these out as possible candidate keys for the dimension tables. Now let\\nus consider some implications of these candidate keys. Let us assume that the product218 PRINCIPLES OF DIMENSIONAL MODELING\\nSTORE KEY\\nPRODUCT KEY\\nTIME KEY\\nDollars\\nUnitsProduct Dimension\\nTime DimensionStore Dimension\\nSTORE KEY \\nStore Desc     \\nDistrict ID      \\nDistrict Desc   \\nRegion ID    \\nRegion Desc \\nLevelFact Table\\nFact Table :   Compound primary key, one \\nsegment for each dimension \\nDimension Table :      Generated primary key \\nFigure 10-13 The STAR schema keys.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b7f3a0f2-7b6c-4e7a-9769-94b23d5a45c6', embedding=None, metadata={'page_label': '239', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='code in the operational system is an 8-position code, two of which positions indicate the\\ncode of the warehouse where the product is normally stored, and two other positions de-note the product category. Let us see what happens if we use the operational system prod-uct code as the primary key for the product dimension table.\\nThe data warehouse contains historic data. Assume that the product code gets changed\\nin the middle of a year, because the product is now stored in a different warehouse of thecompany. So we have to change the product code in the data warehouse. If the productcode is the primary key of the product dimension table, then the newer data for the sameproduct will reside in the data warehouse with different key values. This could cause prob-lems if we need to aggregate the data from before the change with the data from after thechange to the product code. What really has caused this problem? The problem is the re-sult of our decision to use the operational system key as the key for the dimension table. \\nSurrogate Keys\\nHow do we resolve the problem faced in the previous section? Can we use production sys-\\ntem keys as primary keys for dimension tables? If not, what are the other candidate keys?\\nThere are two general principles to be applied when choosing primary keys for dimen-\\nsion tables. The first principle is derived from the problem caused when the product beganto be stored in a different warehouse. In other words, the product key in the operationalsystem has built-in meanings. Some positions in the operational system product key indi-cate the warehouse and some other positions in the key indicate the product category.These are built-in meanings in the key. The first principle to follow is: avoid built-inmeanings in the primary key of the dimension tables.\\nIn some companies, a few of the customers are no longer listed with the companies.\\nThey could have left their respective companies many years ago. It is possible that thecustomer numbers of such discontinued customers are reassigned to new customers. Now,let us say we had used the operational system customer key as the primary key for the cus-tomer dimension table. We will have a problem because the same customer number couldrelate to the data for the newer customer and also to the data of the retired customer. Thedata of the retired customer may still be used for aggregations and comparisons by cityand state. Therefore, the second principle is: do not use production system keys as prima-ry keys for dimension tables. \\nWhat then should we use as primary keys for dimension tables? The answer is to use\\nsurrogate keys. The surrogate keys are simply system-generated sequence numbers. Theydo not have any built-in meanings. Of course, the surrogate keys will be mapped to theproduction system keys. Nevertheless, they are different. The general practice is to keepthe operational system keys as additional attributes in the dimension tables. Please referback to Figure 10-13. The STORE KEY is the surrogate primary key for the store dimen-sion table. The operational system primary key for the store reference table may be kept asjust another nonkey attribute in the store dimension table.\\nForeign Keys\\nEach dimension table is in a one-to-many relationship with the central fact table. So the\\nprimary key of each dimension table must be a foreign key in the fact table. If there arefour dimension tables of product, date, customer, and sales representative, then the prima-ry key of each of these four tables must be present in the orders fact table as foreign keys. STAR SCHEMA KEYS 219', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='da45bff7-c011-4fc2-a8d2-973308f987b5', embedding=None, metadata={'page_label': '240', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Let us reexamine the primary keys for the fact tables. There are three options:\\n1.A single compound primary key whose length is the total length of the keys of the\\nindividual dimension tables. Under this option, in addition to the compound prima-\\nry key, the foreign keys must also be kept in the fact table as additional attributes.This option increases the size of the fact table. \\n2.Concatenated primary key that is the concatenation of all the primary keys of the\\ndimension tables. Here you need not keep the primary keys of the dimension tables\\nas additional attributes to serve as foreign keys. The individual parts of the primarykeys themselves will serve as the foreign keys.\\n3.A generated primary key independent of the keys of the dimension tables. In addi-\\ntion to the generated primary key, the foreign keys must also be kept in the facttable as additional attributes. This option also increases the size of the fact table. \\nIn practice, option (2) is used in most fact tables. This option enables you to easily re-\\nlate the fact table rows with the dimension table rows. \\nADVANTAGES OF THE STAR SCHEMA\\nWhen you look at the STAR schema, you find that it is simply a relational model with a\\none-to-many relationship between each dimension table and the fact table. What is so spe-cial about the arrangement of the STAR schema? Why is it declared to be eminently suit-able for the data warehouse? What are the reasons for its wide use and success in provid-ing optimization for processing queries?\\nAlthough the STAR schema is a relational model, it is not a normalized model. The di-\\nmension tables are purposely denormalized. This is a basic difference between the STARschema and relational schemas for OLTP systems.\\nBefore we discuss some very significant advantages of the STAR schema, we need to\\nbe aware that strict adherence to this arrangement is not always the best option. For exam-ple, if customer is one of the dimensions and if the enterprise has a very large number ofcustomers, a denormalized customer dimension table is not desirable. A large dimensiontable may increase the size of the fact table correspondingly. \\nHowever, the advantages far outweigh any shortcomings. So, let us go over the advan-\\ntages of the STAR schema.\\nEasy for Users to Understand\\nUsers of OLTP systems interact with the applications through predefined GUI screens or\\npreset query templates. There is practically no need for the users to understand the datastructures behind the scenes. The data structures and the database schema remain in therealm of IT professionals. \\nUsers of decision support systems such as data warehouses are different. Here the\\nusers themselves will formulate queries. When they interact with the data warehousethrough third-party query tools, the users should know what to ask for. They must gaina familiarity with what data is available to them in the data warehouse. They must havean understanding of the data structures and how the various pieces are associated with220 PRINCIPLES OF DIMENSIONAL MODELING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a104be4e-8021-4d61-940f-f5b021f0a799', embedding=None, metadata={'page_label': '241', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='one another in the overall scheme. They must comprehend the connections without dif-\\nficulty.\\nThe STAR schema reflects exactly how the users think and need data for querying and\\nanalysis. They think in terms of significant business metrics. The fact table contains themetrics. The users think in terms of business dimensions for analyzing the metrics. Thedimension tables contain the attributes along which the users normally query and analyze.When you explain to the users that the units of product A are stored in the fact table andpoint out the relationship of this piece of data to each dimension table, the users readilyunderstand the connections. That is because the STAR schema defines the join paths inexactly the same way users normally visualize the relationships. The STAR schema is in-tuitively understood by the users. \\nTry to walk a user through the relational schema of an OLTP system. For them to un-\\nderstand the connections, you will have to take them through a maze of normalized tables,sometimes passing through several tables, one by one, to get even the smallest result set.The STAR schema emerges as a clear winner because of its simplicity. Users understandthe structures and the connections very easily.\\nThe STAR schema has definite advantages after implementation. However, the advan-\\ntages even in the development stage cannot be overlooked. Because the users understandthe STAR schema so very well, it is easy to use it as a vehicle for communicating with theusers during the development of the data warehouse.\\nOptimizes Navigation\\nIn a database schema, what is the purpose of the relationships or connections among the\\ndata entities? The relationships are used to go from one table to another for obtaining theinformation you are looking for. The relationships provide the ability to navigate throughthe database. Y ou hop from table to table using the join paths. \\nIf the join paths are numerous and convoluted, your navigation through the database\\ngets difficult and slow. On the other hand, if the join paths are simple and straightforward,your navigation is optimized and becomes faster.\\nA major advantage of the STAR schema is that it optimizes the navigation through the\\ndatabase. Even when you are looking for a query result that is seemingly complex, thenavigation is still simple and straightforward. Let us look at an example and understandhow this works. Please look at Figure 10-14 showing a STAR schema for analyzing de-fects in automobiles. Assume you are the service manager at an automobile dealershipselling GM automobiles. Y ou noticed a high incidence of chipped white paint on theCorvettes in January 2000. Y ou need a tool to analyze such defects, determine the under-lying causes, and resolve the problems. \\nIn the STAR schema, the number of defects is kept as metrics in the middle as part of\\nthe defects fact table. The time dimension contains the model year. The component di-mension has part information; for example, pearl white paint. The problem dimension car-ries the types of problems; for example, chipped paint. The product dimension containsthe make, model, and trim package of the automobiles. The supplier dimension containsdata on the suppliers of parts.\\nNow see how easy it is to determine the supplier causing the chipped paint on the pearl\\nwhite Corvettes. Look at the four arrows pointing to the fact table from the four dimen-sion tables. These arrows show how you will navigate to the rows in the fact table by iso-ADVANTAGES OF THE STAR SCHEMA 221', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='49e17fb7-00d3-4499-b59d-441bb8993cfa', embedding=None, metadata={'page_label': '242', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='lating the Corvette from the product dimension, chipped paint from the problem dimen-\\nsion, pearl white paint from the component dimension, and January 2000 from the timedimension. From the fact table, the navigation goes directly to the supplier dimension toisolate the supplier causing the problem. \\nMost Suitable for Query Processing\\nWe have already mentioned a few times that the STAR schema is a query-centric struc-\\nture. This means that the STAR schema is most suitable for query processing. Let us seehow this is true.\\nLet us form a simple query on the STAR schema for the order analysis shown in Figure\\n10-7. What is the total extended cost of product A sold to customers in San Francisco dur-ing January 2000? This is a three-dimensional query. What should be the characteristicsof the data structure or schema if it is to be most suitable for processing this query? Thefinal result, which is the total extended cost, will come from the rows in the fact table. Butfrom which rows? The answer is those rows relating to product A, relating to customers inSan Francisco, and relating to January 2000.\\nLet us see how the query will be processed. First, select the rows from the customer di-\\nmension table where the city is San Francisco. Then, from the fact table, select only thoserows that are related to these customer dimension rows. This is the first result set of rowsfrom the fact tables. Next, select the rows in the Time dimension table where the month isJanuary 2000. Select from the first result set only those rows that are related to these timedimension rows. This is now the second result set of fact table rows. Move on to the nextdimension of product. Select the rows in the product dimension table where the product isproduct A. Select from the second result only those rows that are related to the selected222 PRINCIPLES OF DIMENSIONAL MODELING\\nDEFECTS\\nPROBLEMCOMPO -\\nNENTTIME\\nPRO -\\nDUCT\\nSUPPLIER\\nFigure 10-14 The STAR schema optimizes navigation.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4cd8c0d3-314e-4845-ad3a-af5ed6f9b89c', embedding=None, metadata={'page_label': '243', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='product dimension rows. Y ou now have the final result of fact table rows. Add up the ex-\\ntended cost to get the total.\\nIrrespective of the number of dimensions that participate in the query and irrespec-\\ntive of the complexity of the query, every query is simply executed first by selectingrows from the dimension tables using the filters based on the query parameters and thenfinding the corresponding fact table rows. This is possible because of the simple andstraightforward join paths and because of the very arrangement of the STAR schema.There is no intermediary maze to be navigated to reach the fact table from the dimen-sion tables. \\nAnother important aspect of data warehouse queries is the ability to drill down or roll\\nup. Let us quickly run through a drill down scenario. Let us say we have queried and ob-tained the total extended cost for all the customers in the state of California. The resultcomes from the set of fact table rows. Then we want to drill down and look at the resultsby Zip Code ranges. This is obtained by making a further selection from the selected facttable rows relating to the chosen Zip Code ranges. Drill down is a process of further selec-tion of the fact table rows. Going the other way, rolling up is a process of expanding theselection of the fact table rows.\\nSTARjoin and STARindex\\nThe STAR schema allows the query processor software to use better execution plans. It\\nenables specific performance schemes to be applied to queries. The STAR schemaarrangement is eminently suitable for special performance techniques such as the STAR-join and the STARindex. \\nSTARjoin is a high-speed, single-pass, parallelizable, multitable join. It can join more\\nthan two tables in a single operation. This special scheme boosts query performance.\\nSTARindex is a specialized index to accelerate join performance. These are indexes\\ncreated on one or more foreign keys of the fact table. These indexes speed up joins be-tween the dimension tables and the fact table.\\nWe will discuss these further in Chapter 18, which deals with the physical design of the\\ndata warehouse.\\nCHAPTER SUMMARY\\n/L50539The components of the dimensional model are derived from the information pack-\\nages in the requirements definition.\\n/L50539The entity-relationship modeling technique is not suitable for data warehouses; the\\ndimensional modeling technique is appropriate.\\n/L50539The STAR schema used for data design is a relational model consisting of fact and\\ndimension tables. \\n/L50539The fact table contains the business metrics or measurements; the dimensional ta-\\nbles contain the business dimensions. Hierarchies within each dimension table areused for drilling down to lower levels of data.\\n/L50539STAR schema advantages are: easy for users to understand, optimizes navigation,\\nmost suitable for query processing, and enables specific performance schemes.CHAPTER SUMMARY 223', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3bfffa28-2937-4080-b070-3af3caaca0ec', embedding=None, metadata={'page_label': '244', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='REVIEW QUESTIONS\\n1. Discuss the major design issues that need to be addressed before proceeding with\\nthe data design.\\n2. Why is the entity-relationship modeling technique not suitable for the data ware-\\nhouse? How is dimensional modeling different?\\n3. What is the STAR schema? What are the component tables?4. A dimension table is wide; the fact table is deep. Explain.5. What are hierarchies and categories as applicable to a dimension table?6. Differentiate between fully additive and semiadditive measures.7. Explain the sparse nature of the data in the fact table.8. Describe the composition of the primary keys for the dimension and fact tables.9. Discuss data granularity in a data warehouse.\\n10. Name any three advantages of the STAR schema. Can you think of any disadvan-\\ntages of the STAR schema?\\nEXERCISES\\n1. Match the columns:\\n1. information package A. enable drill-down\\n2. fact table B. reference numbers3. case tools C. level of detail4. dimension hierarchies D. users understand easily5. dimension table E. semiadditive6. degenerate dimensions F . STAR schema components7. profit margin percentage G. used for dimensional modeling8. data granularity H. dimension attribute 9. STAR schema I. contains metrics\\n10. customer demographics J. wide \\n2. Refer back to the information package given for a hotel chain in Chapter 5 (Figure\\n5-6). Use this information package and design a STAR schema.\\n3. What is a factless fact table? Design a simple STAR schema with a factless fact\\ntable to track patients in a hospital by diagnostic procedures and time. \\n4. Y ou are the data design specialist on the data warehouse project team for a manu-\\nfacturing company. Design a STAR schema to track the production quantities. Pro-duction quantities are normally analyzed along the business dimensions of product,time, parts used, production facility, and production run. State your assumptions.\\n5. In a STAR schema to track the shipments for a distribution company, the following\\ndimension tables are found: (1) time, (2) customer ship-to, (3) ship-from, (4) prod-uct, (5) type of deal, and (6) mode of shipment. Review these dimensions and listthe possible attributes for each of the dimension tables. Also, designate a primarykey for each table.224 PRINCIPLES OF DIMENSIONAL MODELING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='60cad040-ce2d-42e9-8af5-295f04221e4d', embedding=None, metadata={'page_label': '245', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 11\\nDIMENSIONAL MODELING: \\nADVANCED TOPICS\\nCHAPTER OBJECTIVES\\n/L50539Discuss and get a good grasp of slowly changing dimensions\\n/L50539Understand large dimensions and how to deal with them\\n/L50539Examine the snowflake schema in detail\\n/L50539Learn about aggregate tables and determine when to use them\\n/L50539Completely survey families of STARS and their applications\\nFrom the previous chapter, you have learned the basics of dimensional modeling. Y ou\\nknow that the STAR schema is composed of the fact table in the middle surrounded by thedimension tables. Although this is a good visual representation, it is still a relational mod-el in which each dimension table is in a parent–child relationship with the fact table. Theprimary key of each dimension table, therefore, is a foreign key in the fact table.\\nY ou have also grasped the nature of the attributes within the fact table and the dimen-\\nsion tables. Y ou have understood the advantages of the STAR schema in decision supportsystems. The STAR schema is easy for the users to understand; it optimizes navigationthrough the data warehouse content and is most suitable for query-centric environments. \\nOur study of dimensional modeling will not be complete until we consider some\\nmore topics. In the STAR schema, the dimension tables enable the analysis in many dif-ferent ways. We need to explore the dimension tables in further detail. How about sum-marizing the metrics and storing aggregate numbers in additional fact tables? How muchprecalculated aggregation is necessary? The STAR schema is a denormalized design.Does this result in too much redundancy and inefficiency? If so, is there an alternativeapproach?\\nLet us now move beyond the basics of dimensional modeling and consider additional\\n225Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8e64735a-f7a3-4e9c-ab56-ab865e9d335a', embedding=None, metadata={'page_label': '246', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='features and issues. Let us discuss the pertinent advanced topics and extend our study fur-\\nther. \\nUPDATES TO THE DIMENSION TABLES\\nGoing back to Figure 10-4 of the previous chapter, you see the STAR schema for au-\\ntomaker sales. The fact table Auto Sales contains the measurements or metrics such as Ac-\\ntual Sale Price, Options Price, and so on. Over time, what happens to the fact table?\\nEvery day as more and more sales take place, more and more rows get added to the facttable. The fact table continues to grow in the number of rows over time. Very rarely are therows in a fact table updated with changes. Even when there are adjustments to the priornumbers, these are also processed as additional adjustment rows and added to the facttable.\\nNow consider the dimension tables. Compared to the fact table, the dimension tables\\nare more stable and less volatile. However, unlike the fact table, which changes throughthe increase in the number of rows, a dimension table does not change just through the in-crease in the number of rows, but also through changes to the attributes themselves.\\nLook at the product dimension table. Every year, rows are added as new models be-\\ncome available. But what about the attributes within the product dimension table? If a par-ticular product is moved to a different product category, then the corresponding valuesmust be changed in the product dimension table. Let us examine the types of changes thataffect dimension tables and discuss the ways for dealing with these types. \\nSlowly Changing Dimensions\\nIn the above example, we have mentioned a change to the product dimension table be-\\ncause the product category for a product was changed. Consider the customer demograph-ics dimension table. What happens when a customer’ s status changes from rental home toown home? The corresponding row in that dimension table must be changed. Next, look atthe payment method dimension table. When finance type changes for one of the paymentmethods, this change must be reflected in the payment method dimension table.\\nFrom the consideration of the changes to the dimension tables, we can derive the fol-\\nlowing principles:\\n/L50539Most dimensions are generally constant over time\\n/L50539Many dimensions, though not constant over time, change slowly\\n/L50539The product key of the source record does not change\\n/L50539The description and other attributes change slowly over time\\n/L50539In the source OLTP systems, the new values overwrite the old ones\\n/L50539Overwriting of dimension table attributes is not always the appropriate option in a\\ndata warehouse\\n/L50539The ways changes are made to the dimension tables depend on the types of changes\\nand what information must be preserved in the data warehouse\\nThe usual changes to dimension tables may be classified into three distinct types. We\\nwill discuss these three types in detail. Y ou will understand why you must use different226 DIMENSIONAL MODELING: ADVANCED TOPICS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9351e15f-191c-47ac-8c3c-10c7bff51fc6', embedding=None, metadata={'page_label': '247', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='techniques for making the changes falling into these different types. Data warehousing\\npractitioners have come up with different techniques for applying the changes. They havealso given names to these three types of dimension table changes. Y es, your guess is right.The given names are Type 1 changes, Type 2 changes, and Type 3 changes.\\nWe will study these three types by using a simple STAR schema for tracking orders for\\na distributor of industrial products, as shown in Figure 11-1. This STAR schema consistsof the fact table and four dimension tables. Let us assume some changes to these dimen-sions and review the techniques for applying the changes to the dimension tables.\\nType 1 Changes: Correction of Errors\\nNature of Type 1 Changes. These changes usually relate to the corrections of errors\\nin the source systems. For example, suppose a spelling error in the customer name is cor-rected to read as Michael Romano from the erroneous entry of Michel Romano. Also,suppose the customer name for another customer is changed from Kristin Daniels toKristin Samuelson, and the marital status changed from single to married. \\nConsider the changes to the customer name in both cases. There is no need to preserve\\nthe old values. In the case of Michael Romano, the old name is erroneous and needs to bediscarded. When the users need to find all the orders from Michael Romano, the userswill use the correct name. The same principles apply to the change in customer name forKristin Samuelson.\\nBut the change in the marital status is slightly different. This change can be handled in\\nthe same way as the change in customer name only if that change is a correction of error.Otherwise, you will cause problems when the users want to analyze orders by marital sta-tus.UPDATES TO THE DIMENSION TABLES 227\\nFigure 11-1 STAR Schema for order tracking.Product Key  \\nTime Key     \\nCustomer Key  \\nSalesperson Key  \\nOrder Dollars   \\nCost Dollars      \\nMargin Dollars  \\nSale UnitsORDER FACTSCUSTOMER\\nSALESPERSONPRODUCT\\nTIMECustomer Key \\nCustomer Name \\nCustomer Code  \\nMarital Status   \\nAddress         \\nState                 \\nZip\\nSalesperson Key \\nSalesperson Name  \\nTerritory Name    \\nRegion NameTime Key       \\nDate            \\nMonth        \\nQuarter         \\nYearProduct Key  \\nProduct Name  \\nProduct Code  \\nProduct Line    \\nBrand', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9b9aa3bc-d0ae-42e3-a06e-7958c16c2272', embedding=None, metadata={'page_label': '248', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Here are the general principles for Type 1 changes:\\n/L50539Usually, the changes relate to correction of errors in source systems\\n/L50539Sometimes the change in the source system has no significance\\n/L50539The old value in the source system needs to be discarded\\n/L50539The change in the source system need not be preserved in the data warehouse\\nApplying Type 1 Changes to the Data Warehouse. Please look at Figure 11-2\\nshowing the application of Type 1 changes to the customer dimension table. The methodfor applying Type 1 changes is:\\n/L50539Overwrite the attribute value in the dimension table row with the new value\\n/L50539The old value of the attribute is not preserved\\n/L50539No other changes are made in the dimension table row\\n/L50539The key of this dimension table or any other key values are not affected\\n/L50539This type is easiest to implement\\nType 2 Changes: Preservation of History\\nNature of Type 2 Changes. Go back to the change in the marital status for Kristin\\nSamuelson. Assume that in your data warehouse one of the essential requirements is totrack orders by marital status in addition to tracking by other attributes. If the change tomarital status happened on October 1, 2000, all orders from Kristin Samuelson before that228 DIMENSIONAL MODELING: ADVANCED TOPICS\\nCustomer Key:\\nCustomer \\nName:\\nCustomer \\nCode:\\nMarital Status:Address:State:Zip:33154112Kristin Daniels\\nK12356\\nSingle\\n733 Jackie Lane,       \\nBaldwin Harbor            \\nNY \\n1151033154112Kristin Samuelson\\nK12356\\nSingle\\n733 Jackie Lane,       \\nBaldwin Harbor            \\nNJ \\n11510Customer Code:              \\nK12356\\nCustomer Name:            \\nKristin SamuelsonINCREMENTAL LOAD \\n-- TYPE 1 CHANGE\\n33154112          K12356\\nBEFORE AFTERKEY RESTRUCTURING\\nFigure 11-2 The method for applying Type 1 changes.Single Single\\nNY', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5da12424-6598-4370-a8a0-ed8169f47ef9', embedding=None, metadata={'page_label': '249', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='date must be included under marital status: single, and all orders on or after October 1,\\n2000 should be included under marital status: married. \\nWhat exactly is needed in this case? In the data warehouse, you must have a way of\\nseparating the orders for the customer so that the orders before and after that date can beadded up separately. \\nNow let us add another change to the information about Kristin Samuelson. Assume\\nthat she moved to a new address in California from her old address in New Y ork on No-vember 1, 2000. If it is a requirement in your data warehouse that you must be able totrack orders by state, then this change must also be treated like the change to marital sta-tus. Any orders prior to November 1, 2000 will go under the state: NY .\\nThe types of changes we have discussed for marital status and customer address are\\nType 2 changes. Here are the general principles for this type of change:\\n/L50539They usually relate to true changes in source systems\\n/L50539There is a need to preserve history in the data warehouse\\n/L50539This type of change partitions the history in the data warehouse\\n/L50539Every change for the same attribute must be preserved\\nApplying Type 2 Changes to the Data Warehouse. Please look at Figure 11-3\\nshowing the application of Type 2 changes to the customer dimension table. The methodfor applying Type 2 changes is:UPDATES TO THE DIMENSION TABLES 229\\nCustomer Key:\\nCustomer \\nName:\\nCustomer \\nCode:\\nMarital Status:Address:State:Zip:33154112Kristin Daniels\\nK12356\\nSingle\\n733 Jackie Lane,       \\nBaldwin Harbor            \\nNY\\n1151051141234Kristin Samuelson\\nK12356\\nMarried\\n733 Jackie Lane,       \\nBaldwin Harbor            \\nNY \\n11510Customer Code: K12356\\nMarital Status: Married\\nAddress: 1417 Ninth  Street ,                 \\nSacramento\\nState: CA Zip: 94236INCREMENTAL LOAD -- TYPE 2 \\nCHANGES ON 10/1/2000 & 11/1/2000\\n33154112            K12356   \\n51141234                   52789342\\nBEFOREAFTER -Eff. 10/1/2000KEY RESTRUCTURING\\n52789342\\nKristin Samuelson\\nK12356\\nMarried\\n1417 Ninth Street,    \\nSacramento            \\nCA\\n11510AFTER -Eff. 11/1/2000\\nFigure 11-3 The method for applying Type 2 changes.Single Married Married\\n94236', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3a17d4d2-1363-4b06-a8d4-359834d5d836', embedding=None, metadata={'page_label': '250', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Add a new dimension table row with the new value of the changed attribute\\n/L50539An effective date field may be included in the dimension table\\n/L50539There are no changes to the original row in the dimension table\\n/L50539The key of the original row is not affected \\n/L50539The new row is inserted with a new surrogate key\\nType 3 Changes: Tentative Soft Revisions\\nNature of Type 3 Changes. Almost all the usual changes to dimension values are\\neither Type 1 or Type 2 changes. Of these two, Type 1 changes are more common. Type 2changes preserve the history. When you apply a Type 2 change on a certain date, that dateis a cut-off point. In the above case of change to marital status on October 1, 2000, thatdate is the cut-off date. Any orders from the customer prior to that date fall into the olderorders group; orders on or after that date fall into the newer orders group. An order for thiscustomer has to fall in one or the other group; it cannot be counted in both groups for anyperiod of time. \\nWhat if you have the need to count the orders on or after the cut-off date in both groups\\nduring a certain period after the cut-off date? Y ou cannot handle this change as a Type 2change. Sometimes, though rarely, there is a need to track both the old and new values ofchanged attributes for a certain period, in both forward and backward directions. Thesetypes of changes are Type 3 changes.\\nType 3 changes are tentative or soft changes. An example will make this clearer. As-\\nsume your marketing department is contemplating a realignment of the territorial assign-ments for salespersons. Before making a permanent realignment, they want to count theorders in two ways: according to the current territorial alignment and also according to theproposed realignment. This type of provisional or tentative change is a Type 3 change.\\nAs an example, let us say you want to move salesperson Robert Smith from New Eng-\\nland territory to Chicago territory with the ability to trace his orders in both territories.Y ou need to track all orders through Robert Smith in both territories. \\nHere are the general principles for Type 3 changes:\\n/L50539They usually relate to “soft” or tentative changes in the source systems\\n/L50539There is a need to keep track of history with old and new values of the changed at-\\ntribute\\n/L50539They are used to compare performances across the transition\\n/L50539They provide the ability to track forward and backward\\nApplying Type 3 Changes to the Data Warehouse. Please look at Figure 11-4\\nshowing the application of Type 3 changes to the customer dimension table. The methodsfor applying Type 3 changes are:\\n/L50539Add an “old” field in the dimension table for the affected attribute\\n/L50539Push down the existing value of the attribute from the “current” field to the “old”\\nfield\\n/L50539Keep the new value of the attribute in the “current” field\\n/L50539Also, you may add a “current” effective date field for the attribute\\n/L50539The key of the row is not affected230 DIMENSIONAL MODELING: ADVANCED TOPICS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='40834fba-9e4c-4648-8c07-4a9de947e21b', embedding=None, metadata={'page_label': '251', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539No new dimension row is needed \\n/L50539The existing queries will seamlessly switch to the “current” value\\n/L50539Any queries that need to use the “old” value must be revised accordingly\\n/L50539The technique works best for one “soft” change at a time\\n/L50539If there is a succession of changes, more sophisticated techniques must be devised\\nMISCELLANEOUS DIMENSIONS\\nHaving considered the types of changes to dimension attributes and the ways to handle the\\ndimension changes in the data warehouse, let us now turn our attention to a few other im-portant issues about dimensions. One issue relates to dimension tables that are very wideand very deep. \\nIn our earlier discussion, we had assumed that dimension attributes do not change too\\nrapidly. If the change is a Type 2 change, you know that you have to create another rowwith the new value of the attribute. If the value of the attribute changes again, then youcreate another row with the newer value. What if the value changes too many times or toorapidly? Such a dimension is no longer a slowly changing dimension. What must you doabout a not-so-slowly-changing dimension? We will complete our discussion of dimen-sions by considering such relevant issues.\\nLarge Dimensions\\nY ou may consider a dimension large based on two factors. A large dimension is very deep;\\nthat is, the dimension has a very large number of rows. A large dimension may also beMISCELLANEOUS DIMENSIONS 231\\nSalesperson Key\\nSalesperson \\nName:\\nOld Territory \\nName:\\nCurrent \\nTerritory Name:\\nEffective Date: Region Name:12345Robert Smith\\nNew England          \\nJanuary 1, 1998 \\nNorthSalesperson ID:              \\nRS199701\\nTerritory Name:            \\nChicagoINCREMENTAL LOAD --\\nTYPE 3 CHANGE Eff. 12/1/2000\\n12345              RS199701\\nBEFORE AFTERKEY RESTRUCTURING\\n12345\\nRobert Smith\\nNew England\\nChicago          \\nDecember 1, 2000\\nNorth\\nFigure 11-4 Applying Type 3 changes.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='649db48b-6ccc-429d-99e6-c05b4cdc9187', embedding=None, metadata={'page_label': '252', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='very wide; that is, the dimension may have a large number of attributes. In either case, you\\nmay declare the dimension as large. There are special considerations for large dimensions.Y ou may have to attend to populating large-dimension tables in a special way. Y ou maywant to separate out some minidimensions from a large dimension. We will take a simpleSTAR schema designed for order analysis. Assume this to be the schema for a manufac-turing company and that the marketing department is interested in determining how theyare making progress with the orders received by the company.\\nIn a data warehouse, the customer and product dimensions are typically likely to be\\nlarge. Whenever an enterprise deals with the general public, the customer dimension isexpected to be gigantic. The customer dimension of a national retail chain can approachthe size of the number of the U.S. households. Such customer dimension tables may haveas many as 100 million rows. Next on the scale, the number of dimension table rows ofcompanies in telecommunications and travel industries may also run in the millions. Tenor twenty million customer rows is not uncommon. The product dimension of large retail-ers is also quite huge. \\nHere are some typical features of large customer and product dimensions:\\nCustomer\\n/L50539Huge—in the range of 20 million rows\\n/L50539Easily up to 150 dimension attributes\\n/L50539Can have multiple hierarchies\\nProduct\\n/L50539Sometimes as many as 100,000 product variations\\n/L50539Can have more than 100 dimension attributes\\n/L50539Can have multiple hierarchies\\nLarge dimensions call for special considerations. Because of the sheer size, many data\\nwarehouse functions involving large dimensions could be slow and inefficient. Y ou needto address the following issues by using effective design methods, by choosing proper in-dexes, and by applying other optimizing techniques:\\n/L50539Population of very large dimension tables\\n/L50539Browse performance of unconstrained dimensions, especially where the cardinality\\nof the attributes is low\\n/L50539Browsing time for cross-constrained values of the dimension attributes \\n/L50539Inefficiencies in fact table queries when large dimensions need to be used\\n/L50539Additional rows created to handle Type 2 slowly changing dimensions\\nMultiple Hierarchies. Large dimensions usually possess another distinct characteris-\\ntic. They tend to have multiple hierarchies. Take the example of the product dimension fora large retailer. One set of attributes may form the hierarchy for the marketing department.Users from that department use these attributes to drill down or up. In the same way, thefinance department may need to use their own set of attributes from the same product di-mension to drill down or up. Figure 11-5 shows multiple hierarchies within a large prod-uct dimension. 232 DIMENSIONAL MODELING: ADVANCED TOPICS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ecdd5feb-8a17-4042-8ebf-4e35eec204af', embedding=None, metadata={'page_label': '253', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Rapidly Changing Dimensions\\nAs you know, when you deal with a Type 2 change, you create an additional dimension\\ntable row with the new value of the changed attribute. By doing so, you are able to pre-serve the history. If the same attribute changes a second time, you create one more dimen-sion table row with the latest value.\\nMost product dimensions change very infrequently, maybe once or twice a year. If the\\nnumber of rows in such a product dimension is about 100,000 or so, using the approach ofcreating additional rows with the new values of the attributes is easily manageable. Evenif the number of rows is in the range of several thousands, the approach of applying thechanges as Type 2 changes is still quite feasible. \\nHowever, consider another dimension such as the customer dimension. Here the num-\\nber of rows tends to be large, sometimes in the range of even a million or more rows. If theattributes of a large number of rows change, but change infrequently, the Type 2 approachis not too difficult. But significant attributes in a customer dimension may change manytimes in a year. Rapidly changing large dimensions can be too problematic for the Type 2approach. The dimension table could be littered with a very large number of additionalrows created every time there is an incremental load. \\nBefore rushing to explore other options for handling rapidly changing large dimen-\\nsions, deal with each large dimension individually. The Type 2 approach is still good in aMISCELLANEOUS DIMENSIONS 233\\nProduct Key\\nProduct DescriptionProduct Source Key\\nProduct Line\\nProduct Group\\nBrand\\nVendor Make\\nSub-Category\\nCategory\\nMajor Group\\nDepartment\\nDivision\\nHemisphere\\nPackage Size\\nPackage Type\\nWeight\\nUnit of Measure\\nStackable\\nShelf Height\\nShelf Depth\\nShelf WidthPRODUCT \\nDIMENSIONHierarchy for \\nMarketingHierarchy for \\nFinance\\nFigure 11-5 Multiple hierarchies in a large product dimension.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0e40bf7d-8aba-495e-a345-7c1e54fc57a7', embedding=None, metadata={'page_label': '254', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STAR schema design. Here are some reasons why the Type 2 approach could work in\\nmany cases for rapidly changing dimensions:\\n/L50539When the dimension table is kept flat, it allows symmetric cross-browsing among\\nthe various attributes of the dimension.\\n/L50539Even when additional dimension table rows get created, the basic dimensional struc-\\nture is still preserved. The fact table is connected to all the dimension tables by for-eign keys. The advantages of the STAR schema are still available.\\n/L50539Only when the end-user queries are based on a changed attribute does the existence\\nof multiple rows for the same customer becomes apparent. For other queries, the ex-istence of multiple rows is practically hidden.\\nWhat if the dimension table is too large and is changing too rapidly? Then seek alterna-\\ntives to straightforward application of the Type 2 approach. One effective approach is tobreak the large dimension table into one or more simpler dimension tables. How can youaccomplish this?\\nObviously, you need to break off the rapidly changing attributes into another dimen-\\nsion table, leaving the slowly changing attributes behind in the original table. Figure 11-6shows how a customer dimension table may be separated into two dimension tables. Thefigure illustrates the general technique of separating out the rapidly changing attributes.Use this as a guidance when dealing with large, rapidly changing dimensions in your datawarehouse environment.234 DIMENSIONAL MODELING: ADVANCED TOPICS\\nCustomer Key (PK)\\nCustomer Name\\nAddress\\nState\\nZip \\nPhone\\n…………….\\n……………..\\nBehavior Key (PK)\\nCustomer Type\\nProduct Returns \\nCredit Rating\\nMarital Status\\nPurchases Range\\nLife Style\\nIncome Level\\nHome OwnershipCustomer \\nKey\\nBehavior Key\\nOther keys\\n…………...\\nMetricsCustomer \\nKey\\nOther keys\\n…………...\\nMetricsCustomer Key (PK)\\nCustomer Name\\nAddress\\nState\\nZip \\nCustomer Type\\nProduct Returns \\nCredit Rating\\nMarital Status\\nPurchases Range\\nLife Style\\nIncome Level\\nHome Ownership\\n…………….\\n……………..CUSTOMER \\nDIMENSION (Original)CUSTOMER \\nDIMENSION (New) \\nBEHAVIOR \\nDIMENSION (New) Any FACT tableAny FACT table\\nFigure 11-6 Dividing a large, rapidly changing dimension table.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='847afc42-d58b-4efa-8e86-2a923160890e', embedding=None, metadata={'page_label': '255', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Junk Dimensions\\nExamine your source legacy systems and review the individual fields in source data struc-\\ntures for customer, product, order, sales territories, promotional campaigns, and so on.Most of these fields wind up in the dimension tables. Y ou will notice that some fields likemiscellaneous flags and textual fields are left in the source data structures. These includeyes/no flags, textual codes, and free form texts.\\nSome of these flags and textual data may be too obscure to be of real value. These\\nmay be leftovers from past conversions from manual records created long ago. However,many of the flags and texts could be of value once in a while in queries. These may notbe included as significant fields in the major dimensions. At the same time, these flagsand texts cannot be discarded either. So, what are your options? Here are the mainchoices:\\n/L50539Exclude and discard all flags and texts. Obviously, this is not a good option for the\\nsimple reason that you are likely to throw away some useful information.\\n/L50539Place the flags and texts unchanged in the fact table. This option is likely to swell up\\nthe fact table to no specific advantage. \\n/L50539Make each flag and text a separate dimension table on its own. Using this option,\\nthe number of dimension tables will greatly increase. \\n/L50539Keep only those flags and texts that are meaningful; group all the useful flags into a\\nsingle “junk” dimension. “Junk” dimension attributes are useful for constrainingqueries based on flag/text values.\\nTHE SNOWFLAKE SCHEMA\\n“Snowflaking” is a method of normalizing the dimension tables in a STAR schema. When\\nyou completely normalize all the dimension tables, the resultant structure resembles asnowflake with the fact table in the middle. First, let us begin with Figure 11-7, whichshows a simple STAR schema for sales in a manufacturing company.\\nThe sales fact table contains quantity, price, and other relevant metrics. Sales rep, cus-\\ntomer, product, and time are the dimension tables. This is a classic STAR schema, denor-malized for optimal query access involving all or most of the dimensions. The model isnot in the third normal form. \\nOptions to Normalize\\nAssume that there are 500,000 product dimension rows. These products fall under 500\\nproduct brands and these product brands fall under 10 product categories. Now supposeone of your users runs a query constraining just on product category. If the product di-mension table is not indexed on product category, the query will have to search through500,000 rows. On the other hand, even if the product dimension is partially normalized byseparating out product brand and product category into separate tables, the initial searchfor the query will have to go through only 10 rows in the product category table. Figure11-8 illustrates this reduction in the search process.\\nIn Figure 11-8, we have not completely normalized the product dimension. We can alsoTHE SNOWFLAKE SCHEMA 235', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d037e772-7fcb-4fd8-855b-b1012cb2bf13', embedding=None, metadata={'page_label': '256', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='236 DIMENSIONAL MODELING: ADVANCED TOPICS\\nProduct Key \\nTime Key      \\nCustomer Key  \\nSalesRep Key \\nSales Quantity     \\nSales Dollars  \\nSales Price \\nMarginSALES FACTSCUSTOMER\\nSALESREPPRODUCT\\nTIMECustomer Key \\nCustomer Name \\nCustomer Code  \\nMarital Status   \\nAddress           \\nState                  \\nZip    \\nClassification\\nSalesrep Key \\nSalesperson Name  \\nTerritory Name    \\nRegion NameTime Key       \\nDate             \\nMonth        \\nQuarter        \\nYearProduct Key  \\nProduct Name  \\nProduct Code  \\nBrand Name \\nProduct Category \\nPackage Type\\nFigure 11-7 Sales: a simple STAR schema.\\nFigure 11-8 Product dimension: partially normalized.Product Key \\nTime Key      \\nCustomer Key  \\nSalesRep Key \\nSales Quantity     \\nSales Dollars  \\nSales Price \\nMarginSALES FACTSCUSTOMER\\nSALESREPPRODUCT\\nTIMECustomer Key \\nCustomer Name \\nCustomer Code  \\nMarital Status   \\nAddress          \\nState            \\nZip             \\nCountry\\nSalesrep Key \\nSalesperson Name  \\nTerritory Name    \\nRegion NameTime Key       \\nDate             \\nMonth        \\nQuarter        \\nYearProduct Key  \\nProduct Name  \\nProduct Code  \\nPackage Type  \\nBrand KeyCATEGORY\\nBrand Key   \\nBrand Name \\nCategory KeyCategory Key  \\nProduct CategoryBRAND', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='33e3e599-13dc-4783-8ef0-c68f6945880c', embedding=None, metadata={'page_label': '257', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='move other attributes out of the product dimension table and form normalized structures.\\n“Snowflaking” or normalization of the dimension tables can be achieved in a few differentways. When you want to “snowflake,” examine the contents and the normal usage of eachdimension table.\\nThe following options indicate the different ways you may want to consider for nor-\\nmalization of the dimension tables:\\n/L50539Partially normalize only a few dimension tables, leaving the others intact\\n/L50539Partially or fully normalize only a few dimension tables, leaving the rest intact\\n/L50539Partially normalize every dimension table\\n/L50539Fully normalize every dimension table\\nFigure 11-9 shows the version of the snowflake schema for sales in which every di-\\nmension table is partially or fully normalized. \\nThe original STAR schema for sales as shown in Figure 11-7 contains only five tables,\\nwhereas the normalized version now extends to eleven tables. Y ou will notice that in thesnowflake schema, the attributes with low cardinality in each original dimension table areremoved to form separate tables. These new tables are linked back to the original dimen-sion table through artificial keys. THE SNOWFLAKE SCHEMA 237\\nProduct Key \\nTime Key      \\nCustomer Key  \\nSalesRep Key \\nSales Quantity     \\nSales Dollars  \\nSales Price \\nMarginSALES FACTSCUSTOMER\\nSALESREPPRODUCT\\nTIMECustomer Key \\nCustomer Name \\nCustomer Code  \\nMarital Status   \\nAddress           \\nState                  \\nZip             \\nCountry Key\\nSalesrep Key \\nSalesperson Name  \\nTerritory KeyTime Key       \\nDate             \\nMonth        \\nQuarter        \\nYearProduct Key  \\nProduct Name  \\nProduct Code \\nPackage Key\\nBrand KeyBRAND\\nBrand Key   \\nBrand Name \\nCategory KeyCategory Key   \\nProduct CategoryCATEGORY\\nCountryKey   \\nCountry Name\\nTerritory Key   \\nTerritory Name \\nRegion KeyRegion Key   \\nRegion Name\\nPackage Key   \\nPackage TypeCOUNTRY\\nREGION\\nTERRITORYPACKAGE\\nFigure 11-9 Sales: “snowflake” schema.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ce25b1e9-e797-4eb8-b295-87c0f9bbbea6', embedding=None, metadata={'page_label': '258', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advantages and Disadvantages\\nY ou may want to snowflake for one obvious reason. By eliminating all the long text fields\\nfrom the dimension tables, you expect to save storage space. For example, if you have“men’ s furnishings” as one of the category names, that text will be repeated on everyproduct row in that category. At first blush, removing such redundancies might appear tosave significant storage space when the dimensions are large.\\nLet us assume that your product dimension table has 500,000 rows. By snowflaking\\nyou are able to remove 500,000 20-byte category names. At the same time, you have toadd a 4-byte artificial category key to the dimension table. The net savings work out to beapproximately 500,000 times 16, that is, about 8 MB. Y our average 500,000-row productdimension table occupies about 200 MB of storage space and the corresponding fact tableanother 20 GB. The savings are just 4%. Y ou will find that the small savings in space doesnot compensate for the other disadvantages of snowflaking.\\nHere is a brief summary of the advantages and limitations of snowflaking:\\nAdvantages\\n/L50539Small savings in storage space\\n/L50539Normalized structures are easier to update and maintain\\nDisadvantages\\n/L50539Schema less intuitive and end-users are put off by the complexity\\n/L50539Ability to browse through the contents difficult\\n/L50539Degraded query performance because of additional joins \\nSnowflaking is not generally recommended in a data warehouse environment. Query\\nperformance takes the highest significance in a data warehouse and snowflaking hampersthe performance. \\nWhen to Snowflake\\nAs an IT professional, you have an affinity for third normal form structures. We very well\\nknow all the problems unnormalized structures could cause. Further, wasted space couldbe another consideration for snowflaking. \\nIn spite of the apparent disadvantages, are there any circumstances under which\\nsnowflaking may be permissible? The principle behind snowflaking is normalization ofthe dimension tables by removing low cardinality attributes and forming separate tables.In a similar manner, some situations provide opportunities to separate out a set of attribut-es and form a subdimension. This process is very close to the snowflaking technique.Please look at Figure 11-10 showing how a demographic subdimension is formed out ofthe customer dimension.\\nAlthough forming subdimensions may be construed snowflaking, it makes a lot of\\nsense to separate out the demographic attributes into another table. Y ou usually load thedemographic data at different times from the times for the load of the other dimension at-tributes. The two sets of attributes differ in granularity. If the customer dimension is verylarge, running into millions of rows, the savings in storage space could be substantial. An-other valid reason for separating out the demographic attributes relates to the browsing of238 DIMENSIONAL MODELING: ADVANCED TOPICS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c21c7518-27c5-489f-a247-fc04683c4767', embedding=None, metadata={'page_label': '259', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='attributes. Users may browse the demographic attributes more than the others in the cus-\\ntomer dimension table. \\nAGGREGATE FACT TABLES\\nAggregates are precalculated summaries derived from the most granular fact table. These\\nsummaries form a set of separate aggregate fact tables. Y ou may create each aggregatefact table as a specific summarization across any number of dimensions. Let us begin byexamining a sample STAR schema. Choose a simple STAR schema with the fact table atthe lowest possible level of granularity. Assume there are four dimension tables surround-ing this most granular fact table. Figure 11-11 shows the example we want to examine.\\nWhen you run a query in an operational system, it produces a result set about a single\\ncustomer, a single order, a single invoice, a single product, and so on. But, as you know,the queries in a data warehouse environment produce large result sets. These queries re-trieve hundreds and thousands of table rows, manipulate the metrics in the fact tables, andthen produce the result sets. The manipulation of the fact table metrics may be a simpleaddition, an addition with some adjustments, a calculation of averages, or even an applica-tion of complex arithmetic algorithms.\\nLet us review a few typical queries against the sample STAR schema shown in Figure\\n11-11. \\nQuery 1: Total sales for customer number 12345678 during the first week of Decem-\\nber 2000 for product Widget-1.AGGREGATE FACT TABLES 239\\nCity Class Key (PK)\\nCity CodeClass Description Population RangeCost of LivingPollution IndexQuality of LifePublic TransportationRoads and StreetsParksCommerce IndexCustomer Key\\n…………..Other keys…………...MetricsCustomer Key (PK)\\nCustomer NameAddressStateZip City Class Key\\n……………..\\n…………….……………..CUSTOMER \\nDIMENSION\\nCITY \\nCLASSIFICATION Any FACT table\\nCITY CLASSIFICATION contains attributes to \\nclassify each city within a limited set of classes. \\nThese attributes are separated from the \\nCUSTOMER DIMENSION to form a separate \\nsub-dimension as CITY CLASSIFICATION .\\nFigure 11-10 Forming a subdimension.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cb863bf1-6f50-427d-b58a-cc4507515f88', embedding=None, metadata={'page_label': '260', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Query 2: Total sales for customer number 12345678 during the first three months of\\n2000 for product Widget-1.\\nQuery 3: Total sales for all customers in the South-Central territory for the first two\\nquarters of 2000 for product category Bigtools. \\nScrutinize these queries and determine how the totals will be calculated in each case.\\nThe totals will be calculated by adding the sales quantities and sales dollars from the qual-ifying rows of the fact table. In each case, let us review the qualifying rows that contributeto the total in the result set.\\nQuery 1: All fact table rows where the customer key relates to customer number\\n12345678, the product key relates to product Widget-1, and the time key re-lates to the seven days in the first week of December 2000. Assuming that acustomer may make at most one purchase of a single product in a single day,only a maximum of 7 fact table rows participate in the summation.\\nQuery 2: All fact table rows where the customer key relates to customer number\\n12345678, the product key relates to product Widget-1, and the time key re-lates to about 90 days of the first quarter of 2000. Assuming that a customermay make at most one purchase of a single product in a single day, onlyabout 90 fact table rows or less participate in the summation.\\nQuery 3: All fact table rows where the customer key relates to all customers in the\\nSouth-Central territory, the product key relates to all products in the productcategory Bigtools, and the time key relates to about 180 days in the first two240 DIMENSIONAL MODELING: ADVANCED TOPICS\\nProduct Key \\nTime Key      \\nCustomer Key  \\nSales Region Key \\nUnit Sales      \\nSales DollarsSALES FACTSCUSTOMER\\nSALES REGIONPRODUCT\\nTIMECustomer Key \\nCustomer Name \\nCustomer Code  \\nAddress           \\nState                  \\nZip\\nSales Region Key \\nTerritory Name    \\nRegion NameTime Key       \\nDate              \\nWeek Number             \\nMonth        \\nQuarter        \\nYearProduct Key  \\nProduct Name  \\nProduct Code  \\nProduct Category\\nGranularity :\\nOne fact table row per \\nday, for each product,  for each customer\\nFigure 11-11 STAR schema with most granular fact table.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='82d6cd5d-21da-4d12-a6bf-bfdfa42f460e', embedding=None, metadata={'page_label': '261', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='quarters of 2000. In this case, clearly a large number of fact table rows par-\\nticipate in the summation.\\nObviously, Query 3 will run long because of the large number of fact table rows to be\\nretrieved. What can be done to reduce the query time? This is where aggregate tables canbe helpful. Before we discuss aggregate fact tables in detail, let us review the sizes ofsome typical fact tables in real-world data warehouses.\\nFact Table Sizes\\nPlease see Figure 11-12. This represents the STAR schema for sales of a large supermar-\\nket chain. There are about two billion rows of the base fact table with the lowest level ofgranularity. Please study the calculations shown below:\\nTime dimension: 5 years × 365 days = 1825\\nStore dimension: 300 stores reporting daily salesProduct dimension: 40,000 products in each store (about 4000 sell in each store daily)Promotion dimension: a sold item may be in only one promotion in a store on a given\\nday\\nMaximum number of base fact table records: 1825 × 300 × 4000 × 1 = 2 billionAGGREGATE FACT TABLES 241\\nProduct Key \\nTime Key      \\nStore Key  \\nPromotion Key \\nUnit Sales     \\nDollar Sales \\nDollar CostSALES FACTSSTORE\\nPROMOTIONPRODUCT\\nTIMEStore Key      \\nStore Name \\nStore ID      \\nAddress    \\nCity  \\nState                  \\nZip              \\nDistrict       \\nManager  \\nFloor Plan     \\nServices Type\\nPromotion Key \\nProomotion Name  \\nPromotion Type    \\nDisplay Type    \\nCoupon Type     \\nMedia Type  \\nPromotion Cost    \\nStart Date            \\nEnd Date  \\nResponsible ManagerTime Key       \\nDate                 \\nDay of Week    \\nWeek Number             \\nMonth           \\nMonth Number        \\nQuarter \\nYear            \\nHoliday FlagProduct Key \\nSKU Number   \\nProduct Description    \\nBrand Name \\nProduct Sub-Category \\nProduct Category  \\nDepartment      \\nPackage Size   \\nPackage Type    \\nWeight                   \\nUnit of Measure  \\nUnits per case       \\nShelf  level            \\nShelf width            \\nShelf depth\\n5 years or 1,825 days300 stores40,000 products \\n(only 4,000 sell in each store daily) \\nA sold item \\nin only one promotion, per store, per day.2 billion fact table \\nrows\\nFigure 11-12 STAR schema: grocery chain.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b09c234e-ed17-4d57-88de-05585a42b872', embedding=None, metadata={'page_label': '262', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Here are a few more estimates of the fact table sizes in other typical cases:\\nTelephone Call Monitoring\\nTime dimension: 5 years = 1825 daysNumber of calls tracked each day: 150 millionMaximum number of base fact table records: 274 billion \\nCredit Card Transaction Tracking\\nTime dimension: 5 years = 60 monthsNumber of credit card accounts: 150 millionAverage number of monthly transactions per account: 20Maximum number of base fact table records: 180 billion \\nFrom the above examples you see the typical enormity of the fact tables that are at the\\nlowest level of granularity. Although none of the queries from the users would call for datajust from a single row in these fact tables, data at the lowest level of detail is needed. Thisis because when a user performs various forms of analysis, he or she must be able to getresult sets comprising of a variety of combinations of individual fact table rows. If you donot keep details by individual stores, you cannot retrieve result sets for products by indi-vidual stores. On the other hand, if you do not keep details by individual products, youcannot retrieve result sets for stores by individual products.\\nSo, here is the question. If you need detailed data at the lowest level of granularity in\\nthe base fact tables, how do you deal with summations of huge numbers of fact table rowsto produce query results? Consider the following queries related to a grocery chain datawarehouse:\\n/L50539How did the three new stores in Wisconsin perform during the last three months\\ncompared to the national average?\\n/L50539What is the effect of the latest holiday sales campaign on meat and poultry?\\n/L50539How do the July 4th holiday sales by product categories compare to last year?\\nEach of these three queries requires selections and summations from the fact table\\nrows. For these types of summations, you need detailed data based on one or more dimen-sions, but only summary totals based on the other dimensions. For example, for the lastquery, you need detailed daily data based on the time dimension, but summary totals byproduct categories. In any case, if you had summary totals or precalculated aggregatesreadily available, the queries would run faster. With properly aggregated summaries, theperformance of each of these queries can be dramatically improved. \\nNeed for Aggregates\\nPlease refer to Figure 11-12 showing the STAR schema for a grocery chain. In those 300\\nstores, assume there are 500 products per brand. Of the 40,000 products, assume thatthere is at least one sale per product per store per week. Let us estimate the number of facttable rows to be retrieved and summarized for the following types of queries:\\nQuery involves 1 product, 1 store, 1 week—retrieve/summarize only 1 fact table row\\nQuery involves 1 product, all stores, 1 week—retrieve/summarize 300 fact table rows242 DIMENSIONAL MODELING: ADVANCED TOPICS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='44e37af0-3d44-4b95-b7e9-3deb6569195d', embedding=None, metadata={'page_label': '263', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Query involves 1 brand, 1 store, 1 week—retrieve/summarize 500 fact table rows\\nQuery involves 1 brand, all stores, 1 year—retrieve/summarize 7,800,000 fact table\\nrows\\nSuppose you had precalculated and created an aggregate fact table in which each row\\nsummarized the totals for a brand, per store, per week. Then the third query must retrieveonly one row from this aggregate fact table. Similarly, the last query must retrieve only15,600 rows from this aggregate fact table, much less than the 7 million rows.\\nFurther, if you precalculate and create another aggregate fact table in which each row\\nsummarized the totals for a brand, per store, per year, the last query must retrieve only 300rows. \\nAggregates have fewer rows than the base tables. Therefore, when most of the queries\\nare run against the aggregate fact tables instead of the base fact table, you notice a tremen-dous boost to performance in the data warehouse. Formation of aggregate fact tables iscertainly a very effective method to improve query performance.\\nAggregating Fact Tables\\nAs we have seen, aggregate fact tables are merely summaries of the most granular data at\\nhigher levels along the dimension hierarchies. Please refer to Figure 11-13 illustrating thehierarchies along three dimensions. Examine the hierarchies in the three dimensions. Thehierarchy levels in the time dimension move up from day at the lowest level to year at thehighest level. City is at the lowest level in the store dimension and product at the lowestlevel in the product dimension. AGGREGATE FACT TABLES 243\\nProduct Key \\nTime Key      \\nStore Key \\nUnit Sales      \\nSales DollarsSALES FACTSSTOREPRODUCT\\nTIMEStore Key      \\nStore Name \\nTerritory      \\nRegion\\nTime Key       \\nDate              \\nMonth        \\nQuarter           \\nYearProduct Key  \\nProduct    \\nCategory  \\nDepartment\\nAll Stores\\nAll ProductsHIERARCHY  \\nLEVELSHIERARCHY  \\nLEVELS\\nHIERARCHY  \\nLEVELSHIERARCHY  \\nLEVELS\\nLowest Level\\nHighest Level\\nFigure 11-13 Dimension hierarchies.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='775c4134-964a-4965-b12e-073dad328432', embedding=None, metadata={'page_label': '264', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='In the base fact table, the rows reflect the numbers at the lowest levels of the dimension\\nhierarchies. For example, each row in the base fact table shows the sales units and salesdollars relating to one date, one store, and one product. By moving up one or more notch-es along the hierarchy in each dimension, you can create a variety of aggregate fact tables.Let us explore the possibilities. \\nMulti-Way Aggregate Fact Tables. Please see Figure 11-14 illustrating the differ-\\nent ways aggregate fact tables may be formed and also read the following descriptions ofpossible aggregates.\\nOne-Way Aggregates. When you rise to higher levels in the hierarchy of one dimen-\\nsion and keep the level at the lowest in the other dimensions, you create one-way aggre-gate tables. Please review the following examples:\\n/L50539Product category by store by date\\n/L50539Product department by store by date\\n/L50539All products by store by date\\n/L50539Territory by product by date\\n/L50539Region by product by date\\n/L50539All stores by product by date\\n/L50539Month by store by product\\n/L50539Quarter by store by product\\n/L50539Y ear by store by product244 DIMENSIONAL MODELING: ADVANCED TOPICS\\nStore\\nRegionTerritory\\nAll \\nStoresSTORE\\nDate\\nQuarte\\nrMonth\\nYearProduct\\nDepartmentCategory\\nAll ProductsPRODUCT TIME EXAMPLES\\nOne-way \\nAggregate\\nStore\\nRegionTerritory\\nAll \\nStoresQuarterMonth\\nYearProduct\\nDepartmentCategory\\nAll ProductsStore\\nRegionTerritory\\nAll \\nStoresQuarterMonth\\nYearProduct\\nDepartmentCategory\\nAll ProductsDate\\nDate\\nThree-way \\nAggregateTwo-way \\nAggregate\\nFigure 11-14 Forming aggregate fact tables.Quarter\\nQuarter\\nQuarter', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1f259a65-c398-4497-bf48-690a61f0bdba', embedding=None, metadata={'page_label': '265', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Two-Way Aggregates. When you rise to higher levels in the hierarchies of two dimen-\\nsions and keep the level at the lowest in the other dimension, you create two-way aggre-gate tables. Please review the following examples:\\n/L50539Product category by territory by date\\n/L50539Product category by region by date\\n/L50539Product category by all stores by date\\n/L50539Product category by month by store\\n/L50539Product category by quarter by store\\n/L50539Product category by year by store\\n/L50539Product department by territory by date\\n/L50539Product department by region by date\\n/L50539Product department by all stores by date\\n/L50539Product department by month by store\\n/L50539Product department by quarter by store\\n/L50539Product department by year by store\\n/L50539All products by territory by date\\n/L50539All products by region by date\\n/L50539All products by all stores by date\\n/L50539All products by month by store\\n/L50539All products by quarter by store\\n/L50539All products by year by store\\n/L50539District by month by product\\n/L50539District by quarter by product\\n/L50539District by year by product\\n/L50539Territory by month by product\\n/L50539Territory by quarter by product\\n/L50539Territory by year by product\\n/L50539Region by month by product\\n/L50539Region by quarter by product\\n/L50539Region by year by product\\n/L50539All stores by month by product\\n/L50539All stores by quarter by product\\n/L50539All stores by year by product\\nThree-Way Aggregates. When you rise to higher levels in the hierarchies of all the\\nthree dimensions, you create three-way aggregate tables. Please review the following ex-amples:\\n/L50539Product category by territory by month\\n/L50539Product department by territory by month\\n/L50539All products by territory by monthAGGREGATE FACT TABLES 245', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='465299a6-c377-4cc8-9a8f-143e3ed07e6f', embedding=None, metadata={'page_label': '266', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Product category by region by month\\n/L50539Product department by region by month\\n/L50539All products by region by month\\n/L50539Product category by all stores by month\\n/L50539Product department by all stores by month\\n/L50539Product category by territory by quarter\\n/L50539Product department by territory by quarter\\n/L50539All products by territory by quarter\\n/L50539Product category by region by quarter\\n/L50539Product department by region by quarter\\n/L50539All products by region by quarter\\n/L50539Product category by all stores by quarter\\n/L50539Product department by all stores by quarter\\n/L50539Product category by territory by year\\n/L50539Product department by territory by year\\n/L50539All products by territory by year\\n/L50539Product category by region by year\\n/L50539Product department by region by year\\n/L50539All products by region by year\\n/L50539Product category by all stores by year\\n/L50539Product department by all stores by year\\n/L50539All products by all stores by year\\nEach of these aggregate fact tables is derived from a single base fact table. The derived\\naggregate fact tables are joined to one or more derived dimension tables. See Figure 11-15showing a derived aggregate fact table connected to a derived dimension table.\\nEffect of Sparsity on Aggregation. Consider the case of the grocery chain with 300\\nstores, 40,000 products in each store, but only 4000 selling in each store in a day. As dis-cussed earlier, assuming that you keep records for 5 years or 1825 days, the maximumnumber of base fact table rows is calculated as follows:\\nProduct = 40,000\\nStore = 300Time = 1825Maximum number of base fact table rows = 22 billion\\nBecause only 4,000 products sell in each store in a day, not all of these 22 billion rows\\nare occupied. Because of this sparsity, only 10% of the rows are occupied. Therefore, thereal estimate of the number of base table rows is 2 billion.\\nNow let us see what happens when you form aggregates. Scrutinize a one-way aggre-\\ngate: brand totals by store by day. Calculate the maximum number of rows in this one-wayaggregate.246 DIMENSIONAL MODELING: ADVANCED TOPICS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aa6879af-eed0-48f7-a011-034906b952b2', embedding=None, metadata={'page_label': '267', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Brand = 80\\nStore = 300Time = 1825Maximum number of aggregate table rows = 43,800,000\\nWhile creating the one-way aggregate, you will notice that the sparsity for this aggre-\\ngate is not 10% as in the case of the base table. This is because when you aggregate bybrand, more of the brand codes will participate in combinations with store and time codes.The sparsity of the one-way aggregate would be about 50%, resulting in a real estimate of21,900,000. If the sparsity had remained as the 10% applicable to the base table, the realestimate of the number of rows in the aggregate table would be much less.\\nWhen you go for higher levels of aggregates, the sparsity percentage moves up and\\neven reaches 100%. Because of the failure of sparsity to stay lower, you are faced with thequestion whether aggregates do improve performance that much. Do they reduce thenumber of rows that dramatically? \\nExperienced data warehousing practitioners have a suggestion. When you form aggre-\\ngates, make sure that each aggregate table row summarizes at least 10 rows in the lowerlevel table. If you increase this to 20 rows or more, it would be really remarkable.\\nAggregation Options\\nGoing back to our discussion of one-way, two-way, and three-way aggregates for a basic\\nSTAR schema with just three dimensions, you could count more than 50 different ways youAGGREGATE FACT TABLES 247\\nProduct Key \\nTime Key      \\nStore Key \\nUnit Sales      \\nSales DollarsSALES FACTSSTOREPRODUCT\\nTIMEStore Key      \\nStore Name \\nTerritory      \\nRegion\\nTime Key       \\nDate              \\nMonth        \\nQuarter           \\nYearProduct Key  \\nProduct  \\nCategory  \\nDepartment\\nCATEGORY\\nCategory Key \\nCategory  \\nDepartment\\nCategory Key \\nTime Key      \\nStore Key \\nUnit Sales      \\nSales DollarsSALES FACTSBASE TABLE\\nONE-WAY AGGREGATEDIMENSION \\nDERIVED FROM \\nPRODUCT\\nFigure 11-15 Aggregate fact table and derived dimension table. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d0cc2b19-b7b7-4ea3-9f43-691fddca6bc1', embedding=None, metadata={'page_label': '268', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='may create aggregates. In the real world, the number of dimensions is not just three, but\\nmany more. Therefore, the number of possible aggregate tables escalates into the hundreds. \\nFurther, from the reference to the failure of sparsity in aggregate tables, you know that\\nthe aggregation process does not reduce the number of rows proportionally. In otherwords, if the sparsity of the base fact table is 10%, the sparsity of the higher-level aggre-gate tables does not remain at 10%. The sparsity percentage increases more and more asyour aggregate tables climb higher and higher in levels of summarization. \\nIs aggregation that much effective after all? What are some of the options? How do you\\ndecide what to aggregate? First, set a few goals for aggregation for your data warehouseenvironment.\\nGoals for Aggregation Strategy. Apart from the general overall goal of improving\\ndata warehouse performance, here are a few specific, practical goals:\\n/L50539Do not get bogged down with too many aggregates. Remember, you have to create\\nadditional derived dimensions as well to support the aggregates.\\n/L50539Try to cater to a wide range of user groups. In any case, provide for your power\\nusers.\\n/L50539Go for aggregates that do not unduly increase the overall usage of storage. Look\\ncarefully into larger aggregates with low sparsity percentages.\\n/L50539Keep the aggregates hidden from the end-users. That is, the aggregates must be\\ntransparent to the end-user query. The query tool must be the one to be aware of theaggregates to direct the queries for proper access.\\n/L50539Attempt to keep the impact on the data staging process as less intensive as possible. \\nPractical Suggestions. Before doing any calculations to determine the types of ag-\\ngregates needed for your data warehouse environment, spend a good deal of time on de-termining the nature of the common queries. How do your users normally report results?What are the reporting levels? By stores? By months? By product categories? Go throughthe dimensions, one by one, and review the levels of the hierarchies. Check if there aremultiple hierarchies within the same dimension. If so, find out which of these multiple hi-erarchies are more important. In each dimension, ascertain which attributes are used forgrouping the fact table metrics. The next step is to determine which of these attributes areused in combinations and what the most common combinations are. \\nOnce you determine the attributes and their possible combinations, look at the number\\nof values for each attribute. For example, in a hotel chain schema, assume that hotel is atthe lowest level and city is at the next higher level in the hotel dimension. Let us say thereare 25,000 values for hotel and 15,000 values for city. Clearly, there is no big advantage ofaggregating by cities. On the other hand, if city has only 500 values, then city is a level atwhich you may consider aggregation. Examine each attribute in the hierarchies within adimension. Check the values for each of the attributes. Compare the values of attributes atdifferent levels of the same hierarchy and decide which ones are strong candidates to par-ticipate in aggregation. \\nDevelop a list of attributes that are useful candidates for aggregation, then work out the\\ncombinations of these attributes to come up with your first set of multiway aggregate facttables. Determine the derived dimension tables you need to support these aggregate facttables. Go ahead and implement these aggregate fact tables as the initial set.248 DIMENSIONAL MODELING: ADVANCED TOPICS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6a11f86f-9462-4047-af10-47525aa2096b', embedding=None, metadata={'page_label': '269', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Bear in mind that aggregation is a performance tuning mechanism. Improved query per-\\nformance drives the need to summarize, so do not be too concerned if your first set of ag-gregate tables do not perform perfectly. Y our aggregates are meant to be monitored and re-vised as necessary. The nature of the bulk of the query requests is likely to change. As yourusers become more adept at using the data warehouse, they will devise new ways of group-ing and analyzing data. So what is the practical advice? Do your preparatory work, startwith a reasonable set of aggregate tables, and continue to make adjustments as necessary.\\nFAMILIES OF STARS\\nWhen you look at a single STAR schema with its fact table and the surrounding dimen-\\nsion tables, you know that is not the extent of a data warehouse. Almost all data warehous-es contain multiple STAR schema structures. Each STAR serves a specific purpose totrack the measures stored in the fact table. When you have a collection of related STARschemas, you may call the collection a family of STARS. Families of STARS are formedfor various reasons. Y ou may form a family by just adding aggregate fact tables and thederived dimension tables to support the aggregates. Sometimes, you may create a corefact table containing facts interesting to most users and customized fact tables for specificuser groups. Many factors lead to the existence of families of STARS. First, look at theexample provided in Figure 11-16.\\nThe fact tables of the STARS in a family share dimension tables. Usually, the time di-\\nmension is shared by most of the fact tables in the group. In the above example, all theFAMILIES OF STARS 249\\nFigure 11-16 Family of STARS.FACT \\nTABLEDIMENSION \\nTABLE\\nDIMENSION \\nTABLEDIMENSION \\nTABLE\\nDIMENSION \\nTABLE\\nDIMENSION \\nTABLEDIMENSION \\nTABLEFACT \\nTABLEFACT \\nTABLEDIMENSION \\nTABLE\\nDIMENSION \\nTABLE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7ed76356-272d-44bf-8ec6-c228a48a14d8', embedding=None, metadata={'page_label': '270', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='three fact tables are likely to share the time dimension. Going the other way, dimension ta-\\nbles from multiple STARS may share the fact table of one STAR. \\nIf you are in a business like banking or telephone services, it makes sense to capture in-\\ndividual transactions as well as snapshots at specific intervals. Y ou may then use familiesof STARS consisting of transaction and snapshot schemas. If you are in a manufacturingcompany or a similar production-type enterprise, your company needs to monitor the met-rics along the value chain. Some other institutions are like a medical center, where value isadded not in a chain but at different stations within the enterprise. For these enterprises,the family of STARS supports the value chain or the value circle. We will get into detailsin the next few sections. \\nSnapshot and Transaction Tables\\nLet us review some basic requirements of a telephone company. A number of individual\\ntransactions make up a telephone customer’ s account. Many of the transactions occur dur-ing the hours of 6 a.m. to 10 p.m. of the customer’ s day. More transactions happen duringthe holidays and weekends for residential customers. Institutional customers use thephones on weekdays rather than over the weekends. A telephone accumulates a very largecollection of rich transaction data that can be used for many types of valuable analysis.The telephone company needs a schema capturing transaction data that supports strategicdecision making for expansions, new service improvements, and so on. This transactionschema answers questions such as how does the revenue of peak hours over the weekendsand holidays compare with peak hours over weekdays. \\nIn addition, the telephone company needs to answer questions from the customers as to\\naccount balances. The customer service departments are constantly bombarded with ques-tions on the status of individual customer accounts. At periodic intervals, the accountingdepartment may be interested in the amounts expected to be received by the middle ofnext month. What are the outstanding balances for which bills will be sent this month-end? For these purposes, the telephone company needs a schema to capture snapshots atperiodic intervals. Please see Figure 11-17 showing the snapshot and transaction fact ta-bles for a telephone company. Make a note of the attributes in the two fact tables. Onetable tracks the individual phone transactions. The other table holds snapshots of individ-ual accounts at specific intervals. Also, notice how dimension tables are shared betweenthe two fact tables.\\nSnapshot and transaction tables are also common for banks. For example, an ATM\\ntransaction table stores individual ATM transactions. This fact table keeps track of indi-vidual transaction amounts for the customer accounts. The snapshot table holds the bal-ance for each account at the end of each day. The two tables serve two distinct functions.From the transaction table, you can perform various types of analysis of the ATM transac-tions. The snapshot table provides total amounts held at periodic intervals showing theshifting and movement of balances.\\nFinancial data warehouses also require snapshot and transaction tables because of the\\nnature of the analysis in these cases. The first set of questions for these warehouses relatesto the transactions affecting given accounts over a certain period of time. The other set ofquestions centers around balances in individual accounts at specific intervals or totals ofgroups of accounts at the end of specific periods. The transaction table answers the ques-tions of the first set; the snapshot table handles the questions of the second set.250 DIMENSIONAL MODELING: ADVANCED TOPICS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='824820c0-4c5e-4d5c-9a8e-4fd5f59ef59e', embedding=None, metadata={'page_label': '271', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Core and Custom Tables\\nConsider two types of businesses that are apparently dissimilar. First take the case of a\\nbank. A bank offers a large variety of services all related to finance in one form or anoth-er. Most of the services are different from one another. The checking account service andthe savings account service are similar in most ways. But the savings account service doesnot resemble the credit card service in any way. How do you track these dissimilar ser-vices? \\nNext, consider a manufacturing company producing a number of heterogeneous prod-\\nucts. Although a few factors may be common to the various products, by and large the fac-tors differ. What must you do to get information about heterogeneous products?\\nA different type of the family of STARS satisfies the requirements of these companies.\\nIn this type of family, all products and services connect to a core fact table and each prod-uct or service relates to individual custom tables. In Figure 11-18, you will see the coreand custom tables for a bank. Note how the core fact table holds the metrics that are com-mon to all types of accounts. Each custom fact table contains the metrics specific to thatline of service. Also note the shared dimension and notice how the tables form a family ofSTARS.\\nSupporting Enterprise Value Chain or Value Circle\\nIn a manufacturing business, a product travels through various steps, starting off as raw\\nmaterials and ending as finished goods in the warehouse inventory. Usually, the steps in-clude addition of ingredients, assembly of materials, process control, packaging, and ship-ping to the warehouse. From finished goods inventory, a product moves into shipment todistributor, distributor inventory, distributor shipment, retail inventory, and retail sales. AtFAMILIES OF STARS 251\\nTime Key      \\nAccount Key  \\nTransaction Key \\nDistrict Key \\nTrans Reference\\nAccount Number      \\nAmountTELEPHONE \\nTRANSACTION \\nFACT TABLE\\nSTATUS\\nStatus Key\\n…………...ACCOUNT\\nAccount Key\\n…………...DISTRICT\\nDistrict Key\\n…………...\\nTIME\\nTime Key\\n…………...Time Key      \\nAccount Key  \\nStatus Key    \\nTransaction Count \\nEnding BalanceTELEPHONE \\nSNAPSHOT \\nFACT TABLE\\nTRANSACTION\\nTransaction Key\\n…………...\\nFigure 11-17 Snapshot and transaction tables.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d0664739-6236-4bab-9426-dfaccf490a08', embedding=None, metadata={'page_label': '272', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='each step, value is added to the product. Several operational systems support the flow\\nthrough these steps. The whole flow forms the supply chain or the value chain. Similarly,in an insurance company, the value chain may include a number of steps from sales of in-surance through issuance of policy and then finally claims processing. In this case, thevalue chain relates to the service. \\nIf you are in one of these businesses, you need to track important metrics at different\\nsteps along the value chain. Y ou create STAR schemas for the significant steps and thecomplete set of related schemas forms a family of STARS. Y ou define a fact table and aset of corresponding dimensions for each important step in the chain. If your company hasmultiple value chains, then you have to support each chain with a separate family ofSTARS.\\nA supply chain or a value chain runs in a linear fashion beginning with a certain step\\nand ending at another step with many steps in between. Again, at each step, value isadded. In some other kinds of businesses where value gets added to services, similar lin-ear movements do not exist. For example, consider a health care institution where valuegets added to patient service from different units almost as if they form a circle around theservice. We perceive a value circle in such organizations. The value circle of a large healthmaintenance organization may include hospitals, clinics, doctors’ offices, pharmacies,laboratories, government agencies, and insurance companies. Each of these units eitherprovide patient treatments or measure patient treatments. Patient treatment by each unitmay be measured in different metrics. But most of the units would analyze the metrics us-252 DIMENSIONAL MODELING: ADVANCED TOPICS\\nTime Key      \\nAccount Key  \\nBranch Key \\nHousehold Key \\nBalance \\nFees Charged \\nTransactionsBRANCH\\nBranch Key\\n…………...ACCOUNT\\nAccount Key\\n…………...TIME\\nTime Key\\n…………...Account Key  \\nDeposits    \\nWithdrawals \\nInterest Earned \\nBalance       \\nService Charges\\nHOUSEHOLD\\nHousehold Key\\n…………...Account Key  \\nATM Trans. \\nDrive-up Trans. \\nWalk-in Trans. \\nDeposits      \\nChecks Paid   \\nOverdraftBANK CORE FACT TABLESAVINGS CUSTOM \\nFACT TABLE\\nCHECKING CUSTOM \\nFACT TABLE\\nFigure 11-18 Core and custom tables.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f7d2a443-de74-47e1-b61c-0895913bdfd7', embedding=None, metadata={'page_label': '273', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ing the same set of conformed dimensions such as time, patient, health care provider,\\ntreatment, diagnosis, and payer. For a value circle, the family of STARS comprises multi-ple fact tables and a set of conformed dimensions. \\nConforming Dimensions\\nWhile exploring families of STARS, you will have noticed that dimensions are shared\\namong fact tables. Dimensions form common links between STARS. For dimensions tobe conformed, you have to deliberately make sure that common dimensions may be usedbetween two or more STARS. If the product dimension is shared between two fact tablesof sales and inventory, then the attributes of the product dimension must have the samemeaning in relation to each of the two fact tables. Figure 11-19 shows a set of conformeddimensions. \\nThe order and shipment fact tables share the conformed dimensions of product, date,\\ncustomer, and salesperson. A conformed dimension is a comprehensive combination ofattributes from the source systems after resolving all discrepancies and conflicts. For ex-ample, a conformed product dimension must truly reflect the master product list of the en-terprise and must include all possible hierarchies. Each attribute must be of the correctdata type and must have proper lengths and constraints. \\nConforming dimensions is a basic requirement in a data warehouse. Pay special atten-\\ntion and take the necessary steps to conform all your dimensions. This is a major respon-sibility of the project team. Conformed dimensions allow rollups across data marts. Userinterfaces will be consistent irrespective of the type of query. Result sets of queries will beFAMILIES OF STARS 253\\nProduct Key  \\nTime Key     \\nCustomer Key  \\nSalesperson Key  \\nOrder Dollars   \\nCost Dollars      \\nMargin Dollars  \\nSale UnitsORDERCUSTOMER\\nSALESPERSONPRODUCT\\nDATECustomer Key \\n……………….\\nSalesperson Key \\n………………...\\nDate Key         \\n………………...Product Key  \\n……………...\\nProduct Key  \\nTime Key     \\nCustomer Key  \\nSalesperson Key \\nChannel Key  \\nShip-to Key  \\nShip-from Key \\nInvoice Number  \\nOrder Number  \\nShip Date   \\nArrival DateSHIPMENTCHANNEL\\nChannel Key \\n………………...\\nSHIP-TO\\nShip-to Key \\n………………...\\nSHIP-FROM\\nShip-from Key \\n………………...CONFORMED \\nDIMENSIONS\\nFigure 11-19 Conformed dimensions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0d87dd8a-38f3-4355-8f09-73b8117f8801', embedding=None, metadata={'page_label': '274', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='consistent across data marts. Of course, a single conformed dimension can be used\\nagainst multiple fact tables. \\nStandardizing Facts\\nIn addition to the task of conforming dimensions is the requirement to standardize facts.\\nWe have seen that fact tables work across STAR schemas. Review the following issues re-lating to the standardization of fact table attributes:\\n/L50539Ensure same definitions and terminology across data marts\\n/L50539Resolve homonyms and synonyms\\n/L50539Types of facts to be standardized include revenue, price, cost, and profit margin\\n/L50539Guarantee that the same algorithms are used for any derived units in each fact\\ntable\\n/L50539Make sure each fact uses the right unit of measurement\\nSummary of Family of STARS\\nLet us end our discussion of the family of STARS with a comprehensive diagram showing\\na set of standardized fact tables and conformed dimension tables. Study Figure 11-20 care-254 DIMENSIONAL MODELING: ADVANCED TOPICS\\nDate Key      \\nAccount Key  \\nBranch Key \\nBalanceBRANCH\\nBranch Key   \\nBranch Name   \\nState          \\nRegionACCOUNT\\nAccount Key\\n…………...DATE\\nDate Key      \\nDate              \\nMonth             \\nYear\\nAccount Key  \\nATM Trans. \\nOther Trans.STATE\\nState Key       \\nState            \\nRegionDate Key      \\nAccount Kye  \\nState Key    \\nBalanceMONTH\\nMonth Key       \\nMonth             \\nYearMonth Key      \\nAccount Key      \\nState Key  \\nBalance2-WAY AGGREGATE\\n1-WAY AGGREGATE\\nBANK CORE TABLE\\nCHECKING CUSTOM TABLE\\nFigure 11-20 A comprehensive family of STARS.Branch Name', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='302bbe17-b2f6-4c9a-ae67-62af7912fc5e', embedding=None, metadata={'page_label': '275', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='fully. Note the aggregate fact tables and the corresponding derived dimension tables. What\\ntypes of aggregates are these? One-way or two-way? Which are the base fact tables? Noticethe shared dimensions. Are these conformed dimensions? See how the various fact tablesand the dimension tables are related.\\nCHAPTER SUMMARY\\n/L50539Slowly changing dimensions may be classified into three different types based on\\nthe nature of the changes. Type 1 relates to corrections, Type 2 to preservation ofhistory, and Type 3 to soft revisions. Applying each type of revision to the datawarehouse is different.\\n/L50539Large dimension tables such as customer or product need special considerations for\\napplying optimizing techniques.\\n/L50539“Snowflaking” or creating a snowflake schema is a method of normalizing the\\nSTAR schema. Although some conditions justify the snowflake schema, it is gener-ally not recommended. \\n/L50539Miscellaneous flags and textual data are thrown together in one table called a junk\\ndimension table.\\n/L50539Aggregate or summary tables improve performance. Formulate a strategy for build-\\ning aggregate tables. \\n/L50539A set of related STAR schemas make up a family of STARS. Examples are snapshot\\nand transaction tables, core and custom tables, and tables supporting a value chainor a value circle. A family of STARS relies on conformed dimension tables andstandardized fact tables.\\nREVIEW QUESTIONS\\n1. Describe slowly changing dimensions. What are the three types? Explain each\\ntype very briefly.\\n2. Compare and contrast Type 2 and Type 3 slowly changing dimensions.3. Can you treat rapidly changing dimensions in the same way as Type 2 slowly\\nchanging dimensions? Discuss.\\n4. What are junk dimensions? Are they necessary in a data warehouse?5. How does a snowflake schema differ from a STAR schema? Name two advantages\\nand two disadvantages of the snowflake schema.\\n6. Differentiate between slowly and rapidly changing dimensions.7. What are aggregate fact tables? Why are they needed? Give an example.8. Describe with examples snapshot and transaction fact tables. How are they relat-\\ned?\\n9. Give an example of a value circle. Explain how a family of STARS can support a\\nvalue circle.\\n10. What is meant by conforming the dimensions? Why is this important in a data\\nwarehouse?REVIEW QUESTIONS 255', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='67bd5c72-8e81-4f38-9cc5-8f1f013389ff', embedding=None, metadata={'page_label': '276', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='EXERCISES\\n1. Indicate if true or false:\\nA. Type 1 changes for slowly changing dimensions relate to correction of errors.\\nB. To apply Type 3 changes of slowly changing dimensions, overwrite the attribute\\nvalue in the dimension table row with the new value.\\nC. Large dimensions usually have multiple hierarchies.D. The STAR schema is a normalized version of the snowflake schema.E. Aggregates are precalculated summaries.F . The percentage of sparsity of the base table tends to be higher than that of aggre-\\ngate tables.\\nG. The fact tables of the STARS in a family share dimension tables.H. Core and custom fact tables are useful for companies with several lines of ser-\\nvice.\\nI. Conforming dimensions is not absolutely necessary in a data warehouse.J. A value circle usually needs a family of STARS to support the business.\\n2. Assume you are in the insurance business. Find two examples of Type 2 slowly\\nchanging dimensions in that business. As an analyst on the project, write the speci-fications for applying the Type 2 changes to the data warehouse with regard to thetwo examples. \\n3. Y ou are the data design specialist on the data warehouse project team for a retail\\ncompany. Design a STAR schema to track the sales units and sales dollars withthree dimension tables. Explain how you will decide to select and build four two-way aggregates. \\n4. As the data designer for an international bank, consider the possible types of snap-\\nshot and transaction tables. Complete the design with one set of snapshot and trans-action tables.\\n5. For a manufacturing company, design a family of three STARS to support the value\\nchain.256 DIMENSIONAL MODELING: ADVANCED TOPICS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cb4c3a23-48d2-4243-8a26-e9d7d73079f5', embedding=None, metadata={'page_label': '277', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 12\\nDATA EXTRACTION, TRANSFORMATION,\\nAND LOADING \\nCHAPTER OBJECTIVES\\n/L50539Survey broadly all the various aspects of the data extraction, transformation, and\\nloading (ETL) functions\\n/L50539Examine the data extraction function, its challenges, its techniques, and learn how\\nto evaluate and apply the techniques \\n/L50539Discuss the wide range of tasks and types of the data transformation function\\n/L50539Understand the meaning of data integration and consolidation\\n/L50539Perceive the importance of the data load function and probe the major methods for\\napplying data to the warehouse\\n/L50539Gain a true insight into why ETL is crucial, time-consuming, and arduous\\nY ou may be convinced that the data in your organization’ s operational systems is total-\\nly inadequate for providing information for strategic decision making. As informationtechnology professionals, we are fully aware of the futile attempts in the past two decadesto provide strategic information from operational systems. These attempts did not work.Data warehousing can fulfill that pressing need for strategic information.\\nMostly, the information contained in a warehouse flows from the same operational sys-\\ntems that could not be directly used to provide strategic information. What constitutes thedifference between the data in the source operational systems and the information in thedata warehouse? It is the set of functions that fall under the broad group of data extrac-tion, transformation, and loading (ETL). \\nETL functions reshape the relevant data from the source systems into useful informa-\\ntion to be stored in the data warehouse. Without these functions, there would be no strate-\\n257Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='592bcb38-5d5e-4821-93ff-d19eeb05e654', embedding=None, metadata={'page_label': '278', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='gic information in the data warehouse. If the source data is not extracted correctly,\\ncleansed, and integrated in the proper formats, query processing, the backbone of the datawarehouse, could not happen.\\nIn Chapter 2, when we discussed the building blocks of the data warehouse, we\\nbriefly looked at ETL functions as part of the data staging area. In Chapter 6 we revis-ited ETL functions and examined how the business requirements drive these functionsas well. Further, in Chapter 8, we explored the hardware and software infrastructure op-tions to support the data movement functions. Why, then, is additional review of ETLnecessary?\\nETL functions form the prerequisites for the data warehouse information content. ETL\\nfunctions rightly deserve more consideration and discussion. In this chapter, we will delvedeeper into issues relating to ETL functions. We will review many significant activitieswithin ETL. In the next chapter, we need to continue the discussion by studying anotherimportant function that falls within the overall purview of ETL—data quality. Now, let usbegin with a general overview of ETL.\\nETL OVERVIEW\\nIf you recall our discussion of the functions and services of the technical architecture of\\nthe data warehouse, you will see that we divided the environment into three functionalareas. These areas are data acquisition, data storage, and information delivery. Data ex-traction, transformation, and loading encompass the areas of data acquisition and datastorage. These are back-end processes that cover the extraction of data from the sourcesystems. Next, they include all the functions and procedures for changing the source datainto the exact formats and structures appropriate for storage in the data warehouse data-base. After the transformation of the data, these processes consist of all the functions forphysically moving the data into the data warehouse repository.\\nData extraction, of course, precedes all other functions. But what is the scope and ex-\\ntent of the data you will extract from the source systems? Do you not think that the usersof your data warehouse are interested in all of the operational data for some type of queryor analysis? So, why not extract all of operational data and dump it into the data ware-house? This seems to be a straightforward approach. Nevertheless, this approach is some-thing driven by the user requirements. Y our requirements definition should guide you as towhat data you need to extract and from which source systems. Avoid creating a datajunkhouse by dumping all the available data from the source systems and waiting to seewhat the users will do with it. Data extraction presupposes a selection process. Select theneeded data based on the user requirements. \\nThe extent and complexity of the back-end processes differ from one data warehouse\\nto another. If your enterprise is supported by a large number of operational systems run-ning on several computing platforms, the back-end processes in your case would be exten-sive and possibly complex as well. So, in your situation, data extraction becomes quitechallenging. The data transformation and data loading functions may also be equally diffi-cult. Moreover, if the quality of the source data is below standard, this condition furtheraggravates the back-end processes. In addition to these challenges, if only a few of theloading methods are feasible for your situation, then data loading could also be difficult.Let us get into specifics about the nature of the ETL functions. 258 DATA EXTRACTION, TRANSFORMATION, AND LOADING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a69f7819-ffa6-4eea-b0c2-2d5b2ab67bff', embedding=None, metadata={'page_label': '279', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Most Important and Most Challenging\\nEach of the ETL functions fulfills a significant purpose. When you want to convert data\\nfrom the source systems into information stored in the data warehouse, each of thesefunctions is essential. For changing data into information you first need to capture thedata. After you capture the data, you cannot simply dump that data into the data ware-house and call it strategic information. Y ou have to subject the extracted data to all mannerof transformations so that the data will be fit to be converted into information. Once youhave transformed the data, it is still not useful to the end-users until it is moved to the datawarehouse repository. Data loading is an essential function. Y ou must perform all threefunctions of ETL for successfully transforming data into information.\\nTake as an example an analysis your user wants to perform. The user wants to com-\\npare and analyze sales by store, by product, and by month. The sale figures are availablein the several sales applications in your company. Also, you have a product master file.Further, each sales transaction refers to a specific store. All these are pieces of data inthe source operational systems. For doing the analysis, you have to provide informationabout the sales in the data warehouse database. Y ou have to provide the sales units anddollars in a fact table, the products in a product dimension table, the stores in a store di-mension table, and months in a time dimension table. How do you do this? Extract thedata from each of the operational systems, reconcile the variations in data representa-tions among the source systems, and transform all the sales of all the products. Thenload the sales into the fact and dimension tables. Now, after completion of these threefunctions, the extracted data is sitting in the data warehouse, transformed into informa-tion, ready for analysis. Notice that it is important for each function to be performed,and performed in sequence.\\nETL functions are challenging primarily because of the nature of the source systems.\\nMost of the challenges in ETL arise from the disparities among the source operationalsystems. Please review the following list of reasons for the types of difficulties in ETLfunctions. Consider each carefully and relate it to your environment so that you may findproper resolutions.\\n/L50539Source systems are very diverse and disparate.\\n/L50539There is usually a need to deal with source systems on multiple platforms and dif-\\nferent operating systems. \\n/L50539Many source systems are older legacy applications running on obsolete database\\ntechnologies.\\n/L50539Generally, historical data on changes in values are not preserved in source opera-\\ntional systems. Historical information is critical in a data warehouse.\\n/L50539Quality of data is dubious in many old source systems that have evolved over time. \\n/L50539Source system structures keep changing over time because of new business condi-\\ntions. ETL functions must also be modified accordingly.\\n/L50539Gross lack of consistency among source systems is commonly prevalent. Same data\\nis likely to be represented differently in the various source systems. For example,data on salary may be represented as monthly salary, weekly salary, and bimonthlysalary in different source payroll systems.\\n/L50539Even when inconsistent data is detected among disparate source systems, lack of a\\nmeans for resolving mismatches escalates the problem of inconsistency.ETL OVERVIEW 259', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e13c3bbd-e30a-44ec-b0c0-8adf6a10ecaa', embedding=None, metadata={'page_label': '280', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Most source systems do not represent data in types or formats that are meaningful\\nto the users. Many representations are cryptic and ambiguous. \\nTime-Consuming and Arduous\\nWhen the project team designs the ETL functions, tests the various processes, and deploys\\nthem, you will find that these consume a very high percentage of the total project effort. Itis not uncommon for a project team to spend as much as 50–70% of the project effort onETL functions. Y ou have already noted several factors that add to the complexity of theETL functions. \\nData extraction itself can be quite involved depending on the nature and complexity of\\nthe source systems. The metadata on the source systems must contain information onevery database and every data structure that are needed from the source systems. Y ou needvery detailed information, including database size and volatility of the data. Y ou have toknow the time window during each day when you can extract data without impacting theusage of the operational systems. Y ou also need to determine the mechanism for capturingthe changes to data in each of the relevant source systems. These are strenuous and time-consuming activities.\\nActivities within the data transformation function can run the gamut of transformation\\nmethods. Y ou have to reformat internal data structures, resequence data, apply variousforms of conversion techniques, supply default values wherever values are missing, andyou must design the whole set of aggregates that are needed for performance improve-ment. In many cases, you need to convert from EBCDIC to ASCII formats.\\nNow turn your attention to the data loading function. The sheer massive size of the ini-\\ntial loading can populate millions of rows in the data warehouse database. Creating andmanaging load images for such large numbers are not easy tasks. Even more difficult isthe task of testing and applying the load images to actually populate the physical files inthe data warehouse. Sometimes, it may take two or more weeks to complete the initialphysical loading. \\nWith regard to extracting and applying the ongoing incremental changes, there are sev-\\neral difficulties. Finding the proper extraction method for individual source datasets canbe arduous. Once you settle on the extraction method, finding a time window to apply thechanges to the data warehouse can be tricky if your data warehouse cannot suffer longdowntimes.\\nETL Requirements and Steps\\nBefore we highlight some key issues relating to ETL, let us review the functional steps.\\nFor initial bulk refresh as well as for the incremental data loads, the sequence is simply asnoted here: triggering for incremental changes, filtering for refreshes and incrementalloads, data extraction, transformation, integration, cleansing, and applying to the datawarehouse database. \\nWhat are the major steps in the ETL process? Please look at the list shown in Figure\\n12-1. Each of these major steps breaks down into a set of activities and tasks. Use this fig-ure as a guide to come up with a list of steps for the ETL process of your data warehouse.\\nThe following list enumerates the types of activities and tasks that compose the ETL\\nprocess. This list is by no means complete for every data warehouse, but it gives a goodinsight into what is involved to complete the ETL process.260 DATA EXTRACTION, TRANSFORMATION, AND LOADING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7dc6fa7d-56c3-46ef-9724-ca92e872b950', embedding=None, metadata={'page_label': '281', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Combine several source data structures into a single row in the target database of\\nthe data warehouse.\\n/L50539Split one source data structure into several structures to go into several rows of the\\ntarget database.\\n/L50539Read data from data dictionaries and catalogs of source systems.\\n/L50539Read data from a variety of file structures including flat files, indexed files\\n(VSAM), and legacy system databases (hierarchical/network).\\n/L50539Load details for populating atomic fact tables.\\n/L50539Aggregate for populating aggregate or summary fact tables.\\n/L50539Transform data from one format in the source platform to another format in the tar-\\nget platform.\\n/L50539Derive target values for input fields (example: age from date of birth).\\n/L50539Change cryptic values to values meaningful to the users (example: 1 and 2 to male\\nand female).\\nKey Factors\\nBefore we move on, let us point out a couple of key factors. The first relates to the com-\\nplexity of the data extraction and transformation functions. The second is about the dataloading function.\\nRemember that the primary reason for the complexity of the data extraction and trans-\\nformation functions is the tremendous diversity of the source systems. In a large enter-prise, we could have a bewildering combination of computing platforms, operating sys-tems, database management systems, network protocols, and source legacy systems. Y ouneed to pay special attention to the various sources and begin with a complete inventoryof the source systems. With this inventory as a starting point, work out all the details ofETL OVERVIEW 261\\nDetermine all the target data needed in the data warehouse.Determine all the data sources, both internal and external.Prepare data mapping for target data elements from sources.Establish comprehensive data extraction rules.Determine data transformation and cleansing rules.Plan for aggregate tables.Organize data staging area and test tools.Write procedures for all data loads.ETL for dimension tables.ETL for fact tables.\\nFigure 12-1 Major steps in the ETL process.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='678ebba3-16a5-4615-9367-63cafbc052c1', embedding=None, metadata={'page_label': '282', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='data extraction. The difficulties encountered in the data transformation function also re-\\nlate to the heterogeneity of the source systems. \\nNow, turning your attention to the data loading function, you have a couple of issues to\\nbe careful about. Usually, the mass refreshes, whether for initial load or for periodic re-freshes, cause difficulties, not so much because of complexities, but because these loadjobs run too long. Y ou will have to find the proper time to schedule these full refreshes.Incremental loads have some other types of difficulties. First, you have to determine thebest method to capture the ongoing changes from each source system. Next, you have toexecute the capture without impacting the source systems. After that, at the other end, youhave to schedule the incremental loads without impacting the usage of the data warehouseby the users.\\nPay special attention to these key issues while designing the ETL functions for your\\ndata warehouse. Now let us take each of the three ETL functions, one by one, and studythe details.\\nDATA EXTRACTION\\nAs an IT professional, you must have participated in data extractions and conversions\\nwhen implementing operational systems. When you went from a VSAM file-oriented or-der entry system to a new order processing system using relational database technology,you may have written data extraction programs to capture data from the VSAM files toget the data ready for populating the relational database.\\nTwo major factors differentiate the data extraction for a new operational system from\\nthe data extraction for a data warehouse. First, for a data warehouse, you have to extractdata from many disparate sources. Next, for a data warehouse, you have to extract data onthe changes for ongoing incremental loads as well as for a one-time initial full load. Foroperational systems, all you need is one-time extractions and data conversions.\\nThese two factors increase the complexity of data extraction for a data warehouse and,\\ntherefore, warrant the use of third-party data extraction tools in addition to in-house pro-grams or scripts. Third-party tools are generally more expensive than in-house programs,but they record their own metadata. On the other hand, in-house programs increase the costof maintenance and are hard to maintain as source systems change. If your company is in anindustry where frequent changes to business conditions are the norm, then you may want tominimize the use of in-house programs. Third-party tools usually provide built-in flexibili-ty. All you have to do is to change the input parameters for the third-part tool you are using. \\nEffective data extraction is a key to the success of your data warehouse. Therefore, you\\nneed to pay special attention to the issues and formulate a data extraction strategy for yourdata warehouse. Here is a list of data extraction issues:\\n/L50539Source Identification —identify source applications and source structures.\\n/L50539Method of extraction —for each data source, define whether the extraction process\\nis manual or tool-based.\\n/L50539Extraction frequency —for each data source, establish how frequently the data ex-\\ntraction must by done—daily, weekly, quarterly, and so on.\\n/L50539Time window —for each data source, denote the time window for the extraction\\nprocess.262 DATA EXTRACTION, TRANSFORMATION, AND LOADING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ac76db20-de2a-4938-bbb6-2d9c5a41762c', embedding=None, metadata={'page_label': '283', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Job sequencing —determine whether the beginning of one job in an extraction job\\nstream has to wait until the previous job has finished successfully. \\n/L50539Exception handling —determine how to handle input records that cannot be extract-\\ned.\\nSource Identification\\nLet us consider the first of the above issues, namely, source identification. We will deal\\nwith the rest of the issues later as we move through the remainder of this chapter. Sourceidentification, of course, encompasses the identification of all the proper data sources. Itdoes not stop with just the identification of the data sources. It goes beyond that to exam-ine and verify that the identified sources will provide the necessary value to the data ware-house. Let us walk through the source identification process in some detail.\\nAssume that a part of your database, maybe one of your data marts, is designed to pro-\\nvide strategic information on the fulfillment of orders. For this purpose, you need to storehistorical information about the fulfilled and pending orders. If you ship orders throughmultiple delivery channels, you need to capture data about these channels. If your usersare interested in analyzing the orders by the status of the orders as the orders go throughthe fulfillment process, then you need to extract data on the order statuses.\\nIn the fact table for order fulfillment, you need attributes about the total order amount,\\ndiscounts, commissions, expected delivery time, actual delivery time, and dates at differ-ent stages of the process. Y ou need dimension tables for product, order disposition, deliv-ery channel, and customer. First, you have to determine if you have source systems to pro-vide you with the data needed for this data mart. Then, from the source systems, you haveto establish the correct data source for each data element in the data mart. Further, youhave to go through a verification process to ensure that the identified sources are reallythe right ones. \\nFigure 12-2 describes a stepwise approach to source identification for order fulfill-\\nment. Source identification is not as simple a process as it may sound. It is a critical firstprocess in the data extraction function. Y ou need to go through the source identificationprocess for every piece of information you have to store in the data warehouse. As youmight have already figured out, source identification needs thoroughness, lots of time,and exhaustive analysis.\\nData Extraction Techniques\\nBefore examining the various data extraction techniques, you must clearly understand the\\nnature of the source data you are extracting or capturing. Also, you need to get an insightinto how the extracted data will be used. Source data is in a state of constant flux. \\nBusiness transactions keep changing the data in the source systems. In most cases, the\\nvalue of an attribute in a source system is the value of that attribute at the current time. Ifyou look at every data structure in the source operational systems, the day-to-day businesstransactions constantly change the values of the attributes in these structures. When a cus-tomer moves to another state, the data about that customer changes in the customer tablein the source system. When two additional package types are added to the way a productmay be sold, the product data changes in the source system. When a correction is appliedto the quantity ordered, the data about that order gets changed in the source system. DATA EXTRACTION 263', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2e02ff98-395f-4e87-a496-bd386a89e6b9', embedding=None, metadata={'page_label': '284', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Data in the source systems are said to be time-dependent or temporal. This is because\\nsource data changes with time. The value of a single variable varies over time. Again, takethe example of the change of address of a customer for a move from New Y ork state toCalifornia. In the operational system, what is important is that the current address of thecustomer has CA as the state code. The actual change transaction itself, stating that theprevious state code was NY and the revised state code is CA, need not be preserved. Butthink about how this change affects the information in the data warehouse. If the statecode is used for analyzing some measurements such as sales, the sales to the customer pri-or to the change must be counted in New Y ork state and those after the move must becounted in California. In other words, the history cannot be ignored in the data ware-house. This brings us to the question: how do you capture the history from the source sys-tems? The answer depends on how exactly data is stored in the source systems. So let usexamine and understand how data is stored in the source operational systems. \\nData in Operational Systems. These source systems generally store data in two\\nways. Operational data in the source system may be thought of as falling into two broadcategories. The type of data extraction technique you have to use depends on the nature ofeach of these two categories.\\nCurrent Value. Most of the attributes in the source systems fall into this category. Here\\nthe stored value of an attribute represents the value of the attribute at this moment of time.The values are transient or transitory. As business transactions happen, the values change.There is no way to predict how long the present value will stay or when it will get changed264 DATA EXTRACTION, TRANSFORMATION, AND LOADING\\nPRODUCT\\nDATA\\nORDER\\nMETRICSTIME\\nDATADISPOSITION\\nDATADELIVERY\\nCHANNEL DATACUSTOMERTARGET SOURCE\\nDelivery Contracts\\nShipment Tracking\\nInventory ManagementProductCustomerOrder ProcessingSOURCE IDENTIFICATION PROCESS\\n•List each data item of\\nmetrics or facts needed for\\nanalysis in fact tables.\\n\\x7fList each dimension\\nattribute from all dimensions.\\n\\x7fFor each target data item,\\nfind the source system and\\nsource data item.\\n\\x7fIf there are multiple\\nsources for one data element,\\nchoose the preferred source.\\n\\x7fIdentify multiple source\\nfields for a single target field\\nand form consolidation rules.\\n\\x7fIdentify single source field\\nfor multiple target fields and\\nestablish splitting rules.\\n\\x7fAscertain default values.\\n\\x7fInspect source data for\\nmissing values.\\nFigure 12-2 Source identification: a stepwise approach.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='818b002e-7f6e-4309-9791-0217cb362cc9', embedding=None, metadata={'page_label': '285', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='next. Customer name and address, bank account balances, and outstanding amounts on in-\\ndividual orders are some examples of this category.\\nWhat is the implication of this category for data extraction? The value of an attribute\\nremains constant only until a business transaction changes it. There is no telling when itwill get changed. Data extraction for preserving the history of the changes in the datawarehouse gets quite involved for this category of data.\\nPeriodic Status. This category is not as common as the previous category. In this cate-\\ngory, the value of the attribute is preserved as the status every time a change occurs. Ateach of these points in time, the status value is stored with reference to the time when thenew value became effective. This category also includes events stored with reference tothe time when each event occurred. Look at the way data about an insurance policy is usu-ally recorded in the operational systems of an insurance company. The operational data-bases store the status data of the policy at each point of time when something in the policychanges. Similarly, for an insurance claim, each event, such as claim initiation, verifica-tion, appraisal, and settlement, is recorded with reference to the points in time.\\nFor operational data in this category, the history of the changes is preserved in the\\nsource systems themselves. Therefore, data extraction for the purpose of keeping historyin the data warehouse is relatively easier. Whether it is status data or data about an event,the source systems contain data at each point in time when any change occurred. \\nPlease study Figure 12-3 and confirm your understanding of the two categories of data\\nstored in the operational systems. Pay special attention to the examples.\\nHaving reviewed the categories indicating how data is stored in the operational sys-DATA EXTRACTION 265\\nEXAMPLES OF ATTRIBUTESVALUES OF ATTRIBUTES AS STORED  IN\\nOPERATIONAL SYSTEMS AT DIFFERENT DATES\\nStoring Current Value\\nStoring Periodic StatusAttribute :  Customer’s State of Residence\\n6/1/2000      Value: OH\\n9/15/2000   Changed to CA\\n1/22/2001   Changed to NY\\n3/1/2001     Changed to NJ\\nAttribute :  Status of Property consigned\\nto an auction house for sale.\\n6/1/2000      Value: RE\\n(property receipted)\\n9/15/2000   Changed to ES\\n(value estimated)\\n1/22/2001   Changed to AS\\n(assigned to auction)\\n3/1/2001     Changed to SL\\n(property sold)6/1/2000\\nOH9/15/2000 1/22/2001 3/1/2001\\nCA NY NJ\\n6/1/2000   RE6/1/2000   RE\\n9/15/2000 ES\\n1/22/2001 AS\\n3/1/2001   SL6/1/2000   RE\\n9/15/2000 ES\\n1/22/2001 AS6/1/2000   RE\\n9/15/2000 ES6/1/2000 9/15/2000 1/22/20013/1/2001\\nFigure 12-3 Data in operational systems.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='54dc0f12-e97e-45ec-a352-90a96610ceee', embedding=None, metadata={'page_label': '286', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='tems, we are now in a position to discuss the common techniques for data extraction.\\nWhen you deploy your data warehouse, the initial data as of a certain time must be movedto the data warehouse to get it started. This is the initial load. After the initial load, yourdata warehouse must be kept updated so the history of the changes and statuses are re-flected in the data warehouse. Broadly, there are two major types of data extractions fromthe source operational systems: “as is” (static) data and data of revisions.\\n“As is” or static data is the capture of data at a given point in time. It is like taking a\\nsnapshot of the relevant source data at a certain point in time. For current or transient data,this capture would include all transient data identified for extraction. In addition, for datacategorized as periodic, this data capture would include each status or event at each pointin time as available in the source operational systems. \\nY ou will use static data capture primarily for the initial load of the data warehouse.\\nSometimes, you may want a full refresh of a dimension table. For example, assume thatthe product master of your source application is completely revamped. In this case, youmay find it easier to do a full refresh of the product dimension table of the target datawarehouse. So, for this purpose, you will perform a static data capture of the productdata. \\nData of revisions is also known as incremental data capture. Strictly, it is not incremen-\\ntal data but the revisions since the last time data was captured. If the source data is tran-sient, the capture of the revisions is not easy. For periodic status data or periodic eventdata, the incremental data capture includes the values of attributes at specific times. Ex-tract the statuses and events that have been recorded since the last date of extract. \\nIncremental data capture may be immediate or deferred. Within the group of immedi-\\nate data capture there are three distinct options. Two separate options are available for de-ferred data capture.\\nImmediate Data Extraction. In this option, the data extraction is real-time. It occurs as\\nthe transactions happen at the source databases and files. Figure 12-4 shows the immedi-ate data extraction options. \\nNow let us go into some details about the three options for immediate data extraction.Capture through Transaction Logs. This option uses the transaction logs of the DBMSs\\nmaintained for recovery from possible failures. As each transaction adds, updates, ordeletes a row from a database table, the DBMS immediately writes entries on the log file.This data extraction technique reads the transaction log and selects all the committedtransactions. There is no extra overhead in the operational systems because logging is al-ready part of the transaction processing. \\nY ou have to make sure that all transactions are extracted before the log file gets re-\\nfreshed. As log files on disk storage get filled up, the contents are backed up on other me-dia and the disk log files are reused. Ensure that all log transactions are extracted for datawarehouse updates.\\nIf all of your source systems are database applications, there is no problem with this\\ntechnique. But if some of your source system data is on indexed and other flat files, thisoption will not work for these cases. There are no log files for these nondatabase applica-tions. Y ou will have to apply some other data extraction technique for these cases.\\nWhile we are on the topic of data capture through transaction logs, let us take a side\\nexcursion and look at the use of replication. Data replication is simply a method for creat-ing copies of data in a distributed environment. Please refer to Figure 12-5 illustratinghow replication technology can be used to capture changes to source data. 266 DATA EXTRACTION, TRANSFORMATION, AND LOADING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='598f6e8a-61de-4f55-bb53-b74d9ec8251a', embedding=None, metadata={'page_label': '287', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='DATA EXTRACTION 267\\nSOURCE DATABASES\\nSOURCE\\nOPERATIONAL\\nSYSTEMSTransaction\\nLog\\nFiles\\nExtract\\nFiles from\\nSource\\nSystems Output Files\\nof Trigger\\nProgramsTrigger\\nProgramsSource\\nData\\nDBMSOPTION 1:\\nCapture through\\ntransaction logs\\nOPTION 2:\\nCapture through\\ndatabase triggers\\nOPTION 3:\\nCapture in source\\napplications\\nFigure 12-4 Immediate data extraction: options.\\nFigure 12-5 Data extraction: using replication technology.SOURCE DATABASES\\nSOURCE\\nOPERATIONAL\\nSYSTEMSTransaction\\nLog\\nFiles\\nDBMS\\nREPLICATION\\nSERVERLog Transaction\\nManager\\nReplicated Log\\nTransactions stored in\\nData Staging AreaSource\\nDataDATA STAGING AREA', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d0d4c14a-2dbe-4e02-a413-f4f2ec4d45bf', embedding=None, metadata={'page_label': '288', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The appropriate transaction logs contain all the changes to the various source database\\ntables. Here are the broad steps for using replication to capture changes to source data:\\n/L50539Identify the source system DB table\\n/L50539Identify and define target files in staging area\\n/L50539Create mapping between source table and target files\\n/L50539Define the replication mode \\n/L50539Schedule the replication process\\n/L50539Capture the changes from the transaction logs\\n/L50539Transfer captured data from logs to target files\\n/L50539Verify transfer of data changes\\n/L50539Confirm success or failure of replication\\n/L50539In metadata, document the outcome of replication\\n/L50539Maintain definitions of sources, targets, and mappings \\nCapture through Database Triggers. Again, this option is applicable to your source sys-\\ntems that are database applications. As you know, triggers are special stored procedures(programs) that are stored on the database and fired when certain predefined events occur.Y ou can create trigger programs for all events for which you need data to be captured. Theoutput of the trigger programs is written to a separate file that will be used to extract datafor the data warehouse. For example, if you need to capture all changes to the records in thecustomer table, write a trigger program to capture all updates and deletes in that table. \\nData capture through database triggers occurs right at the source and is therefore quite\\nreliable. Y ou can capture both before and after images. However, building and maintainingtrigger programs puts an additional burden on the development effort. Also, execution oftrigger procedures during transaction processing of the source systems puts additionaloverhead on the source systems. Further, this option is applicable only for source data indatabases. \\nCapture in Source Applications. This technique is also referred to as application-assist-\\ned data capture. In other words, the source application is made to assist in the data capturefor the data warehouse. Y ou have to modify the relevant application programs that write tothe source files and databases. Y ou revise the programs to write all adds, updates, anddeletes to the source files and database tables. Then other extract programs can use theseparate file containing the changes to the source data.\\nUnlike the previous two cases, this technique may be used for all types of source data\\nirrespective of whether it is in databases, indexed files, or other flat files. But you have torevise the programs in the source operational systems and keep them maintained. Thiscould be a formidable task if the number of source system programs is large. Also, thistechnique may degrade the performance of the source applications because of the addi-tional processing needed to capture the changes on separate files. \\nDeferred Data Extraction. In the cases discussed above, data capture takes place while\\nthe transactions occur in the source operational systems. The data capture is immediate orreal-time. In contrast, the techniques under deferred data extraction do not capture thechanges in real time. The capture happens later. Please see Figure 12-6 showing the de-ferred data extraction options. 268 DATA EXTRACTION, TRANSFORMATION, AND LOADING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dfacb077-aa6c-454b-ae91-3e2f54ac74f1', embedding=None, metadata={'page_label': '289', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Now let us discuss the two options for deferred data extraction.\\nCapture Based on Date and Time Stamp. Every time a source record is created or up-\\ndated it may be marked with a stamp showing the date and time. The time stamp providesthe basis for selecting records for data extraction. Here the data capture occurs at a latertime, not while each source record is created or updated. If you run your data extractionprogram at midnight every day, each day you will extract only those with the date andtime stamp later than midnight of the previous day. This technique works well if the num-ber of revised records is small.\\nOf course, this technique presupposes that all the relevant source records contain date\\nand time stamps. Provided this is true, data capture based on date and time stamp canwork for any type of source file. This technique captures the latest state of the source data.Any intermediary states between two data extraction runs are lost.\\nDeletion of source records presents a special problem. If a source record gets deleted in\\nbetween two extract runs, the information about the delete is not detected. Y ou can getaround this by marking the source record for delete first, do the extraction run, and thengo ahead and physically delete the record. This means you have to add more logic to thesource applications. \\nCapture by Comparing Files. If none of the above techniques are feasible for specific\\nsource files in your environment, then consider this technique as the last resort. This tech-nique is also called the snapshot differential technique because it compares two snapshotsof the source data. Let us see how this technique works.\\nSuppose you want to apply this technique to capture the changes to your product data.DATA EXTRACTION 269\\nSOURCE DATABASES\\nSOURCE\\nOPERATIONAL\\nSYSTEMSToday’s\\nExtract\\nExtract\\nFiles based\\non time -\\nstampSource\\nData\\nDBMS\\nOPTION 1:\\nCapture based\\non date and time\\nstampEXTRACT\\nPROGRAMSYesterday’s\\nExtract\\nFILE\\nCOMPARISON\\nPROGRAMSExtract\\nFiles based\\non file\\ncomparisonOPTION 2:\\nCapture by\\ncomparing files\\nFigure 12-6 Deferred data extraction: options.DATA STAGING AREA', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7d426003-b35a-4e75-be05-c582d9984d26', embedding=None, metadata={'page_label': '290', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='While performing today’ s data extraction for changes to product data, you do a full file\\ncomparison between today’ s copy of the product data and yesterday’ s copy. Y ou also com-pare the record keys to find the inserts and deletes. Then you capture any changes be-tween the two copies. \\nThis technique necessitates the keeping of prior copies of all the relevant source data.\\nThough simple and straightforward, comparison of full rows in a large file can be very in-efficient. However, this may be the only feasible option for some legacy data sources thatdo not have transaction logs or time stamps on source records. \\nEvaluation of the Techniques\\nTo summarize, the following options are available for data extraction:\\n/L50539Capture of static data\\n/L50539Capture through transaction logs\\n/L50539Capture through database triggers\\n/L50539Capture in source applications\\n/L50539Capture based on date and time stamp\\n/L50539Capture by comparing files\\nY ou are faced with some big questions. Which ones are applicable in your environ-\\nment? Which techniques must you use? Y ou will be using the static data capture techniqueat least in one situation when you populate the data warehouse initially at the time of de-ployment. After that, you will usually find that you need a combination of a few of thesetechniques for your environment. If you have old legacy systems, you may even have theneed for the file comparison method. \\nFigure 12-7 highlights the advantages and disadvantages of the different techniques.\\nPlease study it carefully and use it to determine the techniques you would need to use inyour environment.\\nLet us make a few general comments. Which of the techniques are easy and inexpen-\\nsive to implement? Consider the techniques of using transaction logs and database trig-gers. Both of these techniques are already available through the database products. Bothare comparatively cheap and easy to implement. The technique based on transaction logsis perhaps the most inexpensive. There is no additional overhead on the source operationalsystems. In the case of database triggers, there is a need to create and maintain triggerprograms. Even here, the maintenance effort and the additional overhead on the sourceoperational systems are not that much compared to other techniques.\\nData capture in source systems could be the most expensive in terms of development\\nand maintenance. This technique needs substantial revisions to existing source systems.For many legacy source applications, finding the source code and modifying it may notbe feasible at all. However, if the source data does not reside on database files and dateand time stamps are not present in source records, this is one of the few available op-tions.\\nWhat is the impact on the performance of the source operational systems? Certainly,\\nthe deferred data extraction methods have the least impact on the operational systems.Data extraction based on time stamps and data extraction based on file comparisons areperformed outside the normal operation of the source systems. Therefore, these two are270 DATA EXTRACTION, TRANSFORMATION, AND LOADING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a70ef87d-10a9-43e6-bb54-4ba9e8107f51', embedding=None, metadata={'page_label': '291', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='preferred options when minimizing the impact on operational systems is a priority. How-\\never, these deferred capture options suffer from some inadequacy. They track the changesfrom the state of the source data at the time of the current extraction as compared to itsstate at the time of the previous extraction. Any interim changes are not captured. There-fore, wherever you are dealing with transient source data, you can only come up with ap-proximations of the history.\\nSo what is the bottom line? Use the data capture technique in source systems sparingly\\nbecause it involves too much development and maintenance work. For your source data ondatabases, capture through transaction logs and capture through database triggers are ob-vious first choices. Between these two, capture through transaction logs is a better choicebecause of better performance. Also, this technique is applicable to nonrelational databas-es. The file comparison method is the most time-consuming for data extraction. Use itonly if all others cannot be applied.\\nDATA TRANSFORMATION\\nBy making use of the several techniques discussed in the previous section, you design the\\ndata extraction function. Now the extracted data is raw data and it cannot be applied to thedata warehouse. First, all the extracted data must be made usable in the data warehouse.Having information that is usable for strategic decision making is the underlying principleof the data warehouse. Y ou know that the data in the operational systems is not usable forthis purpose. Next, because operational data is extracted from many old legacy systems,the quality of the data in those systems is less likely to be good enough for the data ware-DATA TRANSFORMATION 271\\nCapture of static data Capture in source applications\\nCapture through transaction logs Capture based on date and time stamp\\nCapture through database triggers Capture by comparing filesGood flexibility for capture specifications.     \\nPerformance of source systems not affected.    No revisions to existing applications.                Can be used on legacy systems.                            Can be used on file-oriented systems.                 Vendor products are used. No internal costs.  \\nNot much flexibility for capture specifications.     \\nPerformance of source systems not affected.    No revisions to existing applications.                Can be used on most legacy systems.                            Cannot be used on file-oriented systems.                 Vendor products are used. No internal costs.  \\nNot much flexibility for capture specifications.     \\nPerformance of source systems affected a bit.    No revisions to existing applications.                Cannot be used on most legacy systems.                          Cannot be used on file-oriented systems.                 Vendor products are used. No internal costs.  Good flexibility for capture specifications.     \\nPerformance of source systems affected a bit.    Major revisions to existing applications.             Can be used on most legacy systems.                  Can be used on file-oriented systems.                 High internal costs because of in-house work.  \\nGood flexibility for capture specifications.     \\nPerformance of source systems not affected.     Major revisions to existing applications likely.   Cannot be used on most legacy systems.             Can be used on file-oriented systems.                 Vendor products may be used.  \\nGood flexibility for capture specifications.     \\nPerformance of source systems not affected.     No revisions to existing applications.                May  be used on legacy systems.                         May  be used on file-oriented systems.                Vendor products are used. No internal costs.  \\nFigure 12-7 Data capture techniques: advantages and disadvantages.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ff8d00fc-f58b-422e-a5a4-98e0369988ff', embedding=None, metadata={'page_label': '292', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='house. Y ou have to enrich and improve the quality of the data before it could be usable in\\nthe data warehouse.\\nBefore moving the extracted data from the source systems into the data warehouse, you\\ninevitably have to perform various kinds of data transformations. Y ou have to transformthe data according to standards because they come from many dissimilar source systems.Y ou have to ensure that after all the data is put together, the combined data does not vio-late any business rules. \\nConsider the data structures and data elements that you need in your data warehouse.\\nNow think about all the relevant data to be extracted from the source systems. From thevariety of source data formats, data values, and the condition of the data quality, you knowthat you have to perform several types of transformations to make the source data suitablefor your data warehouse. Transformation of source data encompasses a wide variety ofmanipulations to change all the extracted source data into usable information to be storedin the data warehouse.\\nMany companies underestimate the extent and complexity of the data transformation\\nfunctions. They start out with a simple departmental data mart as the pilot project. Almostall of the data for this pilot comes from a single source application. The data transforma-tion just entails field conversions and some reformatting of the data structures. Do notmake the mistake of taking the data transformation functions too lightly. Be prepared toconsider all the different issues and allocate sufficient time and effort to the task of de-signing the transformations. \\nData warehouse practitioners have attempted to classify data transformations in several\\nways, beginning with the very general and broad classifications of simple transformationsand complex transformations. There is also some confusion about the semantics. Onepractitioner may refer to data integration as the process within the data transformationfunction that is some kind of preprocessing of the source data. To another practitioner,data integration may mean the mapping of the source fields to the target fields in the datawarehouse. Resisting the temptation to generalize and classify, we will highlight and dis-cuss the common types of major transformation functions. Y ou may review each type anddecide for yourself if that type is going to be simple or complex in your own data ware-house environment.\\nOne major effort within data transformation is the improvement of data quality. In a\\nsimple sense, this includes filling in the missing values for attributes in the extracted data.Data quality is of paramount importance in the data warehouse because the effect ofstrategic decisions based on incorrect information can be devastating. Therefore, we willdiscuss data quality issues extensively in the next chapter. \\nData Transformation: Basic Tasks\\nIrrespective of the variety and complexity of the source operational systems, and regard-\\nless of the extent of your data warehouse, you will find that most of your data transforma-tion functions break down into a few basic tasks. Let us go over these basic tasks so thatyou can view data transformation from a fundamental perspective. Here is the set of basictasks:\\nSelection. This takes place at the beginning of the whole process of data transforma-\\ntion. Y ou select either whole records or parts of several records from the source sys-tems. The task of selection usually forms part of the extraction function itself. How-272 DATA EXTRACTION, TRANSFORMATION, AND LOADING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='81033a28-c5c0-41a4-8a29-982be3fadd57', embedding=None, metadata={'page_label': '293', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ever, in some cases, the composition of the source structure may not be amenable to\\nselection of the necessary parts during data extraction. In these cases, it is prudentto extract the whole record and then do the selection as part of the transformationfunction.\\nSplitting/joining. This task includes the types of data manipulation you need to per-\\nform on the selected parts of source records. Sometimes (uncommonly), you will besplitting the selected parts even further during data transformation. Joining of partsselected from many source systems is more widespread in the data warehouse envi-ronment.\\nConversion. This is an all-inclusive task. It includes a large variety of rudimentary\\nconversions of single fields for two primary reasons—one to standardize among thedata extractions from disparate source systems, and the other to make the fields us-able and understandable to the users.\\nSummarization. Sometimes you may find that it is not feasible to keep data at the\\nlowest level of detail in your data warehouse. It may be that none of your users everneed data at the lowest granularity for analysis or querying. For example, for a gro-cery chain, sales data at the lowest level of detail for every transaction at the check-out may not be needed. Storing sales by product by store by day in the data ware-house may be quite adequate. So, in this case, the data transformation functionincludes summarization of daily sales by product and by store.\\nEnrichment. This task is the rearrangement and simplification of individual fields to\\nmake them more useful for the data warehouse environment. Y ou may use one ormore fields from the same input record to create a better view of the data for thedata warehouse. This principle is extended when one or more fields originate frommultiple records, resulting in a single field for the data warehouse. \\nMajor Transformation Types\\nY ou have looked at the set of basic transformation tasks. When you consider a particular\\nset of extracted data structures, you will find that the transformation functions you need toperform on this set may done by doing a combination of the basic tasks discussed. \\nNow let us consider specific types of transformation functions. These are the most\\ncommon transformation types:\\nFormat Revisions. Y ou will come across these quite often. These revisions include\\nchanges to the data types and lengths of individual fields. In your source systems,product package types may be indicated by codes and names in which the fields arenumeric and text data types. Again, the lengths of the package types may varyamong the different source systems. It is wise to standardize and change the datatype to text to provide values meaningful to the users. \\nDecoding of Fields. This is also a common type of data transformation. When you\\ndeal with multiple source systems, you are bound to have the same data items de-scribed by a plethora of field values. The classic example is the coding for gender,with one source system using 1 and 2 for male and female and another system usingM and F . Also, many legacy systems are notorious for using cryptic codes to repre-sent business values. What do the codes AC, IN, RE, and SU mean in a customerfile? Y ou need to decode all such cryptic codes and change these into values thatDATA TRANSFORMATION 273', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4a863d01-1c31-479d-8481-f6bfbbf28177', embedding=None, metadata={'page_label': '294', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='make sense to the users. Change the codes to Active, Inactive, Regular, and Sus-\\npended. \\nCalculated and Derived Values. What if you want to keep profit margin along with\\nsales and cost amounts in your data warehouse tables? The extracted data from thesales system contains sales amounts, sales units, and operating cost estimates byproduct. Y ou will have to calculate the total cost and the profit margin before datacan be stored in the data warehouse. Average daily balances and operating ratios areexamples of derived fields. \\nSplitting of Single Fields. Earlier legacy systems stored names and addresses of cus-\\ntomers and employees in large text fields. The first name, middle initials, and lastname were stored as a large text in a single field. Similarly, some earlier systemsstored city, state, and Zip Code data together in a single field. Y ou need to store in-dividual components of names and addresses in separate fields in your data ware-house for two reasons. First, you may improve the operating performance by index-ing on individual components. Second, your users may need to perform analysis byusing individual components such as city, state, and Zip Code. \\nMerging of Information. This is not quite the opposite of splitting of single fields.\\nThis type of data transformation does not literally mean the merging of severalfields to create a single field of data. For example, information about a product maycome from different data sources. The product code and description may come fromone data source. The relevant package types may be found in another data source.The cost data may be from yet another source. In this case, merging of informationdenotes the combination of the product code, description, package types, and costinto a single entity.\\nCharacter Set Conversion. This type of data transformation relates to the conversion\\nof character sets to an agreed standard character set for textual data in the data ware-house. If you have mainframe legacy systems as source systems, the source datafrom these systems will be in EBCDIC characters. If PC-based architecture is thechoice for your data warehouse, then you must convert the mainframe EBCDIC for-mat to the ASCII format. When your source data is on other types of hardware andoperating systems, you are faced with similar character set conversions. \\nConversion of Units of Measurements. Many companies today have global branches.\\nMeasurements in many European countries are in metric units. If your company hasoverseas operations, you may have to convert the metrics so that the numbers mayall be in one standard unit of measurement.\\nDate/Time Conversion. This type relates to representation of date and time in stan-\\ndard formats. For example, the American and the British date formats may be stan-dardized to an international format. The date of October 11, 2000 is written as10/11/2000 in the U.S. format and as 11/10/2000 in the British format. This datemay be standardized to be written as 11 OCT 2000.\\nSummarization. This type of transformation is the creating of summaries to be\\nloaded in the data warehouse instead of loading the most granular level of data.For example, for a credit card company to analyze sales patterns, it may not benecessary to store in the data warehouse every single transaction on each creditcard. Instead, you may want to summarize the daily transactions for each creditcard and store the summary data instead of storing the most granular data by in-dividual transactions. 274 DATA EXTRACTION, TRANSFORMATION, AND LOADING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='55843ee8-8e59-4192-8111-30b1ca0e3709', embedding=None, metadata={'page_label': '295', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Key Restructuring. While extracting data from your input sources, look at the prima-\\nry keys of the extracted records. Y ou will have to come up with keys for the fact anddimension tables based on the keys in the extracted records. Please see Figure 12-8.In the example shown in the figure, the product code in this organization is struc-tured to have inherent meaning. If you use this product code as the primary key,there would be problems. If the product is moved to another warehouse, the ware-house part of the product key will have to be changed. This is a typical problem withlegacy systems. When choosing keys for your data warehouse database tables, avoidsuch keys with built-in meanings. Transform such keys into generic keys generatedby the system itself. This is called key restructuring.\\nDeduplication. In many companies, the customer files have several records for the\\nsame customer. Mostly, the duplicates are the result of creating additional recordsby mistake. In your data warehouse, you want to keep a single record for one cus-tomer and link all the duplicates in the source systems to this single record. Thisprocess is called deduplication of the customer file. Employee files and, sometimes,product master files have this kind of duplication problem.\\nData Integration and Consolidation\\nThe real challenge of ETL functions is the pulling together of all the source data from\\nmany disparate, dissimilar source systems. As of today, most the data warehouses get dataextracted from a combination of legacy mainframe systems, old minicomputer applica-tions, and some newer client/server systems. Most of these source systems do not con-form to the same set of business rules. Very often they follow different naming conven-tions and varied standards for data representation. Figure 12-9 shows a typical data sourceenvironment. Notice the challenging issues indicated in the figure.\\nIntegrating the data is the combining of all the relevant operational data into coherentDATA TRANSFORMATION 275\\nPRODUCT CODE:      12  W1  M53  1234  69PRODUCTION SYSTEM   KEY\\nCountry \\nCodeWarehouse \\nCodeSales \\nTerritoryProduct \\nNumberSales \\nPerson\\nDATA WAREHOUSE -- PRODUCT KEY\\n12345678\\nFigure 12-8 Data transformation: key restructuring.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c0918c37-78ee-4cd7-904d-ade55eebf5f0', embedding=None, metadata={'page_label': '296', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='data structures to be made ready for loading into the data warehouse. Y ou may want to\\nthink of data integration and consolidation as a type of preprocess before other majortransformation routines are applied. Y ou have to standardize the names and data represen-tations and resolve discrepancies in the ways in which same data is represented in differ-ent source systems. Although time-consuming, many of the data integration tasks can bemanaged. However, let us go over a couple of more difficult challenges.\\nEntity Identification Problem. If you have three different legacy applications devel-\\noped in your organization at different times in the past, you are likely to have three differ-ent customer files supporting those systems. One system may be the old order entry sys-tem, another the customer service support system, and the third the marketing system.Most of the customers will be common to all three files. The same customer on each ofthe files may have a unique identification number. These unique identification numbersfor the same customer may not be the same across the three systems. \\nThis is a problem of identification in which you do not know which of the customer\\nrecords relate to the same customer. But in the data warehouse you need to keep a singlerecord for each customer. Y ou must be able to get the activities of the single customer fromthe various source systems and then match up with the single record to be loaded to the datawarehouse. This is a common but very difficult problem in many enterprises where appli-cations have evolved over time from the distant past. This type of problem is prevalentwhere multiple sources exist for the same entities. Vendors, suppliers, employees, andsometimes products are the kinds of entities that are prone to this type of problem.\\nIn the above example of the three customer files, you have to design complex algo-\\nrithms to match records from all the three files and form groups of matching records. No276 DATA EXTRACTION, TRANSFORMATION, AND LOADING\\nMINIUNIX\\nMAINFRAME\\n*Multiple character sets (EBCDIC/ASCII)*\\n*Multiple data types* *Missing values *\\n*No default values* *Multiple naming standards*\\n*Conflicting business rules*   *Incompatible structures*\\n*Inconsistent values*\\nFigure 12-9 Typical data source environment.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b16948a7-c3df-477b-9e3f-819d277a1a75', embedding=None, metadata={'page_label': '297', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='matching algorithm can completely determine the groups. If the matching criteria are too\\ntight, then some records will escape the groups. On the other hand, if the matching criteriaare too loose, a particular group may include records of more than one customer. Y ou needto get your users involved in reviewing the exceptions to the automated procedures. Y ouhave to weigh the issues relating to your source systems and decide how to handle the en-tity identification problem. Every time a data extract function is performed for your datawarehouse, which may be every day, do you pause to resolve the entity identificationproblem before loading the data warehouse? How will this affect the availability of thedata warehouse to your users? Some companies, depending on their individual situations,take the option of solving the entity identification problem in two phases. In the firstphase, all records, irrespective of whether they are duplicates or not, are assigned uniqueidentifiers. The second phase consists of reconciling the duplicates periodically throughautomatic algorithms and manual verification. \\nMultiple Sources Problem. This is another kind of problem affecting data integra-\\ntion, although less common and less complex than the entity identification problem. Thisproblem results from a single data element having more than one source. For example,suppose unit cost of products is available from two systems. In the standard costing appli-cation, cost values are calculated and updated at specific intervals. Y our order processingsystem also carries the unit costs for all products. There could be slight variations in thecost figures from these two systems. From which system should you get the cost for stor-ing in the data warehouse?\\nA straightforward solution is to assign a higher priority to one of the two sources and\\npick up the product unit cost from that source. Sometimes, a straightforward solution suchas this may not sit well with needs of the data warehouse users. Y ou may have to selectfrom either of the files based on the last update date. Or, in some other instances, your de-termination of the appropriate source depends on other related fields.\\nTransformation for Dimension Attributes\\nIn Chapter 11, we discussed the changes to dimension table attributes. We reviewed the\\ntypes of changes to these attributes. Also, we suggested ways to handle the three types ofslowly changing dimensions. Type 1 changes are corrections of errors. These changes areapplied to the data warehouse without any need to preserve history. Type 2 changes pre-serve the history in the data warehouse. Type 3 changes are tentative changes where yourusers need the ability to analyze the metrics in both ways—with the changes and withoutthe changes. \\nIn order to apply the changes correctly, you need to transform the incoming changes and\\nprepare the changes to the data for loading into the data warehouse. Figure 12-10 illustrateshow the data extraction of the changes in the source systems are transformed and preparedfor data loading. This figure shows the handling of each type of changes to dimension ta-bles. Types 1, 2, and 3 are shown distinctly. Please review the figure carefully to get a goodgrasp of the solutions. Y ou will be faced with dimension table changes all the time.\\nHow to Implement Transformation\\nThe complexity and the extent of data transformation strongly suggest that manual meth-\\nods alone will not be enough. Y ou must go beyond the usual methods of writing conver-DATA TRANSFORMATION 277', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f17d5b34-80d9-4f1a-aec9-788dd29eea98', embedding=None, metadata={'page_label': '298', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='sion programs when you deployed operational systems. The types of data transformation\\nare by far more difficult and challenging. \\nThe methods you may want to adopt depend on some significant factors. If you are\\nconsidering automating most of the data transformation functions, first consider if youhave the time to select the tools, configure and install them, train the project team on thetools, and integrate the tools into the data warehouse environment. Data transformationtools can be expensive. If the scope of your data warehouse is modest, then the projectbudget may not have room for transformation tools. \\nLet us look at the issues relating to using manual techniques and to the use of data\\ntransformation tools. In many cases, a suitable combination of both methods will prove tobe effective. Find the proper balance based on the available time frame and the money inthe budget.\\nUsing Transformation Tools. In recent years, transformation tools have greatly in-\\ncreased in functionality and flexibility. Although the desired goal for using transformationtools is to eliminate manual methods altogether, in practice this is not completely possi-ble. Even if you get the most sophisticated and comprehensive set of transformation tools,be prepared to use in-house programs here and there.\\nUse of automated tools certainly improves efficiency and accuracy. As a data transfor-\\nmation specialist, you just have to specify the parameters, the data definitions, and therules to the transformation tool. If your input into the tool is accurate, then the rest of thework is performed efficiently by the tool. \\nY ou gain a major advantage from using a transformation tool because of the recording\\nof metadata by the tool. When you specify the transformation parameters and rules, theseare stored as metadata by the tool. This metadata then becomes part of the overall metada-ta component of the data warehouse. It may be shared by other components. When278 DATA EXTRACTION, TRANSFORMATION, AND LOADING\\nSource System\\ndata changes\\nfor dimensionsPerform data\\ntransformation functions\\nTYPE 1TYPE 2TYPE 3\\nConvert production key to\\nexisting surrogate keyPerform data cleansing\\nfunctions\\nConsolidate and integrate\\ndataDetermine type of\\ndimension change\\nConvert production key to\\nnew surrogate keyConvert production key to\\nexisting surrogate key\\nCreate\\nload imageCreate\\nload imageCreate load image\\n(include effective date)\\nFigure 12-10 Transformed for dimension changes.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='73ad9d11-38d5-4be2-badb-01c9ef7677a8', embedding=None, metadata={'page_label': '299', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='changes occur to transformation functions because of changes in business rules or data\\ndefinitions, you just have to enter the changes into the tool. The metadata for the transfor-mations get automatically adjusted by the tool. \\nUsing Manual Techniques. This was the predominant method until recently when\\ntransformation tools began to appear in the market. Manual techniques may still be ade-quate for smaller data warehouses. Here manually coded programs and scripts performevery data transformation. Mostly, these programs are executed in the data staging area.The analysts and programmers who already possess the knowledge and the expertise areable to produce the programs and scripts. \\nOf course, this method involves elaborate coding and testing. Although the initial cost\\nmay be reasonable, ongoing maintenance may escalate the cost. Unlike automated tools,the manual method is more likely to be prone to errors. It may also turn out that several in-dividual programs are required in your environment. \\nA major disadvantage relates to metadata. Automated tools record their own metadata,\\nbut in-house programs have to be designed differently if you need to store and use meta-data. Even if the in-house programs record the data transformation metadata initially,every time changes occur to transformation rules, the metadata has to be maintained. Thisputs an additional burden on the maintenance of the manually coded transformation pro-grams. \\nDATA LOADING\\nIt is generally agreed that transformation functions end as soon as load images are creat-\\ned. The next major set of functions consists of the ones that take the prepared data, applyit to the data warehouse, and store it in the database there. Y ou create load images to cor-respond to the target files to be loaded in the data warehouse database.\\nThe whole process of moving data into the data warehouse repository is referred to in\\nseveral ways. Y ou must have heard the phrases applying the data, loading the data, and re-freshing the data. For the sake of clarity we will use the phrases as indicated below:\\nInitial Load—populating all the data warehouse tables for the very first time\\nIncremental Load—applying ongoing changes as necessary in a periodic mannerFull Refresh—completely erasing the contents of one or more tables and reloading\\nwith fresh data (initial load is a refresh of all the tables)\\nBecause loading the data warehouse may take an inordinate amount of time, loads are\\ngenerally cause for great concern. During the loads, the data warehouse has to be offline.Y ou need to find a window of time when the loads may be scheduled without affectingyour data warehouse users. Therefore, consider dividing up the whole load process intosmaller chunks and populating a few files at a time. This will give you two benefits. Y oumay be able to run the smaller loads in parallel. Also, you might be able to keep someparts of the data warehouse up and running while loading the other parts. It is hard to esti-mate the running times of the loads, especially the initial load or a complete refresh. Dotest loads to verify the correctness and to estimate the running times.\\nWhen you are running a load, do not expect every record in the source load image fileDATA LOADING 279', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='72789553-d65e-477e-ad84-d52ce78d4432', embedding=None, metadata={'page_label': '300', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='to be successfully applied to the data warehouse. For the record you are trying to load to\\nthe fact table, the concatenated key may be wrong and not correspond to the dimension ta-bles. Provide procedures to handle the load images that do not load. Also, have a plan forquality assurance of the loaded records.\\nIf the data staging area and the data warehouse database are on the same server, that\\nwill save you the effort of moving the load images to the data warehouse server. But if youhave to transport the load images to the data warehouse server, consider the options care-fully and select the ones best suited for your environment. The Web, FTP , and databaselinks are a few of the options. Y ou have to consider the necessary bandwidth needed andalso the impact of the transmissions on the network. Think of data compression and havecontingency plans.\\nWhat are the general methods for applying data? The most straightforward method is\\nwriting special load programs. Depending on the size of your data warehouse, the numberof load programs can be large. Managing the load runs of a large number of programs canbe challenging. Further, maintaining a large suite of special load programs consumes a lotof time and effort. Load utilities that come with the DBMSs provide a fast method forloading. Consider this method as a primary choice. When the staging area files and thedata warehouse repository are on different servers, database links are useful.\\nY ou are already aware of some of the concerns and difficulties in data loading. The\\nproject team has to be very familiar with the common challenges so that it can work outproper resolutions. Let us now move on to the details of the data loading techniques andprocesses.\\nApplying Data: Techniques and Processes\\nEarlier in this section, we defined three types of application of data to the data warehouse:\\ninitial load, incremental load, and full refresh. Consider how data is applied in each ofthese types. Let us take the example of product data. For the initial load, you extract thedata for all the products from the various source systems, integrate and transform the data,and then create load images for loading the data into the product dimension table. For anincremental load, you collect the changes to the product data for those product recordsthat have changed in the source systems since the previous extract, run the changesthrough the integration and transformation process, and create output records to be ap-plied to the product dimension table. A full refresh is similar to the initial load. \\nIn every case, you create a file of data to be applied to the product dimension table\\nin the data warehouse. How can you apply the data to the warehouse? What are themodes? Data may be applied in the following four different modes: load, append, de-structive merge, and constructive merge. Please study Figure 12-11 carefully to under-stand the effect of applying data in each of these four modes. Let us explain how eachmode works.\\nLoad. If the target table to be loaded already exists and data exists in the table, the load\\nprocess wipes out the existing data and applies the data from the incoming file. If thetable is already empty before loading, the load process simply applies the data from theincoming file. \\nAppend. Y ou may think of the append as an extension of the load. If data already exists\\nin the table, the append process unconditionally adds the incoming data, preserving the280 DATA EXTRACTION, TRANSFORMATION, AND LOADING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f97a1269-6280-434c-b9fc-062c44eab024', embedding=None, metadata={'page_label': '301', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='existing data in the target table. When an incoming record is a duplicate of an already ex-\\nisting record, you may define how to handle an incoming duplicate. The incoming recordmay be allowed to be added as a duplicate. In the other option, the incoming duplicaterecord may be rejected during the append process.\\nDestructive Merge. In this mode, you apply the incoming data to the target data. If\\nthe primary key of an incoming record matches with the key of an existing record, updatethe matching target record. If the incoming record is a new record without a match withany existing record, add the incoming record to the target table.\\nConstructive Merge. This mode is slightly different from the destructive merge. If\\nthe primary key of an incoming record matches with the key of an existing record, leavethe existing record, add the incoming record, and mark the added record as supercedingthe old record.\\nLet us now consider how these modes of applying data to the data warehouse fit into\\nthe three types of loads. We will discuss these one by one. \\nInitial Load. Let us say you are able to load the whole data warehouse in a single run.\\nAs a variation of this single run, let us say you are able to split the load into separatesubloads and run each of these subloads as single loads. In other words, every load runcreates the database tables from scratch. In these cases, you will be using the load modediscussed above. \\nIf you need more than one run to create a single table, and your load runs for a single\\ntable must be schedule to run several days, then the approach is different. For the first runDATA LOADING 281\\nKey    Data\\n123     AAAAA\\n234     BBBBB\\n345     CCCCCDATA\\nSTAGING\\nKey    Data\\n123     AAAAA\\n234     BBBBB\\n345     CCCCCDATA\\nSTAGING\\nKey    Data\\n123     AAAAA\\n234     BBBBB\\n345     CCCCCDATA\\nSTAGING\\nKey    Data\\n123     AAAAA\\n234     BBBBB\\n345     CCCCCDATA\\nSTAGING\\nB\\nEFOR\\nAF\\nTE\\nRKey    Data\\n555     PPPPP\\n666     QQQQ\\n777    HHHHWAREHOUSE\\nKey    Data\\n123     AAAAA\\n234     BBBBB\\n345     CCCCCWAREHOUSELoad\\nKey    Data\\n111     PPPPPWAREHOUSE\\nKey    Data\\n111    PPPPP\\n123     AAAAA\\n234     BBBBB\\n345     CCCCCWAREHOUSEAppendDestructive\\nMerge\\nKey    Data\\n123    PPPPPWAREHOUSE\\nKey    Data\\n123    AAAAA*\\n123     PPPPP\\n234     BBBBB\\n345     CCCCCWAREHOUSEKey    Data\\n123     PPPPPWAREHOUSE\\nKey    Data\\n123     AAAAA\\n234     BBBBB\\n345     CCCCCWAREHOUSEConstructive\\nMerge\\nFigure 12-11 Modes of applying data.AFTER              BEFORE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='15a0a262-91f1-4f4a-9f60-0f115762334e', embedding=None, metadata={'page_label': '302', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='of the initial load of a particular table, use the load mode. All further runs will apply the\\nincoming data using the append mode. \\nCreation of indexes on initial loads or full refreshes requires special consideration. In-\\ndex creation on mass loads can be too time-consuming. So drop the indexes prior to theloads to make the loads go quicker. Y ou may rebuild or regenerate the indexes when theloads are complete.\\nIncremental Loads. These are the applications of ongoing changes from the source\\nsystems. Changes to the source systems are always tied to specific times, irrespective ofwhether or not they are based on explicit time stamps in the source systems. Therefore,you need a method to preserve the periodic nature of the changes in the data warehouse.\\nLet us review the constructive merge mode. In this mode, if the primary key of an in-\\ncoming record matches with the key of an existing record, the existing record is left inthe target table as is and the incoming record is added and marked as superceding theold record. If the time stamp is also part of the primary key or if the time stamp is in-cluded in the comparison between the incoming and the existing records, then construc-tive merge may be used to preserve the periodic nature of the changes. This is an over-simplification of the exact details of how constructive merge may be used. Nevertheless,the point is that the constructive merge mode is an appropriate method for incrementalloads. The details will have to be worked out based on the nature of the individual tar-get tables. \\nAre there cases in which the mode of destructive merge may be applied? What about a\\nType 1 slowly changing dimension? In this case, the change to a dimension table record ismeant to correct an error in the existing record. The existing record must be replaced bythe corrected incoming record, so you may use the destructive merge mode. This mode isalso applicable to any target tables where the historical perspective is not important. \\nFull Refresh. This type of application of data involves periodically rewriting the entire\\ndata warehouse. Sometimes, you may also do partial refreshes to rewrite only specific ta-bles. Partial refreshes are rare because every dimension table is intricately tied to the facttable.\\nAs far as the data application modes are concerned, full refresh is similar to the ini-\\ntial load. However, in the case of full refreshes, data exists in the target tables before in-coming data is applied. The existing data must be erased before applying the incomingdata. Just as in the case of the initial load, the load and append modes are applicable tofull refresh.\\nData Refresh Versus Update\\nAfter the initial load, you may maintain the data warehouse and keep it up-to-date by us-\\ning two methods:\\nUpdate—application of incremental changes in the data sources\\nRefresh—complete reload at specified intervals\\nTechnically, refresh is a much simpler option than update. To use the update option,\\nyou have to devise the proper strategy to extract the changes from each data source. Thenyou have to determine the best strategy to apply the changes to the data warehouse. The282 DATA EXTRACTION, TRANSFORMATION, AND LOADING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dd255a03-b7c4-4787-89ed-f90bb2661195', embedding=None, metadata={'page_label': '303', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='refresh option simply involves the periodic replacement of complete data warehouse ta-\\nbles. But refresh jobs can take a long time to run. If you have to run refresh jobs everyday, you may have to keep the data warehouse down for unacceptably long times. The caseworsens if your database has large tables.\\nIs there some kind of a guideline as to when refresh is better than update or vice versa?\\nFigure 12-12 shows a graph comparing refresh with update. The cost of refresh remainsconstant irrespective of the number of changes in the source systems. If the number ofchanges increases, the time and effort for doing a full refresh remain the same. On the oth-er hand, the cost of update varies with the number of records to be updated.\\nIf the number of records to be updated falls between 15 and 25% of the total number of\\nrecords, the cost of loading per record tends to be the same whether you opt for a full re-fresh of the entire data warehouse or to do the updates. This range is just a general guide.If more than 25% of the source records change daily, then seriously consider full refresh-es. Generally, data warehouse administrators use the update process. Occasionally, youmay want redo the data warehouse with a full refresh when some major restructuring orsimilar mass changes take place. \\nProcedure for Dimension Tables\\nIn a data warehouse, dimension tables contain attributes that are used to analyze basic\\nmeasurements such as sales and costs. As you know very well, customer, product, time,and sales territory are examples of dimension tables. The procedure for maintaining thedimension tables includes two functions: first, the initial loading of the tables; thereafter,applying the changes on an ongoing basis. Let us consider two issues. \\nThe first one is about the keys of the records in the source systems and the keys of the\\nrecords in the data warehouse. For reasons discussed earlier, we do not use the productionDATA LOADING 283\\nUPDATE\\nREFRESH\\n% OF RECORDS CHANGEDCOST\\n15% to \\n25%After the initial load, the data warehouse is kept up-to-date by\\nREFRESH  - complete reload at specified intervals \\nUPDATE     - application of incremental changes \\nFigure 12-12 Refresh versus update.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8955ff2a-67e6-4b99-9888-8f64aa9b0987', embedding=None, metadata={'page_label': '304', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='system keys for the records in the data warehouse. In the data warehouse, you use system-\\ngenerated keys. The records in the source systems have their own keys. Therefore, beforesource data can be applied to the dimension tables, whether for the initial load or for on-going changes, the production keys must be converted to the system-generated keys in thedata warehouse. Y ou may do the key conversion as part of the transformation functions oryou may do it separately before the actual load functions. The separate key translation sys-tem is preferable.\\nThe next issue relates to the application of the Type 1, Type 2, and Type 3 dimension\\nchanges to the data warehouse. Figure 12-13 shows how these different types are handled.\\nFact Tables: History and Incremental Loads\\nThe key of the fact table is the concatenation of the keys of the dimension tables. There-\\nfore, for this reason, dimension records are loaded first. Then, before loading each facttable record, you have to create the concatenated key for the fact table record from thekeys of the corresponding dimension records. \\nHere are some tips for history loads of the fact tables:\\n/L50539Identify historical data useful and interesting for the data warehouse\\n/L50539Define and refine extract business rules\\n/L50539Capture audit statistics to tie back to operational systems\\n/L50539Perform fact table surrogate key look-up\\n/L50539Improve fact table content\\n/L50539Restructure the data\\n/L50539Prepare the load files284 DATA EXTRACTION, TRANSFORMATION, AND LOADING\\nSource System\\ndata changes\\nfor dimensionsTransform and prepare for\\nloading in Staging area\\nDetermine if rows exist for\\nthis surrogate key\\nMatch changed values with\\nexisting dimension valuesMatches\\nexactly? YESNO\\nACTION\\nMatches\\nexactly? NO\\nOverwrite\\nold valueCreate a new\\ndimension recordPush down changed value\\nto “old” attribute fieldTYPE 1 TYPE 2 TYPE 3PART OF\\nLOAD\\nPROCESS\\nFigure 12-13 Loading changes to dimension tables.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9c07e630-d8e7-432e-b94a-66e32102cf1f', embedding=None, metadata={'page_label': '305', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Given below are a few useful remarks about incremental loads for fact tables:\\n/L50539Incremental extracts for fact tables\\nConsist of new transactionsConsist of update transactionsUse database transaction logs for data capture\\n/L50539Incremental loads for fact tables\\nLoad as frequently as feasibleUse partitioned files and indexesApply parallel processing techniques\\nETL SUMMARY\\nBy now you should be fully convinced that the data extraction, transformation, and load-\\ning functions for a data warehouse cover very wide ground. The conversion functions nor-mally associated with the development of any operational system bear no comparison tothe extent and complexity of the ETL functions in a data warehouse environment. Thedata extraction function in a data warehouse spans several, varied source systems. As adata warehouse developer, you need to carefully examine the challenges the variety ofyour source systems pose and find appropriate data extraction methods. We have dis-cussed most of the common methods. Data extraction for a data warehouse is not a one-time event; it is an ongoing function carried out at very frequent intervals.\\nThere are many types of data transformation in a data warehouse with many different\\ntasks. It is not just a field-to-field conversion. In our discussion, we considered manycommon types of data transformation. The list of types we were able to consider is by nomeans exhaustive or complete. In your data warehouse environment, you will come acrossadditional types of data transformation.\\nWhat about the data loads in a data warehouse in comparison with the loads for a\\nnew operational system? For the implementation of a new operational system, you con-vert and load the data once to get the new system started. Loading of data in a datawarehouse does not cease with the initial implementation. Just like extraction and trans-formation, data loading is not just an initial activity to get the data warehouse started.Apart from the initial data load, you have the ongoing incremental data loads and the pe-riodic full refreshes.\\nFortunately, many vendors have developed powerful tools for data extraction, data\\ntransformation, and data loading. Y ou are no longer left to yourself to handle these chal-lenges with unsophisticated manual methods. Y ou have flexible and suitable vendor solu-tions. Vendor tools cover a wide range of functional options. Y ou have effective tools toperform functions in every part of the ETL process. Tools can extract data from multiplesources, perform scores of transformation functions, and do mass loads as well as incre-mental loads. Let us review some of the options you have with regard to ETL tools.\\nETL Tools Options\\nVendors have approached the challenges of ETL and addressed them by providing tools\\nfalling into the following three broad functional categories:ETL SUMMARY 285', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0348faa2-e616-4e9a-bdad-540dd10987e0', embedding=None, metadata={'page_label': '306', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1.Data transformation engines. These consist of dynamic and sophisticated data ma-\\nnipulation algorithms. The tool suite captures data from a designated set of sourcesystems at user-defined intervals, performs elaborate data transformations, sends theresults to a target environment, and applies the data to target files. These tools pro-vide you with maximum flexibility for pointing to various source systems, to selectthe appropriate data transformation methods, and to apply full loads and incrementalloads. The functionality of these tools sweeps the full range of the ETL process.\\n2.Data capture through replication. Most of these tools use the transaction recov-\\nery logs maintained by the DBMS. The changes to the source systems captured inthe transaction logs are replicated in near real time to the data staging area for fur-ther processing. Some of the tools provide the ability to replicate data through theuse of database triggers. These specialized stored procedures in the database signalthe replication agent to capture and transport the changes. \\n3.Code generators. These are tools that directly deal with the extraction, transforma-\\ntion, and loading of data. The tools enable the process by generating program codeto perform these functions. Code generators create 3GL/4GL data extraction andtransformation programs. Y ou provide the parameters of the data sources and thetarget layouts along with the business rules. The tools generate most of the programcode in some of the common programming languages. When you want to add morecode to handle the types of transformation not covered by the tool, you may do sowith your own program code. The code automatically generated by the tool has ex-its at which points you may add your code to handle special conditions. \\nMore specifically, what can the ETL tools do? Review the following list and as you\\nread each item consider if you need that feature in the ETL tool for your environment:\\n/L50539Data extraction from various relational databases of leading vendors\\n/L50539Data extraction from old legacy databases, indexed files, and flat files\\n/L50539Data transformation from one format to another with variations in source and target\\nfields\\n/L50539Performing of standard conversions, key reformatting, and structural changes\\n/L50539Provision of audit trails from source to target\\n/L50539Application of business rules for extraction and transformation\\n/L50539Combining of several records from the source systems into one integrated target\\nrecord\\n/L50539Recording and management of metadata\\nReemphasizing ETL Metadata\\nChapter 9 covered data warehouse metadata in great detail. We discussed the role and im-\\nportance of metadata in the three major functional areas of the data warehouse. We re-viewed the capture and use of metadata in the three areas of data acquisition, data storage,and information delivery. Metadata in data acquisition and data storage relate to the ETLfunctions.\\nWhen you use vendor tools for performing part or all of the ETL functions, most of\\nthese tools record and manage their own metadata. Even though the metadata is in theproprietary formats of the tools, it is usable and available. ETL metadata contains infor-286 DATA EXTRACTION, TRANSFORMATION, AND LOADING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2b534b8d-54e0-446f-9683-fbda059ba2b4', embedding=None, metadata={'page_label': '307', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='mation about the source systems, mappings between source data and data warehouse tar-\\nget data structures, data transformations, and data loading. \\nBut as you know, your selected tools may not exclusively perform all of the ETL func-\\ntions. Y ou will have to augment your ETL effort with in-house programs. In each of thedata extraction, data transformation, and data loading functions, you may use programswritten by the project team. Depending on the situation in your environment, these in-house programs may vary considerably in number. Although in-house programs give youmore control and flexibility, there is one drawback. Unlike using ETL tools, in-house pro-grams do not record or manage metadata. Y ou have to make a special effort to deal withmetadata. Although we have reviewed metadata extensively in the earlier chapter, we wantto reiterate that you need to pay special attention and ensure that metadata is not over-looked when you use in-house programs for ETL functions. All the business rules, sourcedata information, source-to-target mappings, transformation, and loading informationmust be recorded manually in the metadata directory. This is extremely important to makeyour metadata component complete and accurate.\\nETL Summary and Approach\\nLet us summarize the functions covered in this chapter with Figure 12-14. Look at the fig-\\nure and do a quick review of the major functions. \\nWhat do you think of the size of this chapter and the topics covered? If nothing else, the\\nlength of the chapter alone highlights the importance and complexity of the data extraction,transformation, and loading functions. Why so? Again and again, the variety and heteroge-neous nature of the source systems comes to the forefront as the pressing reason to pay spe-ETL SUMMARY 287\\nDATA EXTRACTION\\nExtraction from \\nheterogeneous source systems \\nand outside sources.DATA TRANSFORMATION\\nConversion and restructuring \\naccording to transformation \\nrules.\\nDATA INTEGRATION\\nCombining all related data \\nfrom various sources based \\non source-to-target mapping.\\nDATA SUMMARIZATION\\nCreating aggregate datasets \\nbased on predefined \\nprocedures.\\nMETADATA UPDATES\\nMaintain and use metadata for \\nExtraction, Transformation, and \\nLoad functions.DATA CLEANSING\\nScrubbing and enriching \\naccording to cleansing          \\nrules.\\nINITIAL DATA LOADING\\nApply initial data in large \\nvolumes to the         \\nwarehouse.\\nONGOING LOADING\\nApply ongoing incremental \\nloads and periodic refreshes to \\nthe warehouse.\\nFigure 12-14 ETL summary.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1c3ad08d-9f25-4e38-b6dc-87a63085ae79', embedding=None, metadata={'page_label': '308', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='cial attention to ETL. For one thing, the variety and heterogeneity add to the challenge of\\ndata extraction. But when you consider the number of different source systems, the morethere are, the more intense and complex will the transformation functions be. More incon-sistencies are likely to be present and more variations from standards are expected. \\nNevertheless, what is required is a systematic and practical approach. Whenever you can\\nbreak down a task into two, do so without hesitation. For example, look for ways to breakdown the initial load into several subloads. Additionally, detailed analysis is crucial. Y oucannot take any source system lightly. Every source system may pose its own challenges.Get down to the details. Spend enough time in the source-to-target mappings. Make an ini-tial list of data transformations and let this list evolve. Do more analysis and add to the list. \\nY ou have to live with data loads every day. Frequent incremental loads are absolutely es-\\nsential to keep your data warehouse up-to-date. Try to automate as much of incrementalloading as possible. Keep in-house programming down to a reasonable level. Manual main-tenance of metadata could impose a large burden. We realize ETL functions are time-con-suming, complex, and arduous; nevertheless, they are very important. Any flaws in the ETLprocess show up in the data warehouse. Y our users will end up using faulty information.What kind of decisions do you think they will make with incorrect and incomplete infor-mation?\\nCHAPTER SUMMARY\\n/L50539ETL functions in a data warehouse are most important, challenging, time-consum-\\ning, and labor-intensive.\\n/L50539Data extraction is complex because of the disparate source systems; data transfor-\\nmation is difficult because of the wide range of tasks; data loading is challengingbecause of the volume of data. \\n/L50539Several data extraction techniques are available, each with its advantages and disad-\\nvantages. Choose the right technique based on the conditions in your environment. \\n/L50539The data transformation function encompasses data conversion, cleansing, consoli-\\ndation, and integration. Implement the transformation function using a combinationof specialized tools and in-house developed software.\\n/L50539The data loading function relates to the initial load, regular periodic incremental\\nloads, and full refreshes from time to time. Four methods to apply data are: load, ap-pend, destructive merge, and constructive merge.\\n/L50539Tools for ETL functions fall into three broad functional categories: data transforma-\\ntion engines, data capture through replication, and code generators \\nREVIEW QUESTIONS\\n1. Give three reasons why you think ETL functions are most challenging in a data\\nwarehouse environment.\\n2. Name any five types of activities that are part of the ETL process. Which of these\\nare time-consuming?\\n3. The tremendous diversity of the source systems is the primary reason for their\\ncomplexity. Do you agree? If so, explain briefly why.288 DATA EXTRACTION, TRANSFORMATION, AND LOADING', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fc787b88-149c-4b67-9261-9ac58df20bae', embedding=None, metadata={'page_label': '309', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. What are the two general categories of data stored in source operational systems?\\nGive two examples for each.\\n5. Name five types of the major transformation tasks. Give an example for each.6. Describe briefly the entity identification problem in data integration and consoli-\\ndation. How do you resolve this problem?\\n7. What is key restructuring? Explain why it is needed.8. Define initial load, incremental load, and full refresh.9. Explain the difference between destructive merge and constructive merge for ap-\\nplying data to the data warehouse repository. When do you use these modes?\\n10. When is a full data refresh preferable to an incremental load? Can you think of an\\nexample?\\nEXERCISES\\n1. Match the columns:\\n1. destructive merge A. use static data capture\\n2. full refresh B. EBCDIC to ASCII3. character set conversion C. technique of last resort4. derived value D. overwrite old value5. immediate data extract E. new record supercedes6. initial load F . average daily balance7. file comparison method G. complete reload8. data enrichment H make data more useful 9. Type 1 dimension changes I. create extraction program\\n10. code generator J. real-time data capture\\n2. As the ETL expert on the data warehouse project team for a telecommunications\\ncompany, write a memo to your project leader describing the types of challenges inyour environment, and suggest some practical steps to meet the challenges.\\n3. Y our project team has decided to use the system logs for capturing the updates from\\nthe source operational systems. Y ou have to extract data for the incremental loadsfrom four operational systems all running on relational databases. These are fourtypes of sales applications. Y ou need data to update the sales data in the data ware-house. Make assumptions and describe the data extraction process. \\n4. In your organization, assume that customer names and addresses are maintained in\\nthree customer files supporting three different source operational systems. Describethe possible entity identification problem you are likely to face when you consoli-date the customer records from the three files. Write a procedure outlining how youpropose to resolve the problem.\\n5. Y ou are the staging area expert in the data warehouse project team for a large toy\\nmanufacturer. Discuss the four modes of applying data to the data warehouse. Se-lect the modes you want to use for your data warehouse and explain the reasons foryour selection.EXERCISES 289', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9c3e05aa-fd5d-46a0-a245-39f242bafe05', embedding=None, metadata={'page_label': '310', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 13\\nDATA QUALITY: A KEY TO SUCCESS \\nCHAPTER OBJECTIVES\\n/L50539Clearly understand why data quality is critical in a data warehouse\\n/L50539Observe the challenges posed by corrupt data and learn the methods to deal with\\nthem\\n/L50539Appreciate the benefits of quality data\\n/L50539Review the various categories of data quality tools and examine their usage\\n/L50539Study the implications of a data quality initiative and learn practical tips on data\\nquality\\nImagine a small error, seemingly inconsequential, creeping into one of your opera-\\ntional systems. While collecting data in that operational system about customers, let ussay the user consistently entered erroneous region codes. The sales region codes of thecustomers are all messed up, but in the operational system, the accuracy of the regioncodes may not be that important because no invoices to the customers are going to bemailed out using region codes. These region codes were entered for marketing purposes. \\nNow take the customer data to the next step and move it into the data warehouse. What\\nis the consequence of this error? All analyses performed by your data warehouse usersbased on region codes will result in serious misrepresentation. An error that seems to beso irrelevant in the operational systems can cause gross distortion in the results from thedata warehouse. This example may not appear to be the true state of affairs in many datawarehouses, but you will be surprised to learn that these kinds of problems are common.Poor data quality in the source systems results in poor decisions by the users of the datawarehouse.\\nDirty data is among the top reasons for failure of a data warehouse. As soon as the\\nusers sense that the data is of unacceptable quality, they lose their confidence in the datawarehouse. They will flee from the data warehouse in droves and all the effort of the\\n291Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bc2ba808-3929-4508-9ed8-e862d61c587a', embedding=None, metadata={'page_label': '311', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='project team will be down the drain. It will be impossible to get back the trust of the\\nusers.\\nMost companies overestimate the quality of the data in their operational systems. Very\\nfew have procedures and systems in place to verify the quality of data in their various op-erational systems. As long as the quality of the data is acceptable enough to perform thefunctions of the operational systems, then the general conclusion is that all of the enter-prise data is good. For some companies building data warehouses, data quality is not ahigher priority. These companies suspect that there may be a problem, but that it is not sopressing as to demand immediate attention. \\nOnly when companies make an effort to ascertain the quality of their data are they\\namazed at the extent of data corruption. Even when companies discover a high level ofdata pollution, they tend to underestimate the effort needed to cleanse the data. They donot allocate sufficient time and resources for the clean-up effort. At best, the problem isaddressed partially. \\nIf your enterprise has several disparate legacy systems from which your data ware-\\nhouse must draw its data, start with the assumption that your source data is likely to becorrupt. Then ascertain the level of the data corruption. The project team must allowenough time and effort and have a plan for correcting the polluted data. In this chapter, wewill define data quality in the context of the data warehouse. We will consider the com-mon types of data quality problems so that when you analyze your source data, you canidentify the types and deal with them. We will explore the methods for data cleansing andalso review the features of the tools available to assist the project team in this crucial un-dertaking.\\nWHY IS DATA QUALITY CRITICAL?\\nData quality in a data warehouse is critical (this sounds so obvious and axiomatic), more\\nso than in an operational system. Strategic decisions made on the basis of informationfrom the data warehouse are likely to be more far-reaching in scope and consequences.Let us list some reasons why data quality is critical. Please examine the following obser-vations. Improved data quality:\\n/L50539boosts confidence in decision making,\\n/L50539enables better customer service,\\n/L50539increases opportunity to add better value to the services,\\n/L50539reduces risk from disastrous decisions,\\n/L50539reduces costs, especially of marketing campaigns,\\n/L50539enhances strategic decision making,\\n/L50539improves productivity by streamlining processes, and\\n/L50539avoids compounding effects of data contamination.\\nWhat is Data Quality?\\nAs an IT professional, you have heard of data accuracy quite often. Accuracy is associated\\nwith a data element. Consider an entity such as customer. The customer entity has attrib-utes such as customer name, customer address, customer state, customer lifestyle, and so292 DATA QUALITY: A KEY TO SUCCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='79310c9a-9b9c-482b-aec4-9a857f5495fb', embedding=None, metadata={'page_label': '312', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='on. Each occurrence of the customer entity refers to a single customer. Data accuracy, as it\\nrelates to the attributes of the customer entity, means that the values of the attributes of asingle occurrence accurately describes the particular customer. The value of the customername for a single occurrence of the customer entity is actually the name of that customer.Data quality implies data accuracy, but it is much more than that. Most cleansing opera-tions concentrate on just data accuracy. Y ou need to go beyond data accuracy.\\nIf the data is fit for the purpose for which it is intended, we can then say such data has\\nquality. Therefore, data quality is to be related to the usage for the data item as defined bythe users. Does the data item in an entity reflect exactly what the user is expecting to ob-serve? Does the data item possess fitness of purpose as defined by the users? If it does,the data item conforms to the standards of data quality. Please scrutinize Figure 13-1. Thisfigure brings out the distinction between data accuracy and data quality.\\nWhat is considered to be data quality in operational systems? If the database records\\nconform to the field validation edits, then we generally say that the database records are ofgood data quality. But such single field edits alone do not constitute data quality.\\nData quality in a data warehouse is not just the quality of individual data items but the\\nquality of the full, integrated system as a whole. It is more than the data edits on individ-ual fields. For example, while entering data about the customers in an order entry applica-tion, you may also collect the demographics of each customer. The customer demograph-ics are not germane to the order entry application and, therefore, they are not given toomuch attention. But you run into problems when you try to access the customer demo-graphics in the data warehouse. The customer data as an integrated whole lacks data qual-ity. WHY IS DATA QUALITY CRITICAL? 293\\nDATA  INTEGRITY DATA  QUALITY\\nSpecific instance of an entity accurately \\nrepresents that occurrence of the entity.\\nData element defined in terms of \\ndatabase technology.\\nData element conforms to validation \\nconstraints.\\nIndividual data items have the correct \\ndata types.\\nTraditionally relates to operational \\nsystems.The data item is exactly fit for the \\npurpose for which the business users have defined it. \\nWider concept grounded in the specific \\nbusiness of the company.\\nRelates not just to single data elements \\nbut to the system as a whole.\\nForm and content of data elements \\nconsistent across the whole system.\\nEssentially needed in a corporate-wide \\ndata warehouse for business users.\\nFigure 13-1 Data accuracy versus data quality.DATA ACCURACY DATA QUALITY', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='020d8dbd-f0af-4769-a640-53ce195e9292', embedding=None, metadata={'page_label': '313', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is just a clarification of the distinction between data accuracy and data quality.\\nBut how can you specifically define data quality? Can you know intuitively whether adata element is of high quality or not by examining it? If so, what kind of examination doyou conduct, and how do you examine the data? As IT professionals, having worked withdata in some capacity, we have a sense of what corrupt data is and how to tell whether adata element is of high data quality or not. But a vague concept of data quality is not ade-quate to deal with data corruption effectively. So let us get into some concrete ways ofrecognizing data quality in the data warehouse.\\nThe following list is a survey of the characteristics or indicators of high-quality data.\\nWe will start with data accuracy, as discussed earlier. Study each of these data quality di-mensions and use the list to recognize and measure the data quality in the systems thatfeed your data warehouse.\\nAccuracy. The value stored in the system for a data element is the right value for that\\noccurrence of the data element. If you have a customer name and an address storedin a record, then the address is the correct address for the customer with that name.If you find the quantity ordered as 1000 units in the record for order number12345678, then that quantity is the accurate quantity for that order.\\nDomain Integrity. The data value of an attribute falls in the range of allowable, de-\\nfined values. The common example is the allowable values being “male” and “fe-male” for the gender data element. \\nData Type. Value for a data attribute is actually stored as the data type defined for that\\nattribute. When the data type of the store name field is defined as “text,” all in-stances of that field contain the store name shown in textual format and not numer-ic codes.\\nConsistency. The form and content of a data field is the same across multiple source\\nsystems. If the product code for product ABC in one system is 1234, then the codefor this product must be 1234 in every source system.\\nRedundancy. The same data must not be stored in more than one place in a system. If,\\nfor reasons of efficiency, a data element is intentionally stored in more than oneplace in a system, then the redundancy must be clearly identified.\\nCompleteness. There are no missing values for a given attribute in the system. For ex-\\nample, in a customer file, there is a valid value for the “state” field. In the file fororder details, every detail record for an order is completely filled. \\nDuplication. Duplication of records in a system is completely resolved. If the product\\nfile is known to have duplicate records, then all the duplicate records for each prod-uct are identified and a cross-reference created.\\nConformance to Business Rules. The values of each data item adhere to prescribed\\nbusiness rules. In an auction system, the hammer or sale price cannot be less thanthe reserve price. In a bank loan system, the loan balance must always be positive orzero.\\nStructural Definiteness. Wherever a data item can naturally be structured into indi-\\nvidual components, the item must contain this well-defined structure. For example,an individual’ s name naturally divides into first name, middle initial, and last name.Values for names of individuals must be stored as first name, middle initial, and lastname. This characteristic of data quality simplifies enforcement of standards and re-duces missing values.294 DATA QUALITY: A KEY TO SUCCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c9b61527-6a08-4630-8565-860874bf3795', embedding=None, metadata={'page_label': '314', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Data Anomaly. A field must be used only for the purpose for which it is defined. If the\\nfield Address-3 is defined for any possible third line of address for long addresses,then this field must be used only for recording the third line of address. It must notbe used for entering a phone or fax number for the customer.\\nClarity. A data element may possess all the other characteristics of quality data but if\\nthe users do not understand its meaning clearly, then the data element is of no valueto the users. Proper naming conventions help to make the data elements well under-stood by the users.\\nTimely. The users determine the timeliness of the data. If the users expect customer di-\\nmension data not to be older than one day, the changes to customer data in thesource systems must be applied to the data warehouse daily.\\nUsefulness. Every data element in the data warehouse must satisfy some requirements\\nof the collection of users. A data element may be accurate and of high quality, but ifit is of no value to the users, then it is totally unnecessary for that data element to bein the data warehouse.\\nAdherence to Data Integrity Rules. The data stored in the relational databases of the\\nsource systems must adhere to entity integrity and referential integrity rules. Anytable that permits null as the primary key does not have entity integrity. Referentialintegrity forces the establishment of the parent–child relationships correctly. In acustomer-to-order relationship, referential integrity ensures the existence of a cus-tomer for every order in the database.\\nBenefits of Improved Data Quality\\nEveryone generally understands that improved data quality is a critical goal, especially in\\na data warehouse. Bad data leads to bad decisions. At this stage, let us review some spe-cific areas where data quality yields definite benefits.\\nAnalysis with Timely Information. Suppose a large retail chain is running daily\\npromotions of many types in most of its 200 stores in the country. This is a major season-al campaign. Promotion is one of the dimensions stored in the data warehouse. The mar-keting department wants to run various analyses using promotion as the primary dimen-sion to monitor and tune the promotions as the season progresses. It is critical for thedepartment to perform the analyses every day. Suppose the promotion details are fed intothe data warehouse only once a week. Do you think the promotional data is timely for themarketing department? Of course not. Is the promotional data in the data warehouse ofhigh quality for the data warehouse users? Not according to the characteristics of qualitydata listed in the previous section. Quality data produces timely information, a significantbenefit for the users.\\nBetter Customer Service. The benefit of accurate and complete information for\\ncustomer service cannot be overemphasized. Let us say the customer service representa-tive at a large bank receives a call. The customer at the other end of the line wants to talkabout the service charge on his checking account. The bank customer service representa-tive notices a balance of $27.38 in the customer’ s checking account. Why is he making abig fuss about the service charge with almost nothing in the account? But let us say thecustomer service representative clicks on the customer’ s other accounts and finds that theWHY IS DATA QUALITY CRITICAL? 295', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b1d15908-5c04-44bc-bce7-86bf2210ef0b', embedding=None, metadata={'page_label': '315', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='customer has $35,000 in his savings accounts and CDs worth more than $120,000. How\\ndo you think the customer service representative will answer the call? With respect, ofcourse. Complete and accurate information improves customer service tremendously. \\nNewer Opportunities. Quality data in a data warehouse is a great boon for market-\\ning. It opens the doors to immense opportunities to cross-sell across product lines and de-partments. The users can select the buyers of one product and determine all the otherproducts that are likely to be purchased by them. Marketing departments can conductwell-targeted campaigns. This is just one example of the numerous opportunities that aremade possible by quality data. On the other hand, if the data is of inferior quality, the cam-paigns will be failures. \\nReduced Costs and Risks. What are some of the risks of poor data quality? The ob-\\nvious risk is strategic decisions that could lead to disastrous consequences. Other risks in-clude wasted time, malfunction of processes and systems, and sometimes even legal ac-tion by customers and business partners. One area where quality data reduces costs is inmailings to customers, especially in marketing campaigns. If the addresses are incom-plete, inaccurate, or duplicate, most of the mailings are wasted. \\nImproved Productivity. Users get an enterprise-wide view of information from the\\ndata warehouse. This is a primary goal of the data warehouse. In areas where a corporate-wide view of information naturally enables the streamlining of processes and operations,you will see productivity gains. For example, a company-wide view of purchasing pat-terns in a large department store can result in better purchasing procedures and strategies.\\nReliable Strategic Decision Making. This point is worth repeating. If the data in\\nthe warehouse is reliable and of high quality, then decisions based on the information willbe sound. No data warehouse can add value to a business until the data is clean and ofhigh quality.\\nTypes of Data Quality Problems\\nAs part of the discussion on why data quality is critical in the data warehouse, we have ex-\\nplored the characteristics of quality data. The characteristics themselves have demonstrat-ed the critical need for quality data. The discussion of the benefits of having quality datafurther strengthens the argument for cleaner data. Our discussion of the critical need forquality data is not complete until we quickly walk through the types of problems you arelikely to encounter if the data is polluted. Description of the problem types will convinceyou even more that data quality is of supreme importance. \\nIf 4% of the sales amounts are wrong in the billing systems of a $2 billion company,\\nwhat is the estimated loss in revenue? $80 million. What happens when a large catalogsales company mails catalogs to customers and prospects? If there are duplicate recordsfor the same customer in the customer files, then, depending on how extensive the du-plication problem is, the company will end up sending multiple catalogs to the same per-son. \\nIn a recent independent survey, businesses with data warehouses were asked the ques-\\ntion: What is the biggest challenge in data warehouse development and usage? Please seeFigure 13-2 for the ranking of the answers. Nearly half of the respondents rated data qual-296 DATA QUALITY: A KEY TO SUCCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3c366d3e-d95d-43f5-9668-60feb1a62110', embedding=None, metadata={'page_label': '316', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ity as their biggest challenge. Data quality is the biggest challenge not just because of the\\ncomplexity and extent of the problem of data pollution. More far-reaching is the effect ofpolluted data on strategic decisions made based on such data.\\nMany of today’ s data warehouses get their data feed from old legacy systems. Data in\\nold systems undergo a decaying process. For example, consider the field for productcodes in a retail chain store. Over the past two decades, the products sold must havechanged many times and in many variations. The product codes must have been assignedand reassigned a number of times. The old codes must have decayed and perhaps some ofthe old codes could have been reassigned to newer products. This is not a problem in oper-ational systems because these systems deal with current data. The old codes would havebeen right at that time in the past when they were current. But data warehouse carries his-torical data and these old codes could cause problems in this repository.\\nLet us go over a list of explicit types of data quality problems. These are specific types\\nof data corruption. This list is by no means exhaustive, but will give you an appreciationof the need for data quality.\\nDummy values in fields. Are you aware of the practice of filling the Social Security\\nnumber field temporarily with nines to pass the numerical edits? The intention is toenter the correct Social Security number when the data becomes available. Manytimes the correction does not happen and you are left with the nines in that field.Sometimes you may enter 88888 in the Zip Code field to pass the edit for an Asiancustomer and 77777 for a European customer. \\nAbsence of data values. This is common in customer data. In operational systems,\\nusers are only concerned with the customer data that is needed to mail a billingWHY IS DATA QUALITY CRITICAL? 297\\n0% 50% 40% 30% 20% 10%\\nPercentage of RespondentsDATA WAREHOUSE\\nCHALLENGES\\nData QualityData ModelingUser ExpectationsData TransformationBusiness RulesManagement ExpectationsDatabase Performance\\nFigure 13-2 Data quality: the top challenge.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7d0afaf1-f6f9-4c26-a359-603165b07841', embedding=None, metadata={'page_label': '317', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='statement, to send a follow-up letter, and to call about an overdue balance. Not too\\nmuch attention is paid to demographic types of data that are not usable in opera-tional systems. So you are left with missing values in the demographic types of datathat are very useful for analysis from the data warehouse. Absence of data values isalso related to other types of data elements.\\nUnofficial use of fields. How many times have you asked your users to place their\\ncomments in the customer contact field because no field was provided for com-ments in the customer record? This is an unofficial use of the customer contactfield. \\nCryptic values. This is a prevalent problem in legacy systems, many of which were not\\ndesigned with end-users in mind. For example, the customer status codes couldhave been started with R = Regular and N = New. Then at one time, another code D= Deceased could have been added. Down the road, a further code A = Archivecould have been included. More recently, the original R and N could have been dis-carded and R = Remove could have been added. Although this example is contrivedto make a point, such cryptic and confusing values for attributes are not uncommonin old legacy systems.\\nContradicting values. There are related fields in the source systems in which the val-\\nues must be compatible. For example, the values in the fields for State and Zip Codemust agree. Y ou cannot have a State value of CA (California) and a Zip Code of08817 (a Zip Code in New Jersey) in the same client record.\\nViolation of business rules. In a personnel and payroll system, an obvious business\\nrule is that the days worked in a year plus the vacation days, holidays, and sick dayscannot exceed 365 or 366. Any employee record that comes up with the number ofdays more the 365 or 366 violates this basic business rule. In a bank loan system,the minimum interest rate cannot be more than the maximum rate for a variable rateloan. \\nReused primary keys. Suppose a legacy system has a 5-digit primary key field as-\\nsigned for the customer record. This field will be adequate as long as the number ofcustomers is less than 100,000. When the number of customers increases, somecompanies resolve the problem by archiving the older customer records and reas-signing the key values so that the newer customers are assigned primary key valuesrestarting with 1. This is not really a problem in the operational systems, but in thedata warehouse, where you capture both present data from the current customer fileand the past data from the archived customer file, you have a problem of duplicationof the reused primary key values.\\nNonunique identifiers. There is a different complication with identifiers. Suppose the\\naccounting systems have their own product codes used as identifiers but they aredifferent from the product codes used in the sales and inventory systems. ProductCode 355 in the sales system may be identified as Product Code A226 in the ac-counting system. Here a unique identifier does not represent the same product intwo different systems. \\nInconsistent values. Codes for policy type in different legacy systems in an expanding\\ninsurance company could have inconsistent values such as A = Auto, H = Home, F= Flood, W = Workers Comp in one system, and 1, 2, 3, and 4, respectively in an-other system. Another variation of these codes could be AU, HO, FL, and WO, re-spectively. 298 DATA QUALITY: A KEY TO SUCCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c6bb6214-9d15-4438-9c1a-a4bbd5f45231', embedding=None, metadata={'page_label': '318', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Incorrect values. Product Code: 146, Product Name: Crystal Vase, and Height: 486\\ninches in the same record point to some sort of data inaccuracy. The values forproduct name and height are not compatible. Perhaps the product code is also in-correct. \\nMultipurpose fields. Same data value in a field entered by different departments may\\nmean different things. A field could start off as a storage area code to indicate thebackroom storage areas in stores. Later, when the company built its own warehouseto store products, it used the same field to indicate the warehouse. This type ofproblem is perpetuated because store codes and warehouse codes were residing inthe same field. Warehouse codes went into the same field by redefining the storecode field. This type of data pollution is hard to correct.\\nErroneous integration. In an auction company, buyers are the customers who bid at\\nauctions and buy the items that are auctioned off. The sellers are the customers whosell their goods through the auction company. The same customer may be a buyer inthe auction system and a seller in the property receipting system. Assume that cus-tomer number 12345 in the auction system is the same customer whose number is34567 in the property receipting system. The data for customer number 12345 inthe auction system must be integrated with the data for customer number 34567 inthe property receipting system. The reverse side of the data integration problem isthis: customer number 55555 in the auction system and customer number 55555 inthe property receipting system are not the same customer but are different. These in-tegration problems arise because, typically, each legacy system had been developedin isolation at different times in the past. \\nDATA QUALITY CHALLENGES\\nThere is an interesting but strange aspect of the whole data cleansing initiative for the data\\nwarehouse. We are striving toward having clean data in the data warehouse. We want toascertain the extent of the pollution. Based on the condition of the data, we plan datacleansing activities. What is strange about this whole set of circumstances is that the pol-lution of data occurs outside the data warehouse. As part of the data warehouse projectteam, you are taking measures to eliminate the corruption that arises in a place outsideyour control. \\nAll data warehouses need historical data. A substantial part of the historical data comes\\nfrom antiquated legacy systems. Frequently, the end-users use the historical data in thedata warehouse for strategic decision making without knowing exactly what the datareally means. In most cases, detailed metadata hardly exists for the old legacy systems.Y ou are expected to fix the data pollution problems that emanate from the old operationalsystems without the assistance of adequate information about the data there. \\nSources of Data Pollution\\nIn order to come up with a good strategy for cleansing the data, it will be worthwhile to\\nreview a list of common sources of data pollution. Why does data get corrupted in thesource systems? Study the following list of data pollution sources against the backgroundof what data quality really is.DATA QUALITY CHALLENGES 299', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f731ea6b-d0d1-42bf-bf02-1b518a557639', embedding=None, metadata={'page_label': '319', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='System conversions. Trace the evolution of order processing in any company. The\\ncompany must have started with a file-oriented order entry system in the early1970s; orders were entered into flat files or indexed files. There was not much stockverification or customer credit verification during the entry of the order. Reportsand hard-copy printouts were used to continue with the process of executing the or-ders. Then this system must have been converted into an online order entry systemwith VSAM files and IBM’ s CICS as the online processing monitor. The next con-version must have been to a hierarchical database system. Perhaps that is whereyour order processing system still remains—as a legacy application. Many compa-nies have moved the system forward to a relational database application. In anycase, what has happened to the order data through all these conversions? Systemconversions and migrations are prominent reasons for data pollution. Try to under-stand the conversions gone through by each of your source systems. \\nData aging. We have already dealt with data aging when we reviewed how over the\\ncourse of many years the values in the product code fields could have decayed. Theolder values lose their meaning and significance. If many of your source systemsare old legacy systems, pay special attention to the possibility of aged data in thosesystems. \\nHeterogeneous system integration. The more heterogeneous and disparate your\\nsource systems are, the stronger is the possibility of corrupted data. In such a sce-nario, data inconsistency is a common problem. Consider the sources for each ofyour dimension tables and the fact table. If the sources for one table are several het-erogeneous systems, be cautious about the quality of data coming into the datawarehouse from these systems. \\nPoor database design. Good database design based on sound principles reduces the\\nintroduction of errors. DBMSs provide for field editing. RDBMSs enable verifica-tion of the conformance to business rules through triggers and stored procedures.Adhering to entity integrity and referential integrity rules prevents some kinds ofdata pollution.\\nIncomplete information at data entry. At the time of the initial data entry about an\\nentity, if all the information is not available, two types of data pollution usually oc-cur. First, some of the input fields are not completed at the time of initial data entry.The result is missing values. Second, if the unavailable data is mandatory at the timeof the initial data entry, then the person entering the data tries to force generic val-ues into the mandatory fields. Entering N/A for not available in the field for city isan example of this kind of data pollution. Similarly, entry of all nines in the SocialSecurity number field is data pollution. \\nInput errors. In olden days when data entry clerks entered data into computer sys-\\ntems, there was a second step of data verification. After the data entry clerk finisheda batch, the entries from the batch were independently verified by another person.Now, users who are also responsible for the business processes enter the data. Dataentry is not their primary vocation. Data accuracy is supposed to be ensured bysight verification and data edits planted on the input screens. Erroneous entry ofdata is a major source of data corruption.\\nInternationalization/localization. Because of changing business conditions, the\\nstructure of the business gets expanded into the international arena. The companymoves into wider geographic areas and newer cultures. As a company is internation-300 DATA QUALITY: A KEY TO SUCCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b4b58dcb-f57a-4add-8c66-449a72e739c7', embedding=None, metadata={'page_label': '320', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='alized, what happens to the data in the source systems? The existing data elements\\nmust adapt to newer and different values. Similarly, when a company wants to con-centrate on a smaller area and localize its operations, some of the values for the dataelements get discarded. This change in the company structure and the resulting revi-sions in the source systems are sources of data pollution. \\nFraud. Do not be surprised to learn that deliberate attempts to enter incorrect data are\\nnot uncommon. Here, the incorrect data entries are actually falsifications to commitfraud. Look out for monetary fields and fields containing units of products. Makesure that the source systems are fortified with tight edits for such fields. \\nLack of policies. In any enterprise, data quality does not just materialize by itself. Pre-\\nvention of entry of corrupt data and preservation of data quality in the source sys-tems are deliberate activities. An enterprise without explicit policies on data qualitycannot be expected to have adequate levels of data quality.\\nValidation of Names and Addresses\\nAlmost every company suffers from the problem of duplication of names and addresses.\\nFor a single person, multiple records can exist among the various source systems. Evenwithin a single source system, multiple records can exist for one person. But in the datawarehouse, you need to consolidate all the activities of each person from the various du-plicate records that exist for that person in the multiple source systems. This type of prob-lem occurs whenever you deal with people, whether they are customers, employees,physicians, or suppliers. \\nTake the specific example of an auction company. Consider the different types of cus-\\ntomers and the different purposes for which the customers seek the services of the auctioncompany. Customers bring property items for sale, buy at auctions, subscribe to the cata-logs for the various categories of auctions, and bring articles to be appraised by expertsfor insurance purposes and for estate dissolution. It is likely that there are different legacysystems at an auction house to service the customers in these different areas. One cus-tomer may come for all of these services and a record gets created for the customer ineach of the different systems. A customer usually comes for the same service many times.On some of these occasions, it is likely that duplicate records are created for the same cus-tomer in one system. Entry of customer data happens at different points of contact of thecustomer with the auction company. If it is an international auction company, entry of cus-tomer data happens at many auction sites worldwide. Can you imagine the possibility forduplication of customer records and the extent of this form of data corruption? \\nName and address data is captured in two ways (see Figure 13-3). If the data entry is in\\nthe multiple field format, then it is easier to check for duplicates at the time of data entry.Here are a few inherent problems with entering names and addresses:\\n/L50539No unique key\\n/L50539Many names on one line\\n/L50539One name on two lines\\n/L50539Name and the address in a single line\\n/L50539Personal and company names mixed\\n/L50539Different addresses for the same person\\n/L50539Different names and spellings for the same customer DATA QUALITY CHALLENGES 301', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d77b2c6c-c737-4038-bfc8-3fafce0ea9df', embedding=None, metadata={'page_label': '321', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Before attempting to deduplicate the customer records, you need to go through a pre-\\nliminary step. First, you have to recast the name and address data into the multiple fieldformat. This is not easy, considering the numerous variations in the way name and addressare entered in free-form textual format. After this first step, you have to devise matchingalgorithms to match the customer records and find the duplicates. Fortunately, many goodtools are available to assist you in the deduplication process.\\nCosts of Poor Data Quality\\nCleansing the data and improving the quality of data takes money and effort. Although\\ndata cleansing is extremely important, you could justify the expenditure of money and ef-fort by counting the costs of not having or using quality data. Y ou can produce estimateswith the help of the users. They are the ones who can really do estimates because the esti-mates are based on forecasts of lost opportunities and possible bad decisions. \\nThe following is a list of categories for which cost estimates can be made. These are\\nbroad categories. Y ou will have to get into the details for estimating the risks and costs foreach category.\\n/L50539Bad decisions based on routine analysis\\n/L50539Lost business opportunities because of unavailable or “dirty” data\\n/L50539Strain and overhead on source systems because of corrupt data causing reruns\\n/L50539Fines from governmental agencies for noncompliance or violation of regulations\\n/L50539Resolution of audit problems302 DATA QUALITY: A KEY TO SUCCESS\\nName & Address: Dr. Jay  A. Harreld,  P.O. Box 999,\\n100 Main Street,Anytown, NX 12345, U.S.A.\\nTitle: Dr.\\nFirst Name: Jay\\nMiddle Initial: A.\\nLast Name: Harreld\\nStreet Address-1: P.O. Box 999\\nStreet Address-2: 100 Main Street\\nCity: Anytown\\nState: NX\\nZip: 12345\\nCountry Code: U.S.A. SINGLE FIELD FORMAT\\nMULTIPLE FIELD FORMAT\\nFigure 13-3 Data entry: name and address formats.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d6963af2-90c3-48e2-bf15-3633ad5a3eb8', embedding=None, metadata={'page_label': '322', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Redundant data unnecessarily using up resources \\n/L50539Inconsistent reports\\n/L50539Time and effort for correcting data every time data corruption is discovered\\nDATA QUALITY TOOLS\\nBased on our discussions in this chapter so far, you are at a point where you are convinced\\nabout the seriousness of data quality in the data warehouse. Companies have begun to rec-ognize dirty data as one of the most challenging problems in a data warehouse. \\nY ou would, therefore, imagine that companies must be investing heavily in data clean-\\nup operations. But according to experts, data cleansing is still not a very high priority forcompanies. This attitude is changing as useful data quality tools arrive on the market. Y oumay choose to apply these tools to the source systems, in the staging area before the loadimages are created, or to the load images themselves.\\nCategories of Data Cleansing Tools\\nGenerally, data cleansing tools assist the project team in two ways. Data error discovery\\ntools work on the source data to identify inaccuracies and inconsistencies. Data correctiontools help fix the corrupt data. These correction tools use a series of algorithms to parse,transform, match, consolidate, and correct the data.\\nAlthough data error discovery and data correction are two distinct parts of the data\\ncleansing process, most of the tools on the market do a bit of both. The tools have featuresand functions that identify and discover errors. The same tools can also perform the clean-ing up and correction of polluted data. In the following sections, we will examine the fea-tures of the two aspects of data cleansing as found in the available tools.\\nError Discovery Features\\nPlease study the following list of error discovery functions that data cleansing tools are\\ncapable of performing.\\n/L50539Quickly and easily identify duplicate records\\n/L50539Identify data items whose values are outside the range of legal domain values\\n/L50539Find inconsistent data\\n/L50539Check for range of allowable values \\n/L50539Detect inconsistencies among data items from different sources\\n/L50539Allow users to identify and quantify data quality problems\\n/L50539Monitor trends in data quality over time\\n/L50539Report to users on the quality of data used for analysis\\n/L50539Reconcile problems of RDBMS referential integrity\\nData Correction Features\\nThe following list describes the typical error correction functions that data cleansing tools\\nare capable of performing.DATA QUALITY TOOLS 303', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ec017b43-128a-45b8-a48b-debacd1461c7', embedding=None, metadata={'page_label': '323', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Normalize inconsistent data\\n/L50539Improve merging of data from dissimilar data sources\\n/L50539Group and relate customer records belonging to the same household\\n/L50539Provide measurements of data quality\\n/L50539Validate for allowable values \\nThe DBMS for Quality Control\\nThe database management system itself is used as a tool for data qualtiy control in many\\nways. Relational database management systems have many features beyond the databaseengine (see list below). Later versions of RDBMS can easily prevent several types of er-rors creeping into the data warehouse.\\nDomain integrity. Provide domain value edits. Prevent entry of data if the entered data\\nvalue is outside the defined limits of value. Y ou can define the edit checks while set-ting up the data dictionary entries.\\nUpdate security. Prevent unauthorized updates to the databases. This feature will stop\\nunauthorized users from updating data in an incorrect way. Casual and untrainedusers can introduce inaccurate or incorrect data if they are given authorization toupdate.\\nEntity integrity checking. Ensure that duplicate records with the same primary key\\nvalues are not entered. Also prevent duplicates based on values of other attributes.\\nMinimize missing values. Ensure that nulls are not allowed in mandatory fields.\\nReferential integrity checking. Ensure that relationships based on foreign keys are\\npreserved. Prevent deletion of related parent rows.\\nConformance to business rules. Use trigger programs and stored procedures to en-\\nforce business rules. These are special scripts compiled and stored in the databaseitself. Trigger programs are automatically fired when the designated data items areabout to be updated or deleted. Stored procedures may be coded to ensure that theentered data conforms to specific business rules. Stored procedures may be calledfrom application programs. \\nDATA QUALITY INITIATIVE\\nIn spite of the enormous importance of data quality, it seems as though many companies\\nstill ask the question whether to pay special attention to it and cleanse the data or not. Inmany instances, the data for the missing values of attributes cannot be recreated. In quite anumber of cases, the data values are so convoluted that the data cannot really be cleansed.A few other questions arise. Should the data be cleansed? If so, how much of it can reallybe cleansed? Which parts of the data deserve higher priority for applying data cleansingtechniques? The indifference and the resistance to data cleansing emerge from a few validfactors:\\n/L50539Data cleansing is tedious and time-consuming. The cleansing activity demands a\\ncombination of the usage of vendor tools, writing of in-house code, and arduous304 DATA QUALITY: A KEY TO SUCCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4681d116-3942-4f11-af69-895b5fdf546e', embedding=None, metadata={'page_label': '324', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='manual tasks of verification and examination. Many companies are unable to sus-\\ntain the effort. This is not the kind of work many IT professionals enjoy.\\n/L50539The metadata on many source systems may be missing or nonexistent. It will be dif-\\nficult or even impossible to probe into dirty data without the documentation.\\n/L50539The users who are asked to ensure data quality have many other business responsi-\\nbilities. Data quality probably receives the least attention.\\n/L50539Sometimes, the data cleansing activity appears to be so gigantic and overwhelming\\nthat companies are terrified of launching a data cleansing initiative.\\nOnce your enterprise decides to institute a data cleansing initiative, you may consider\\none of two approaches. Y ou may opt to let only clean data into your data warehouse. Thismeans only data with a 100% quality can be loaded into the data warehouse. Data that isin any way polluted must be cleansed before it can be loaded. This is an ideal approach,but it takes a while to detect incorrect data and even longer to fix it. This approach is ide-al from the point of view of data quality, but it will take a very long time before all data iscleaned up for data loading. \\nThe second approach is a “clean as you go” method. In this method, you load all the\\ndata “as is” into the data warehouse and perform data cleansing operations in the datawarehouse at a later time. Although you do not withhold data loads, the results of anyquery are suspect until the data gets cleansed. Questionable data quality at any time leadsto losing user confidence that is extremely important for data warehouse success.\\nData Cleansing Decisions\\nBefore embarking on a data cleansing initiative, the project team, including the users,\\nhave to make a number of basic decisions. Data cleansing is not as simple as deciding tocleanse all data and to cleanse it now. Realize that absolute data quality is unrealistic inthe real world. Be practical and realistic. Go for the fitness-for-purpose principle. Deter-mine what the data is being used for and find the purpose. If the data from the warehousehas to provide exact sales dollars of the top twenty-five customers, then the quality of thisdata must be very high. If customer demographics are to be used to select prospects forthe next marketing campaign, the quality of this data may be at a lower level.\\nIn the final analysis, when it comes to data cleansing, you are faced with a few funda-\\nmental questions. Y ou have to make some basic decisions. In the following subsections,we present the basic questions that need to be asked and the basic decisions that need tobe made. \\nWhich Data to Cleanse. This is the root decision. First of all, you and your users\\nmust jointly work out the answer to this question. It must primarily be the users’ deci-sion. IT will help the users make the decision. Decide on the types of questions the datawarehouse is expected to answer. Find the source data needed for getting answers.Weigh the benefits of cleansing each piece of data. Determine how cleansing will helpand how leaving the dirty data in will affect any analysis made by the users in the datawarehouse. \\nThe cost of cleaning up all data in the data warehouse is enormous. Users usually un-\\nderstand this. They do not expect to see 100% data quality and will usually settle for ig-noring the cleansing of unimportant data as long as all the important data is cleaned up.DATA QUALITY INITIATIVE 305', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='67b726be-29bb-4d73-9567-2a7b6b0c4532', embedding=None, metadata={'page_label': '325', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='But be sure of getting the definitions of what is important or unimportant from the users\\nthemselves. \\nWhere to Cleanse. Data for your warehouse originates in the source operational sys-\\ntems, so does the data corruption. Then the extracted data moves into the staging area.From the staging area load images are loaded into the data warehouse. Therefore, theoret-ically, you may cleanse the data in any one of these areas. Y ou may apply data cleansingtechniques in the source systems, in the staging area, or perhaps even in the data ware-house. Y ou may also adopt a method that splits the overall data cleansing effort into partsthat can be applied in two of the areas, or even in all three areas.\\nY ou will find that cleansing the data after it has arrived in the data warehouse reposito-\\nry is impractical and results in undoing the effects of many of the processes for movingand loading the data. Typically, data is cleansed before it is stored in the data warehouse.So that leaves you with two areas where you can cleanse the data. \\nCleansing the data in the staging area is comparatively easy. Y ou have already resolved\\nall the data extraction problems. By the time data is received in the staging area, you arefully aware of the structure, content, and nature of the data. Although this seems to be thebest approach, there are a few drawbacks. Data pollution will keep flowing into the stag-ing area from the source systems. The source systems will continue to suffer from theconsequences of the data corruption. The costs of bad data in the source systems do notget reduced. Any reports produced from the same data from the source systems and fromthe data warehouse may not match and will cause confusion. \\nOn the other hand, if you attempt to cleanse the data in the source systems, you are tak-\\ning on a complex, expensive, and difficult task. Many legacy source systems do not haveproper documentation. Some may not even have the source code for the production pro-grams available for applying the corrections.\\nHow to Cleanse. Here the question is about the usage of vendor tools. Do you use\\nvendor tools by themselves for all of the data cleansing effort? If not, how much of in-house programming is needed for your environment? Many tools are available in the mar-ket for several types of data cleansing functions.\\nIf you decide to cleanse the data in the source systems, then you have to find the ap-\\npropriate tools that can be applied to source system files and formats. This may not beeasy if most of your source systems are fairly old. In that case, you have to fall back on in-house programs. \\nHow to Discover the Extent of Data Pollution. Before you can apply data cleans-\\ning techniques, you have to assess the extent of data pollution. This is a joint responsibili-ty shared among the users of operational systems, the potential users of the data ware-house, and IT. IT staff, supporting both the source systems and the data warehouse, have aspecial role in the discovery of the extent of data pollution. IT is responsible for installingthe data cleansing tools and training the users in using those tools. IT must augment theeffort with in-house programs. \\nIn an earlier section, we discussed the sources of data pollution. Reexamine these\\nsources. Make a list that reflects the sources of pollution found in your environment, thendetermine the extent of the data pollution with regard to each source of pollution. For ex-ample, in your case, data aging could be a source of pollution. If so, make a list of all theold legacy systems that serve as sources of data for your data warehouse. For the data at-306 DATA QUALITY: A KEY TO SUCCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5ea80bca-3f36-4594-80a5-c455d2fdef74', embedding=None, metadata={'page_label': '326', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='tributes that are extracted, examine the sets of values. Check if any of these values do not\\nmake sense and have decayed. Similarly, perform detailed analysis for each type of datapollution source.\\nPlease look at Figure 13-4. In this figure, you find a few typical ways you can detect\\nthe possible presence and extent of data pollution. Use the list as a guide for your environ-ment.\\nSetting Up a Data Quality Framework. Y ou have to contend with so many types\\nof data pollution. Y ou need to make various decisions to embark on the cleansing of data.Y ou must dig into the sources of possible data corruption and determine the pollution.Most companies serious about data quality pull all these factors together and establish adata quality framework. Essentially, the framework provides a basis for launching dataquality initiatives. It embodies a systematic plan for action. The framework identifies theplayers, their roles, and responsibilities. In short, the framework guides the data qualityimprovement effort. Please refer to Figure 13-5. Notice the major functions carried outwithin the framework.\\nWho Should be Responsible?\\nData quality or data corruption originate in the source systems. Therefore, should not the\\nowners of the data in the source systems alone be responsible for data quality? If thesedata owners are responsible for the data, should they also be bear the responsibility forany data pollution that happens in the source systems? If data quality in the source sys-DATA QUALITY INITIATIVE 307\\nqOperational systems converted \\nfrom older versions are prone to the perpetuation of errors.\\nqOperational systems brought in \\nhouse from outsourcing companies converted from their proprietary software may have missing data.\\nqData from outside sources that is \\nnot verified and audited may have potential problems.\\nqWhen applications are \\nconsolidated because of corporate mergers and acquisitions, these may be error-prone because of time pressures.\\nqWhen reports from old legacy \\nsystems are no longer used, that could be because of erroneous data reported.\\nqIf users do not trust certain reports \\nfully, there may be room for suspicion because of bad data.qWhenever certain data elements \\nor definitions are confusing to the users, these may be suspect.\\nqIf each department has its own \\ncopies of standard data such as Customer or Product, it is likely corrupt data exists in these files.\\nqIf reports containing the same \\ndata reformatted differently do not match, data quality is suspect.\\nqWherever users perform too \\nmuch manual reconciliation, it may because of poor data quality.\\nqIf production programs \\nfrequently fail on data exceptions, large parts of the data in those systems are likely to be corrupt.\\nqWherever users are not able to \\nget consolidated reports, it is possible that data is not integrated. \\nFigure 13-4 Discovering the extent of data pollution.➨\\n➨\\n➨\\n➨\\n➨\\n➨➨➨➨➨➨➨\\nbe because of poor data quality.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9383d59a-9573-492d-87bf-1abcf2b0486e', embedding=None, metadata={'page_label': '327', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='tems is high, the data quality in the data warehouse will also be high. But, as you well\\nknow, in operational systems, there are no clear roles and responsibilities for maintainingdata quality. This is a serious problem. Owners of data in the operational systems are gen-erally not directly involved in the data warehouse. They have little interest in keeping thedata clean in the data warehouse. \\nForm a steering committee to establish the data quality framework discussed in the pre-\\nvious section. All the key players must be part of the steering committee. Y ou must haverepresentatives of the data owners of source systems, users of the data warehouse, and ITpersonnel responsible for the source systems and the data warehouse. The steering com-mittee is charged with assignment of roles and responsibilities. Allocation of resources isalso the steering committee’ s responsibility. The steering committee also arranges dataquality audits. \\nFigure 13-6 shows the participants in the data quality initiatives. These persons repre-\\nsent the user departments and IT. The participants serve on the data quality team in specif-ic roles. Listed below are the suggested responsibilities for the roles:\\nData Consumer. Uses the data warehouse for queries, reports, and analysis. Establish-\\nes the acceptable levels of data quality. \\nData Producer. Responsible for the quality of data input into the source systems.\\nData Expert. Expert in the subject matter and the data itself of the source systems. Re-\\nsponsible for identifying pollution in the source systems.\\nData Policy Administrator. Ultimately responsible for resolving data corruption as\\ndata is transformed and moved into the data warehouse. 308 DATA QUALITY: A KEY TO SUCCESS\\nIdentify the\\nbusiness functions\\naffected most by\\nbad data.Establish Data\\nQuality Steering\\nCommittee.Agree on a suitable\\ndata quality\\nframework.\\nInstitute data\\nquality\\npolicy and\\nstandards.\\nDefine quality\\nmeasurement\\nparameters and\\nbenchmarks.Select high impact\\ndata elements and\\ndetermine\\npriorities.\\nPlan and execute\\ndata cleansing for\\nhigh impact data\\nelements.Plan and execute\\ndata cleansing for\\nother less severe\\nelements.INITIAL\\nDATA\\nCLEANSING\\nEFFORTSONGOING\\nDATA\\nCLEANSING\\nEFFORTS\\nIT ProfessionalsUser Representatives\\nFigure 13-5 Data quality framework.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3c81b47d-a1dc-4dd7-862a-062012e40c93', embedding=None, metadata={'page_label': '328', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Data Integrity Specialist. Responsible for ensuring that the data in the source systems\\nconforms to the business rules.\\nData Correction Authority. Responsible for actually applying the data cleansing tech-\\nniques through the use of tools or in-house programs.\\nData Consistency Expert. Responsible for ensuring that all data within the data ware-\\nhouse (various data marts) are fully synchronized.\\nThe Purification Process\\nWe all know that it is unrealistic to hold up the loading of the data warehouse unless the\\nquality of all data is at the 100% level. That level of data quality is extremely rare. If so,how much of the data should you attempt to cleanse? When do you stop the purificationprocess? \\nAgain, we come to the issues of who will use the data and for what purpose. Estimate\\nthe costs and risks of each piece of incorrect data. Users usually settle for some extent oferrors, provided these errors result in no serious consequences. But the users need to bekept informed of the extent of possible data corruption and exactly which parts of the datacould be suspect. \\nHow then could you proceed with the purification process? With the complete partici-\\npation of your users, divide the data elements into priorities for the purpose of datacleansing. Y ou may adopt a simple categorization by grouping the data elements into threepriority categories: high, medium, and low. Achieving 100% data quality is critical for thehigh category. The medium-priority data requires as much cleansing as possible. Some er-rors may be tolerated when you strike a balance between the cost of correction and poten-tial effect of bad data. The low-priority data may be cleansed if you have any time and re-DATA QUALITY INITIATIVE 309\\nDATA\\nQUALITY\\nINITIATIVESDATA\\nCONSUMER\\n(User Dept.)\\nDATA\\nEXPERT\\n(User Dept.)DATA\\nPRODUCER\\n(User Dept.)DATA POLICY\\nADMINISTRATOR\\n(IT Dept.)DATA INTEGRITY\\nSPECIALIST\\n(IT Dept.)\\nDATA CORRECTION\\nAUTHORITY\\n(IT Dept.)DATA CONSISTENCY\\nEXPERT\\n(IT Dept.)\\nFigure 13-6 Data quality: participants and roles.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='76e0a52c-69c4-4ded-8b65-7099ea30423a', embedding=None, metadata={'page_label': '329', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='sources still available. Begin your data cleansing efforts with the high-priority data. Then\\nmove on to the medium-priority data. \\nA universal data corruption problem relates to duplicate records. As we have seen ear-\\nlier, for the same customer, there could be multiple records in the source systems. Activityrecords are related to each of these duplicate records in the source systems. Make sureyour overall data purification process includes techniques for correcting the duplicationproblem. The techniques must be able to identify the duplicate records and then relate allthe activities to this single customer. Duplication normally occurs in records relating topersons such as customers, employees, and business partners.\\nSo far, we have not discussed data quality with regard to data obtained from external\\nsources. Pollution can also be introduced into the data warehouse through errors in exter-nal data. Surely, if you pay for the external data and do not capture it from the public do-main, then you have every right to demand a warranty on data quality. In spite of what thevendor might profess about the quality of the data, for each set of external data, set upsome kind of data quality audit. If the external data fails the audit, be prepared to rejectthe corrupt data and demand a cleaner version.\\nFigure 13-7 illustrates the overall data purification process. Please observe the process\\nas shown in the figure and go through the following summary:\\n/L50539Establish the importance of data quality.\\n/L50539Form data quality steering committee.\\n/L50539Institute a data quality framework.\\n/L50539Assign roles and responsibilities.\\n/L50539Select tools to assist in the data purification process.\\n/L50539Prepare in-house programs as needed.310 DATA QUALITY: A KEY TO SUCCESS\\nSOURCE \\nSYSTEMSDATA \\nWAREHOUSE\\nDATA STAGING AREACleansed \\nDataPolluted \\nDataDATA CLEANSING \\nFUNCTIONS\\nVendor \\nToolsIn-house \\nPrograms\\nDATA QUALITY \\nFRAMEWORK\\nIT Professionals / User Representatives\\nFigure 13-7 Overall data purification process.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2558be9f-a0b8-4839-9813-b65af5118642', embedding=None, metadata={'page_label': '330', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Train the participants in data cleansing techniques.\\n/L50539Review and confirm data standards.\\n/L50539Prioritize data into high, medium, and low categories.\\n/L50539Prepare schedule for data purification beginning with the high priority data.\\n/L50539Ensure that techniques are available to correct duplicate records and to audit exter-\\nnal data.\\n/L50539Proceed with the purification process according to the defined schedule.\\nPractical Tips on Data Quality\\nBefore you run away to implement a comprehensive data quality framework and expend\\ntime and resources on data quality, let us pause to go over a few practical suggestions.Remember, ensuring data quality is a balancing act. Y ou already know that 100% dataquality is an unrealistic expectation. At the same time, overlooking errors that could po-tentially ruin the business is also not an option. Y ou have to find the right balance be-tween the data purification effort and the available time and resources. Here are a fewpractical tips:\\n/L50539Identify high-impact pollution sources and begin your purification process with\\nthese. \\n/L50539Do not try to do everything with in-house programs. \\n/L50539Tools are good and are useful. Select proper tools.\\n/L50539Agree on standards and reconfirm these.\\n/L50539Link data quality with specific business objectives. By itself, data quality work is\\nnot attractive.\\n/L50539Get the senior executive sponsor of your data warehouse project to be actively in-\\nvolved in backing the data cleansing initiative.\\n/L50539Get users totally involved and keep them constantly informed of the developments.\\n/L50539Wherever needed, bring in outside experts for specific assignments.\\nCHAPTER SUMMARY\\n/L50539Data quality is critical because it boosts confidence, enables better customer ser-\\nvice, enhances strategic decision making, and reduces risks from disastrous deci-sions.\\n/L50539Data quality dimensions include accuracy, domain integrity, consistency, complete-\\nness, structural definiteness, clarity, and many more.\\n/L50539Data quality problems run the gamut of dummy values, missing values, cryptic val-\\nues, contradicting values, business rule violations, inconsistent values, and so on. \\n/L50539Data pollution results from many sources in a data warehouse and this variety of\\npollution sources intensifies the challenges faced when attempting to clean up thedata. \\n/L50539Poor data quality of names and addresses presents serious concerns to organiza-\\ntions. This area is one of the greatest challenges.CHAPTER SUMMARY 311', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97e2e5dd-e773-485a-8a04-c6efea70d4db', embedding=None, metadata={'page_label': '331', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Data cleansing tools contain useful error discovery and error correction features.\\nLearn about them and make use of the tools applicable to your environment.\\n/L50539The DBMS itself can be used for data cleansing.\\n/L50539Set up a sound data quality initiative in your organization. Within the framework,\\nmake the data cleansing decisions.\\nREVIEW QUESTIONS\\n1. List five reasons why you think data quality is critical in a data warehouse.\\n2. Explain how data quality is much more than just data accuracy. Give an example.3. Briefly list three benefits of quality data in a data warehouse.4. Give examples of four types of data quality problems.5. What is the problem related to the reuse of primary keys? When does it usually oc-\\ncur? \\n6. Describe the functions of data correction in data cleansing tools.7. Name five common sources of data pollution. Give an example for each type of\\nsource.\\n8. List six types of error discovery features found in data cleansing tools.9. What is the “clean as you go” method? Is this a good approach for the data ware-\\nhouse environment?\\n10. Name any three types of participants on the data quality team. What are their func-\\ntions?\\nEXERCISES\\n1. Match the columns:\\n1. domain integrity A. detect inconsistencies\\n2. data aging B. better customer service3. entity integrity C. synchronize all data4. data consumer D. allowable values5. poor quality data E. used to pass edits6. data consistency expert F . uses warehouse data7. error discovery G. heterogeneous systems integration8. data pollution source H. lost business opportunities9. dummy values I. prevents duplicate key values\\n10. data quality benefit J. decay of field values \\n2. Assume that you are the data quality expert on the data warehouse project team for\\na large financial institution with many legacy systems dating back to the 1970s. Re-view the types of data quality problems you are likely to have and make suggestionson how to deal with those.\\n3. Discuss the common sources of data pollution and provide examples. 312 DATA QUALITY: A KEY TO SUCCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5c94c41d-d406-42ee-9dae-39a572b31f85', embedding=None, metadata={'page_label': '332', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. Y ou are responsible for the selection of data cleansing tools for your data warehouse\\nenvironment. How will you define the criteria for selection? Prepare a checklist forevaluation and selection of these tools.\\n5. As a data warehouse consultant, a large bank with statewide branches has hired you\\nto help the company set up a data quality initiative. List your major considerations.Produce an outline for a document describing the initiative, the policies, and theprocedures.EXERCISES 313', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d160959f-2240-45e4-9418-f61c64c92581', embedding=None, metadata={'page_label': '333', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 14\\nMATCHING INFORMATION TO \\nTHE CLASSES OF USERS \\nCHAPTER OBJECTIVES\\n/L50539Appreciate the enormous information potential of the data warehouse\\n/L50539Carefully note all the users who will use the data warehouse and devise a practical\\nway to classify them\\n/L50539Delve deeply into the types of information delivery mechanisms\\n/L50539Match each class of user to the appropriate information delivery method\\n/L50539Understand the overall information delivery framework and study the components\\nLet us assume that your data warehouse project team has successfully identified all the\\npertinent source systems. Y ou have extracted and transformed the source data. Y ou havethe best data design for the data warehouse repository. Y ou have applied the most effectivedata cleansing methods and gotten rid of most of the pollution from the source data. Usingthe most optimal methods, you have loaded the transformed and cleansed data into yourdata warehouse database. Now what? \\nAfter performing all of these tasks most effectively, if your team has not provided the\\nbest possible mechanism for information delivery to your users, you have really accom-plished nothing from the users’ perspective. As you know, the data warehouse exists forone reason and one reason alone. It is there just for providing strategic information to yourusers. For the users, the information delivery mechanism is the data warehouse. The userinterface for information is what determines the ultimate success of your data warehouse.If the interface is intuitive, easy to use, and enticing, the users will keep coming back tothe data warehouse. If the interface is difficult to use, cumbersome, and convoluted, yourproject team may as well leave the scene.\\nWho are your users? What do they want? Y our project team, of course, knows the an-\\nswers and has designed the data warehouse based on the requirements of these users. How\\n315Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cce48697-be23-470e-8098-8f0547a815f1', embedding=None, metadata={'page_label': '334', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='do you provide the needed information to your users? This depends on who your users are,\\nwhat information they need, when and where they need the information, and in exactlywhat form they need the information. In this chapter, we will consider general classes ofusers of a typical warehouse and the methods for providing information to them. \\nA large portion of the success of your data warehouse rests on the information delivery\\ntools made available to the users. Selecting the right tools is of paramount importance.Y ou have to make sure that the tools are most appropriate for your environment. We willdiscuss in detail the selection of information delivery tools.\\nINFORMATION FROM THE DATA WAREHOUSE\\nAs an IT professional, you have been involved in providing information to the user com-\\nmunity. Y ou must have worked on different types of operational systems that provide in-formation to users. The users in enterprises make use of the information from the opera-tional systems to perform their day-to-day work and run the business. If we have beeninvolved in information delivery from operational systems and we understand what infor-mation delivery to the users entails, then what is the need for this special study on infor-mation delivery from the data warehouse? \\nLet us review how information delivery from a data warehouse differs from informa-\\ntion delivery from an operational system. If the kinds of strategic information made avail-able in a data warehouse were readily available from the source systems, then we wouldnot really need the warehouse. Data warehousing enables the users to make better strate-gic decisions by obtaining data from the source systems and keeping it in a format suit-able for querying and analysis. \\nData Warehouse Versus Operational Systems\\nDatabases already exist in operational systems for querying and reporting. If so, how do\\nthe databases in operational systems differ from those of the databases in the data ware-house? The difference relates to two aspects of the information contained in these data-bases. First, they differ in the usage of the information. Next, they differ in the value ofthe information. Figure 14-1 shows how the data warehouse differs from an operationalsystem in usage and value.\\nUsers go to the data warehouse to find information on their own. They navigate\\nthrough the contents and locate what they want. The users formulate their own queries andrun them. They format their own reports, run them, and receive the results. Some usersmay use predefined queries and preformatted reports but, by and large, the data ware-house is a place where the users are free to make up their own queries and reports. Theymove around the contents and perform their own analysis, viewing the data in ever somany different ways. Each time a user goes to the data warehouse, he or she may run dif-ferent queries and different reports, not repeating the earlier queries or reports. The infor-mation delivery is interactive. \\nCompare this type of usage of the data warehouse to how an operational system is used\\nfor information delivery. How often are the users allowed to run their own queries and for-mat their own reports from an operational system? From an inventory control application,do the users usually run their own queries and make up their own reports? Hardly ever.First of all, because of efficiency considerations, operational systems are not designed to316 MATCHING INFORMATION TO THE CLASSES OF USERS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2020a139-5da3-4d53-ac51-c74414ecb23c', embedding=None, metadata={'page_label': '335', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='let users loose on the systems. The users may impact the performance of the system ad-\\nversely with runaway queries. Another important point is that the users of operational sys-tems do not exactly know the contents of the databases and metadata or data dictionaryentries are typically unavailable to them. Interactive analysis, which forms the bedrock ofinformation delivery in the data warehouse, is almost never present in an operational sys-tem.\\nWhat about the value of the information from the data warehouse to the users? How\\ndoes the value of information from an operational system compare to the value from thedata warehouse? Take the case of information for analyzing the business operations. Theinformation from an operational system shows the users how well the enterprise is doingfor running the day-to-day business. The value of information from an operational systemenables the users to monitor and control the current operations. On the other hand, infor-mation from the data warehouse gives the users the ability to analyze growth patterns inrevenue, profitability, market penetration, and customer base. Based on such analysis, theusers are able to make strategic decisions to keep the enterprise competitive and sound.Look at another area of the enterprise, namely, marketing. With regard to marketing, thevalue of information from the data warehouse is oriented to strategic matters such as mar-ket share, distribution strategy, predictability of customer buying patterns, and marketpenetration. Although this is the case of the value of information from the data warehousefor marketing, what is the value of information from operational systems? Mostly formonitoring sales against target quotas and for attempting to get repeat business from cus-tomers. \\nWe see that the usage and value of information from the data warehouse differ from\\nthose of information from operational systems. What is the implication of the differences?First of all, because of the differences, as an IT professional, you should not try to applyINFORMATION FROM THE DATA WAREHOUSE 317\\nFigure 14-1 Data warehouse versus operational systems.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0ea29c05-7da2-4351-93b5-abdaa09c001a', embedding=None, metadata={'page_label': '336', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='the principles of information delivery from operational systems to the data warehouse. In-\\nformation delivery from the data warehouse is markedly different. Different methods areneeded. Then, you should take serious note of the interactive nature of information deliv-ery from the data warehouse. Users are expected to gather information and perform analy-sis from the data in the data warehouse interactively on their own without the assistance ofIT. The IT staff supporting the data warehouse users do not run the queries and reports forthe users; the users do that by themselves. So make the information from the data ware-house easily and readily available to the users in their own terms. \\nInformation Potential\\nBefore we look at the different types of users and their information needs, we need to gain\\nan appreciation of the enormous information potential of the data warehouse. Because ofthis great potential, we have to pay adequate attention to information delivery from thedata warehouse. We cannot treat information delivery in a special way unless we fully re-alize the significance of how the data warehouse plays a key role in the overall manage-ment of an enterprise. \\nOverall Enterprise Management. In every enterprise, three sets of processes gov-\\nern the overall management. First, the enterprise is engaged in planning. Execution of theplans takes place next. Assessment of the results of the execution follows. Figure 14-2 in-dicates these plan–executive–assess processes.\\nLet us see what happens in this closed loop. Consider the planning for expansion into a\\nspecific geographic market for an enterprise. Let us say your company wants to increaseits market share in the Northwest Region. Now this plan is translated into execution by318 MATCHING INFORMATION TO THE CLASSES OF USERS\\nPLANNING\\nEXECUTIONASSESSMENTPlan\\nmarketing\\ncampaigns\\nExecute\\nmarketing\\ncampaignsAssess\\nresults of\\ncampaignsEnhance\\ncampaigns\\nbased on\\nresultsData\\nWarehouse\\nhelps in\\nplanning\\nData\\nWarehouse\\nhelps\\nassess\\nresults\\nFigure 14-2 Enterprise plan–execute–assess closed loop.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='58d600f1-19c3-482c-b010-21997df530ad', embedding=None, metadata={'page_label': '337', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='means of promotional campaigns, improved services, and customized marketing. After\\nthe plan is executed, your company wants to find the results of the promotional campaignsand the marketing initiatives. Assessment of the results determines the effectiveness of thecampaigns. Based on the assessment of the results, more plans may be made to vary thecomposition of the campaigns or launch additional ones. The cycle of planning, execut-ing, and assessing continues.\\nIt is very interesting to note that the data warehouse, with its specialized information\\npotential, fits nicely in this plan–execute–assess loop. The data warehouse reports on thepast and helps plan the future. First, the data warehouse assists in the planning. Once theplans are executed, the data warehouse is used to assess the effectiveness of the execution. \\nLet us go back to the example of your company wanting to expand in the Northwest\\nRegion. Here the planning consists of defining the proper customer segments in that regionand also defining the products to concentrate on. Y our data warehouse can be used effec-tively to separate out and identify the potential customer segments and product groups forthe purpose of planning. Once the plan is executed with promotional campaigns, your datawarehouse helps the users to assess and analyze the results of the campaigns. Y our users cananalyze the results by product and by individual districts in the Northwest Region. They cancompare the sales to the targets set for the promotional campaigns, or the prior year’ s sales,or against industry averages. The users can estimate the growth in earnings due to the pro-motional campaigns. The assessment can then lead to further planning and execution. Thisplan–execute–assess loop is critical for the success of an enterprise.\\nInformation Potential for Business Areas. We considered one isolated example\\nof how the information potential of your data warehouse can assist in the planning for amarket expansion and in the assessment of the results of the execution of marketing cam-paigns for that purpose. Let us go through a few general areas of the enterprise where thedata warehouse can assist in the planning and assessment phases of the management loop.\\nProfitability Growth. To increase profits, management has to understand how the prof-\\nits are tied to product lines, markets, and services. Management must gain insights intowhich product lines and markets produce greater profitability. The information from thedata warehouse is ideally suited to plan for profitability growth and to assess the resultswhen the plans are executed.\\nStrategic Marketing. Strategic marketing drives business growth. When management\\nstudies the opportunities for up-selling and cross-selling to existing customers and for ex-panding the customer base, they can plan for business growth. The data warehouse hasgreat information potential for strategic marketing. \\nCustomer Relationship Management. A customer’ s interactions with an enterprise\\nare captured in various operational systems. The order processing system contains the or-ders placed by the customer; the product shipment system, the shipments; the sales sys-tem, the details of the products sold to the customer; the accounts receivable system, thecredit details and the outstanding balances. The data warehouse has all the data about thecustomer extracted from the various disparate source systems, transformed, and integrat-ed. Thus, your management can “know” their customers individually from the informa-tion available in the data warehouse. This knowledge results in better customer relation-ship management. INFORMATION FROM THE DATA WAREHOUSE 319', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ee4d9cf8-7f94-4b0a-afed-3f3178ea9998', embedding=None, metadata={'page_label': '338', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Corporate Purchasing. From where can your management get the overall picture of\\ncorporate-wide purchasing patterns? Y our data warehouse. This is where all data aboutproducts and vendors are collected after integration from the source systems. Y our datawarehouse empowers corporate management to plan for streamlining purchasing process-es.\\nRealizing the Information Potential. What is the underlying significance of the infor-\\nmation potential of the data warehouse? The data warehouse enables the users to view thedata in the right business context. The various operational systems collect massive quanti-ties of data on numerous types of business transactions. But these operational systems arenot directly helpful for planning and assessment of results. The users need to assess the re-sults by viewing the data in the proper business context. For example, when viewing thesales in the Northwest Region, the users need to view the sales in the business context ofgeography, product, promotion, and time. The data warehouse is designed for analysis ofmetrics such as sales along these dimensions. The users are able to retrieve the data, trans-form it into useful information, and leverage the information for planning and assessingthe results.\\nThe users interact with the data warehouse to obtain the data, transform it into useful\\ninformation, and realize the full potential. This interaction of the users generally goesthrough the six stages indicated in Figure 14-3 and summarized below. \\n1. Think through the business need and define it in terms of business rules as applica-\\nble to data in the data warehouse.\\n2. Harvest or select the appropriate subset of the data according to the stipulated busi-\\nness rules.320 MATCHING INFORMATION TO THE CLASSES OF USERS\\nDATA\\nWAREHOUSE\\nEND -USERSDefine business\\nneed in terms of\\nwarehouse data.\\nSelect appropriate\\ndata subset from\\nwarehouse.\\nEnrich selected\\nsubset with\\ncalculations, etc.\\nAssociate\\nmeanings to\\nselected data.\\nStructure results\\ninto formats\\nsuitable to users.\\nPresent structured\\nresults in a variety\\nof ways.DATA\\nINFORMATION\\n654321\\nFigure 14-3 Realization of the information potential: stages.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='66cdc30d-0836-4c98-a3e7-7815bfee2abb', embedding=None, metadata={'page_label': '339', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. Enrich the selected subset with calculations such as totals or averages. Apply trans-\\nformations to translate codes to business terms.\\n4. Use metadata to associate the selected data with its business meaning.5. Structure the result in a format useful to the users.6. Present the structured information in a variety of ways, including tables, texts,\\ngraphs, and charts. \\nUser–Information Interface\\nIn order to pass through the six stages and realize the information potential of the data\\nwarehouse, you have to build a solid interface for information delivery to the users. Putthe data warehouse on one side and the entire community of users on the other. The inter-face must be able to let the users realize the full information potential of the data ware-house. \\nThe interface logically sits in the middle, enabling information delivery to the users.\\nThe interface could be a specific set of tools and procedures, tailored for your environ-ment. At this point, we are not discussing the exact composition of the interface; we justwant to specify its features and characteristics. Without getting into the details of the typesof users and their specific information needs, let us define the general characteristics ofthe user–information interface.\\nInformation Usage Modes. When you consider all the various ways the data ware-\\nhouse may be used, you note that all the usage comes down to two basic modes or ways.Both modes relate to obtaining strategic information. Remember, we are not consideringinformation retrieved from operational systems.\\nVerification Mode. In this mode, the business user proposes a hypothesis and asks a se-\\nries of questions to either confirm or repudiate it. Let us see how the usage of the infor-mation in this mode works. Assume that your marketing department planned and executedseveral promotional campaigns on two product lines in the South-Central Region. Nowthe marketing department wants to assess the results of the campaign. The marketing de-partment goes to the data warehouse with the hypothesis that the sales in the South-Central Region have increased. Information from the data warehouse will help confirmthe hypothesis.\\nDiscovery Mode. When using the data warehouse in the discovery mode, the business\\nanalyst does not use a predefined hypothesis. In this case, the business analyst desires todiscover new patterns of customer behavior or product demands. The user does not haveany preconceived notions of what the result sets will indicate. Data mining applicationswith data feeds from the data warehouse are used for knowledge discovery. \\nWe have seen that users interact with the data warehouse for information either in the\\nhypothesis verification mode or in a knowledge discovery mode. What are the approachesfor the interaction? In other words, do the users interact with the data warehouse in an in-formational approach, an analytical approach, or by using data mining techniques?\\nInformational Approach. In this approach, with query and reporting tools, the users\\nretrieve historical or current data and perform some standard statistical analysis. The dataINFORMATION FROM THE DATA WAREHOUSE 321', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3d5a36c5-7d02-4f42-b151-667982c5a309', embedding=None, metadata={'page_label': '340', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='may be lightly or heavily summarized. The result sets may take the form of reports and\\ncharts. \\nAnalytical Approach. As the name of this approach indicates, the users make use of\\nthe data warehouse for performing analysis. They do the analysis along business dimen-sions using historical summaries or detailed data. The business users conduct the analysisusing their own business terms. More complex analysis involves drill down, roll up, orslice and dice. \\nData Mining Approach. Both the informational and analytical approaches work in the\\nverification mode. The data mining approach, however, works in the knowledge discoverymode.\\nWe have reviewed two modes and three approaches for information usage. What about\\nthe characteristics and structures of the data that is being used? How should the data beavailable through the user–information interface? Typically, the information made avail-able through the user–information interface has the following characteristics:\\nPreprocessed Information. These include routine information automatically created\\nand made readily available. Monthly and quarterly sales analysis reports, summaryreports, and routine charts fall into this category. Users simply copy such pre-processed information.\\nPredefined Queries and Reports. This is a set of query templates and report formats\\nkept ready for the users. The users apply the appropriate parameters and run thequeries and reports as and when needed. Sometimes, the users are allowed to makeminor modifications to the templates and formats.\\nAd Hoc Constructions. Users create their own queries and reports using appropriate\\ntools. This category acknowledges the fact that not every need of the users can beanticipated. Generally, only power users and some regular users construct their ownqueries and reports.\\nFinally, let us list the essential features necessary for the user–information interface.\\nThe interface must\\n/L50539Be easy to use, intuitive, and enticing to the users\\n/L50539Support the ability to express the business need clearly\\n/L50539Convert the expressed need into a set of formal business rules\\n/L50539Be able to store these rules for future use\\n/L50539Provide ability to the users to modify retrieved rules \\n/L50539Select, manipulate, and transform data according to the business rules\\n/L50539Have a set of data manipulation and transformation tools\\n/L50539Correctly link to data storage to retrieve the selected data\\n/L50539Be able to link with metadata\\n/L50539Be capable of formatting and structuring output in a variety of ways, both textual\\nand graphical\\n/L50539Have the means of building a procedure for executing specific steps\\n/L50539Have a procedure management facility322 MATCHING INFORMATION TO THE CLASSES OF USERS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='63e53594-24fc-4235-8c1e-4b167b9817ac', embedding=None, metadata={'page_label': '341', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Industry Applications\\nSo far in this section, we have clearly perceived the great information potential of the data\\nwarehouse. This enormous information potential drives the discussion that follows, wherewe get into more specifics and details. Before we do that, let us pause to refresh our mindson how the information potential of data warehouses is realized in a sample of industrysectors.\\nManufacturing: Warranty and service management, product quality control, order ful-\\nfillment and distribution, supplier and logistics integration.\\nRetail and Consumer Goods: Store layout, product bundling, cross-selling, value chain\\nanalysis.\\nBanking and Finance: Relationship management, credit risk management.\\nWHO WILL USE THE INFORMATION?\\nY ou will observe that in six months after deployment of the data warehouse, the number\\nof active users doubles. This is a typical experience for most data warehouses. Who arethese new people arriving at the data warehouse for information? Unless you know how toanticipate who will come to get information, you will not be able to cater to their needsappropriately and adequately. \\nAnyone who needs strategic information is expected to be part of the groups of users.\\nThat includes business analysts, business planners, departmental managers, and senior ex-ecutives. Each of the data marts may be built for the specific needs of one segment of theuser groups. In this case, you can identify the special groups and cater to their needs. Atthis stage, when we are discussing information delivery, we are not considering the infor-mation content so much but the actual mechanism of information delivery.\\nEach group of users has specific business needs for which they expect to get answers\\nfrom the data warehouse. When we try to classify the user groups, it is best to understandthem from the perspective of what they expect to get out of the warehouse. How are theygoing to use the information content in their job functions? Each user is performing a par-ticular business function and needs information for support in that specific job function.Let us, therefore, base our classification of the users on their job functions and organiza-tional levels.\\nFigure 14-4 suggests a way of classifying the user groups. When you classify the\\nusers by their job functions, their positions in the organizational hierarchy, and theircomputing proficiency, you get a firm basis for understanding what they need and howto provide information in the proper formats. If you are considering a user in account-ing and finance, that user will be very comfortable with spreadsheets and financial ra-tios. For a user in customer service, a GUI screen showing consolidated informationabout each customer is most useful. For someone in marketing, a tabular format may besuitable.\\nClasses of Users\\nIn order to make your information delivery mechanism best suited for your environment,\\nyou need to have a thorough understanding of the classes of users. First, let us start by as-WHO WILL USE THE INFORMATION? 323', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a984a529-4841-4192-ae02-45e1dc2eff21', embedding=None, metadata={'page_label': '342', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='sociating the computing proficiency of the users with how each group based on this type\\nof division interacts with the data warehouse.\\nCasual or Novice User. Uses the data warehouse occasionally, not daily. Needs a very\\nintuitive information interface. Looks for the information delivery to prompt theuser with available choices. Needs big button navigation.\\nRegular User. Uses the data warehouse almost daily. Comfortable with computing op-\\ntions but cannot create own reports and queries from scratch. Needs query tem-plates and predefined reports.\\nPower User. Is highly proficient with technology. Can create reports and queries from\\nscratch. Some can write their own macros and scripts. Can import data into spread-sheets and other applications. \\nNow let us change the perspective a bit and look at the user types by the way they wish\\nto interact to obtain information.\\nPreprocessed Reports. Use routine reports run and delivered at regular intervals. \\nPredefined Queries and Templates. Enter own set of parameters and run queries with\\npredefined templates and reports with predefined formats.\\nLimited Ad Hoc Access. Create from scratch and run limited number and simple types\\nof queries and analysis.\\nComplex Ad Hoc Access. Create complex queries and run analysis sessions from\\nscratch regularly. Provide the basis for preprocessed and predefined queries and re-ports.324 MATCHING INFORMATION TO THE CLASSES OF USERS\\nOrganizational \\nHierarchy\\nJob \\nFunction\\nMarketingPersonnelPurchasing AccountingNovice UserRegular \\nUserPower UserComputing \\nProficiency\\nSupportAnalystManagerExecutive\\nFigure 14-4 A method for classifying the users.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1f405552-df87-4150-86e7-cc60b4b69c0f', embedding=None, metadata={'page_label': '343', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Let us view the user groups from yet another perspective. Consider the users based on\\ntheir job functions.\\nHigh-Level Executives and Managers. Need information for high-level strategic de-\\ncisions. Standard reports on key metrics are useful. Customized and personalizedinformation is preferable. \\nTechnical Analysts. Look for complex analysis, statistical analysis, drill-down and\\nslice-dice capabilities, and freedom to access the entire data warehouse. \\nBusiness Analysts. Although comfortable with technology, are not quite adept at creat-\\ning queries and reports from scratch. Predefined navigation helpful. Want to look atthe results in many different ways. To some extent, can modify and customize pre-defined reports.\\nBusiness-Oriented Users. These are knowledge workers who like point-and-click\\nGUIs. Desire to have standard reports and some measure of ad hoc querying.\\nWe have reviewed a few ways of understanding how the users may be grouped. Now,\\nlet us put it all together and label the user classes in terms of their access and informationdelivery practices and preferences. Please see Figure 14-5 showing a way of classifyingthe users adopted by many data warehousing experts and practitioners. This figure showsfive broad classes of users. Within each class, the figure indicates the basic characteristicsof the users in that class. The figure also assigns the users in the organizational hierarchyto specific classes.\\nAlthough the classification appears to be novel and interesting, you will find that it\\nprovides us with a good basis to understand the characteristics of each group of users. Y oucan fit any user into one of these classes. When you observe the computing proficiency,WHO WILL USE THE INFORMATION? 325\\nFARMERSEXPLORERSOPERATORSMINERSTOURISTS\\nDATA\\nWAREHOUSEExecutives :\\ninterested in\\nbusiness\\nindicators\\nSupport\\nstaff:\\ninterested\\nin current\\ndata\\nAnalysts :\\ninterested\\nin routine\\nanalysisSkilled\\nanalysts :\\ninterested in\\nhighly ad\\nhoc analysisSpecial\\npurpose\\nanalysts :\\ninterested in\\nknowledge\\ndiscovery\\nFigure 14-5 Data warehouse user classes.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ce9a39ba-d3a8-4b2e-a4bb-e3e591938e6a', embedding=None, metadata={'page_label': '344', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='the organizational level, the information requirements, or even the frequency of usage,\\nyou can readily identify the user as belonging to one of these groups. That will help you tosatisfy the needs of each user who depends on your data warehouse for information. Itcomes down to this: if you provide proper information delivery to tourists, operators,farmers, explorers, and miners, then you would have taken care of the needs of every oneof your users.\\nWhat They Need\\nBy now we have formalized the broad classifications of the data warehouse users. Let us\\npause and consider how we accomplished this. If you take two of your users with similarinformation access characteristics, computing proficiency, and scope of informationneeds, you may very well place both these users in the same broad class. For example, ifyou take two senior executives in different departments, they are similar in the way theywould like to get information and in the level and scope of information they would like tohave. Y ou may place both of these executives in the tourist class or category. \\nOnce you put both of these users in the tourist category, then it is easy for you to un-\\nderstand and formulate the requirements for information delivery to these two executives.The types of information needed by one user in a certain category are similar to the typesneeded by another user in the same category. An understanding of the needs of a categoryof users, generalized to some extent, provides insight into how best to provide the types ofneeded information. Formal classification leads to understanding the information needs.Understanding the information needs, in turn, leads to establishing proper ways for pro-viding the information. Establishing the best methods and techniques for each class ofusers is the ultimate goal of information delivery.\\nWhat do the tourists need? What do the farmers need? What does each class of users\\nneed? Let us examine each class, one by one, review the information access characteris-tics, and arrive at the information needs.\\nTourists. Imagine a tourist visiting an interesting place. First of all, the tourist has\\nstudied the broader features of the place he or she is visiting and is aware of the richnessof the culture and the variety of sites at this place. Although many interesting sites areavailable, the tourist has to pick and choose the most worthwhile sites to visit. Once he orshe has arrived at the place, the tourist must be able to select the sites to visit with utmostease. At a particular site, if the tourist finds something very attractive, he or she wouldlike to allocate additional time to that site. \\nNow let us apply the tourist story to the data warehouse. A senior level executive arriv-\\ning at the data warehouse for information is like a tourist visiting an interesting and usefulplace. The executive has a broad business perspective and knows about the overall infor-mation content of the data warehouse. However, the executive has no time to browsethrough the data warehouse in any detailed fashion. Each executive has specific key indi-cators. These are like specific sites to be visited. The executive wants to inspect the key in-dicators and if something interesting is found about any of them, the executive wants tospend some more time exploring further. The tourist has predefined expectations abouteach site being visited. If a particular site deviates from the expectations, the tourist wantsto ascertain the reasons why. Similarly, if the executive finds indicators to be out of line,further investigation becomes necessary.326 MATCHING INFORMATION TO THE CLASSES OF USERS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5a96ba56-802f-4730-8795-32e7aad00f38', embedding=None, metadata={'page_label': '345', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Let us, therefore, summarize what the users classified as tourists need from the data\\nwarehouse:\\n/L50539Status of the indicators at routine intervals\\n/L50539Capability to identify items of interest without any difficulty\\n/L50539Selection of what is needed with utmost ease without wasting time in long naviga-\\ntion\\n/L50539Ability to quickly move from one indicator of interest to another\\n/L50539Wherever needed, additional information should be easily available about selected\\nkey indicators for further exploration\\nOperators. We have looked at some of the characteristics of users classified as opera-\\ntors. This class of users is interested in the data warehouse for one primary reason. Theyfind the data warehouse to be the integrated source of information, not for history dataalone, but for current data as well. Operators are interested in current data at a detailedlevel. Operators are really monitors of current performance. Departmental managers, linemanagers, and section supervisors may all be classified as operators.\\nOperators are interested in today’ s performance and problems. They are not interested\\nin historical data. Being extensive users of OLTP systems, operators expect fast responsetimes and quick access to detailed data. How can they resolve the current bottleneck in theproduct distribution system? What are the currently available alternative shipment meth-ods and which industrial warehouse is low on stock? Operators concern themselves withquestions like these relating to current situations. Because the data warehouse receivesand stores data extracted from disparate source systems, operators expect to find their an-swers there.\\nPlease note the following summary of what operators need.\\n/L50539Immediate answers based on reliable current data\\n/L50539Current state of the performance metrics\\n/L50539Data as current as possible with daily or more frequent updates from source systems\\n/L50539Quick access to very detailed information\\n/L50539Rapid analysis of most current data\\n/L50539Simple and straightforward interface for information\\nFarmers. What do some of the data warehouse users and farmers have in common?\\nConsider a few traits of farmers. They are very familiar with the terrain. They know exact-ly what they want in terms of crops. Their requirements are consistent. The farmers knowhow to use the tools, work the fields, and get results. They also know the value of theircrops. Now match these characteristics with the category of data warehouse users classi-fied as farmers.\\nTypically, different types of analysts in an enterprise may be classified as farmers.\\nThese users may be technical analysts or analysts in marketing, sales, or finance. Theseanalysts have standard requirements. The requirements may comprise estimating prof-itability by products or analyzing sales every month. Requirements rarely change. Theyare predictable and routine. WHO WILL USE THE INFORMATION? 327', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='467d18f4-71fc-435a-b093-cc7996902754', embedding=None, metadata={'page_label': '346', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Let us summarize the needs of the users classified as farmers.\\n/L50539Quality data properly integrated from source systems\\n/L50539Ability to run predictable queries easily and quickly\\n/L50539Capability to run routine reports and produce standard result types\\n/L50539Obtain same types of information at predictable intervals\\n/L50539Precise and smaller result sets\\n/L50539Mostly current data with simple comparisons with historical data\\nExplorers. This classification of users is different from the usual kind of routine users.\\nExplorers do not have set ways of looking for information. They tend to go where veryfew venture to proceed. The explorers often combine random probing with unpredictableinvestigation. Many times the investigation may not lead to any results, but the few thatdig up useful patterns and unusual results, produce nuggets of information that are noth-ing but solid gold. So the explorer continues his or her relentless search, using nonstan-dard procedures and unorthodox methods. \\nIn an enterprise, researchers and highly skilled technical analysts may be classified as\\nexplorers. These users use the data warehouse in a highly random manner. The frequencyof their use is quite unpredictable. They may use the data warehouse for several days of in-tense exploration and then stop using it for many months. Explorers analyze data in waysvirtually unknown to other types of users. The queries run by explorers tend to encompasslarge data masses. These users work with lots of detailed data to discern desired patterns.These results are elusive, but the explorers continue until they find the patterns and rela-tionships.\\nAs in the other cases, let us summarize the needs of the users classified as explorers.\\n/L50539Totally unpredictable and intensely ad hoc queries\\n/L50539Ability to retrieve large volumes of detailed data for analysis\\n/L50539Capability to perform complex analysis\\n/L50539Provision for unstructured and completely new and innovative queries and analysis\\n/L50539Long and protracted analysis sessions in bursts\\nMiners. People mining for gold dig to discover precious nuggets of great value. The\\nusers classified as miners also work in a similar manner. Before we get into the character-istics and needs of the miners, let us compare the miners with explorers, because both areinvolved in heavy analysis. Experts state that role of the explorer is to create or suggesthypotheses, whereas the role of the miner is to prove or disprove hypotheses. This is oneway of looking at the miner’ s role. However, the miner works to discover new, unknown,and unsuspected patterns in the data. \\nMiners are a special breed. In an enterprise, they are special purpose analysts with\\nhighly specialized training and skills. Many companies do not have users who might becalled miners. Businesses employ outside consultants for specific data mining projects.Data miners adopt various techniques and perform specialized analysis that discoversclusters of related records, estimation of values for an unknown variable, grouping ofproducts that would be purchased together, and so on. \\nHere is a summary of the needs for the users classified as miners:328 MATCHING INFORMATION TO THE CLASSES OF USERS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8a60f82f-ca9b-4818-af35-4698b468f592', embedding=None, metadata={'page_label': '347', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Access to mountains of data to analyze and mine\\n/L50539Availability of large volumes of historical data going back many years\\n/L50539Ability to wade through large volumes to obtain meaningful correlations\\n/L50539Capability of extracting data from the data warehouse into formats suitable for spe-\\ncial mining techniques\\n/L50539Ability to work with data in two modes: one to prove or disprove a stated hypothe-\\nsis, the other to discover hypotheses without any preconceived notions\\nHow to Provide Information\\nWhat is the point of all this discussion about tourists, operators, farmers, explorers, and\\nminers? What is our goal? As part of a data warehouse project team, your objective is toprovide each user exactly what that user needs in a data warehouse. The information de-livery system must be wide enough and appropriate enough to suit the entire needs ofyour user community. What techniques and tools do your executives and managers need?How do your business analysts look for information? What about your technical analystsresponsible for deeper and more intense analysis and the knowledge worker charged withmonitoring day-to-day current operations? How are they going to interact with your datawarehouse?\\nIn order to provide the best information delivery system, you have to find answers to\\nthese questions. But how? Do you have to go to each individual user and determine howhe or she plans to use the data warehouse? Do you then aggregate all these requirementsand come up with the totality of the information delivery system? This would not be apractical approach. This is why we have come up with the broad classifications of users. Ifyou are able to provide for these classifications of users, then you cover almost all of youruser community. Maybe in your enterprise there are no data miners yet. If so, you do nothave to cater to this group at the present time. \\nWe have reviewed the characteristics of each class of users. We have also studied the\\nneeds of each of these classes, not in terms of the specific information content, but howand in what ways each class needs to interact with the data warehouse. Let us now turnour attention to the most important question: how to provide information. \\nPlease study Figure 14-6 very carefully. This figure describes three aspects of provid-\\ning information to the five classes of users. The architectural implications state the re-quirements relating to components such as metadata and user–information interface.These are broad architectural needs. For each user class, the figure indicates the types oftools most useful for that class. These specify the types. When you select vendors andtools, you will use this as a guide. The “other considerations” listed in the figure includesdesign issues, special techniques, and any out-of-the-ordinary technology requirements.\\nINFORMATION DELIVERY\\nIn all of our deliberations up to now, you have come to realize that there are four underly-\\ning methods for information delivery. Y ou may be catering to the needs of any class ofusers. Y ou may be constructing the information delivery system to satisfy the require-ments of users with simple needs or those of power users. Still the principal means of de-livery are the same. INFORMATION DELIVERY 329', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d948c1b3-b908-4f37-8dd6-a7921d16e651', embedding=None, metadata={'page_label': '348', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The first method is the delivery of information through reports. Of course, the formats\\nand content could be sophisticated. Nevertheless, these are reports. The method of infor-mation delivery through reports is a carry-over from operational systems. Y ou are familiarwith hundreds of reports distributed from legacy operational systems. The next method isalso a perpetuation of a technique from operational systems. In operational systems, theusers are allowed to run queries in a very controlled setup. However, in a data warehouse,query processing is the most common method for information delivery. The types ofqueries run the gamut from simple to very complex. As you know, the main difference be-tween queries in an operational system and in the data warehouse is the extra capabilitiesand openness in the warehouse environment.\\nThe method of interactive analysis is something special in the data warehouse environ-\\nment. Rarely are any users provided with such an interactive method in operational sys-tems. Lastly, the data warehouse is the source for providing integrated data for down-stream decision support applications. The Executive Information System is one suchapplication. But more specialized applications such as data mining make the data ware-house worthwhile. Figure 14-7 shows the comparison of information delivery methodsbetween the data warehouse and operational systems. \\nThe rest of this section is devoted to special considerations relating to these four meth-\\nods. We will highlight some basic features of the reporting and query environments andprovide details to be taken into account while designing these methods of information de-livery.330 MATCHING INFORMATION TO THE CLASSES OF USERSArchitectural Implications Tool Features Other ConsiderationsTourists Operators Farmers Explorers Miners\\nStrong Metadata \\ninterface \\nincluding key \\nword search.\\nWeb-enabled user \\ninterface.\\nCustomized for \\nindividual needs.\\nIntuitive \\nnavigation.\\nAbility to provide \\ninterface through \\nspecial icons.\\nLimited drill-\\ndown.\\nVery moderate \\nOLAP \\ncapabilities.\\nSimple applica-\\ntions for standard \\ninformation.. Fast response \\ntimes.\\nScope of data \\ncontent fairly \\nlarge.\\nSimple user \\ninterface to get \\ncurrent \\ninformation.\\nSimple queries \\nand reports.\\nAbility to create \\nsimple menu-\\ndriven \\napplications.\\nProvide key \\nperformance \\nindicators \\nroutinely \\npublished.\\nSmall result sets. Reasonable \\nresponse times.\\nMultidimensiona\\nl data models \\nwith business \\ndimensions and \\nmetrics.\\nStandard user \\ninterface for \\nqueries and \\nreports.\\nAbility to create \\nreports.\\nLimited drill-\\ndown.\\nRoutine analysis \\nwith definite \\nresults. \\nUsually work \\nwith summary \\ndata.  Reasonable \\nresponse times.\\nNormalized data \\nmodels.\\nSpecial \\narchitecture \\nincluding an \\nexploration \\nwarehouse useful.\\nProvision for \\nlarge queries on \\nhuge volumes of \\ndetailed data.\\nA variety of tools \\nto query and \\nanalyze.\\nSupport for long \\nanalysis sessions.\\nUsually large \\nresult sets for \\nstudy and further \\nanalysis.Special data \\nrepositories \\ngetting data feed \\nfrom the \\nwarehouse.\\nNormalized data \\nmodels.\\nDetailed data, \\nsummarized used \\nhardly ever.\\nRange of special \\ndata mining tools, \\nstatistical analysis \\ntools, and data \\nvisualization \\ntools.\\nDiscovery of \\nunknown patterns \\nand relationships.\\nAbility to interpret \\nresults. \\nFigure 14-6 How to provide information.Multidimensional\\ndata models', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f5d83d2c-24a5-4359-a6dc-04af2b04db61', embedding=None, metadata={'page_label': '349', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Queries\\nQuery management ranks high in the provision of information delivery in a data ware-\\nhouse. Because most of the information delivery is through queries, query management isvery important. The entire query process must be managed with utmost care. First, con-sider the features of a managed query environment:\\n/L50539Query initiation, formulation, and results presentation are provided on the client\\nmachine.\\n/L50539Metadata guides the query process.\\n/L50539Ability for the users to navigate easily through the data structures is absolutely es-\\nsential.\\n/L50539Information is pulled by the users, not pushed to them.\\n/L50539Query environment must be flexible to accommodate different classes of users.\\nLet us look at the arena in which queries are being processed. Essentially, there are\\nthree sections in this arena. The first section deals with the users who need the query man-agement facility. The next section is about the types of queries themselves. Finally, youhave the data that resides in the data warehouse repository. This is the data that is used forthe queries. Figure 14-8 shows the query processing arena with the three sections. Pleasenote the features in each section. When you establish the managed query environment,take into account the features and make proper provisions for them.\\nLet us now highlight a few important services to be made available in the managed\\nquery environment.INFORMATION DELIVERY 331\\nUser -driven\\nqueries.\\nReadily available\\ntemplates.\\nPredefined\\nqueries.User -driven\\nreporting.\\nReadily available\\nreport formats.\\nPreformatted\\nreports.Complex\\nqueries. Long\\ninteractive\\nanalysis  sessions.\\nSpeed -of-thought\\nprocessing.\\nSaving of result\\nsets.Data feed\\nto downstream\\ndecision\\nsupport\\napplications\\nvery\\ncommon.\\nControlled, very\\nlimited,\\npredefined\\nqueries.\\nNo ad hoc query\\nfacility.Predefined and\\npreformatted\\nreports through\\napplications.\\nUser -driven\\nreporting very\\nrare.No\\ncomplex  query\\nfacility.\\nNo\\ninteractive\\nanalysis sessions\\npossible.Data feed\\nto downstream\\napplications\\nrare. Only to\\nother\\noperational\\nsystems.QUERIES REPORTS ANALYSIS APPLICATIONS\\nFigure 14-7 Information delivery: comparison between data warehouse and operational systems.OPERATIONAL SYSTEMS DATA WAREHOUSE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5c0b41d9-479c-4cf8-94cc-1bddf0b7146b', embedding=None, metadata={'page_label': '350', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Query Definition. Make it easy to translate the business need into the proper query\\nsyntax.\\nQuery Simplification. Make the complexity of data and query formulation transpar-\\nent to the users. Provide simple views of the data structures showing tables and at-tributes. Make the rules for combining tables and structures easy to use. \\nQuery Recasting. Even simple-looking queries can result in intensive data retrieval\\nand manipulation. Therefore, provide for parsing incoming queries and recastingthem to work more efficiently. \\nEase of Navigation. Use of metadata to browse through the data warehouse, easily\\nnavigating with business terminology and not technical phrases. \\nQuery Execution. Provide ability for the user to submit the query for execution with-\\nout any intervention from IT.\\nResults Presentation. Present results of the query in a variety of ways. \\nAggregate Awareness. Query processing mechanisms must be aware of aggregate fact\\ntables and, whenever necessary, redirect the queries to the aggregate tables for fasterretrieval.\\nQuery Governance. Monitor and intercept runaway queries before they bring down\\nthe data warehouse operations. \\nReports\\nLet us observe the significant features of the reporting environment in this subsection.\\nEveryone is familiar with reports and how they are used. Without repeating what we al-ready know, let us just discuss reporting services by relating these to the data warehouse.332 MATCHING INFORMATION TO THE CLASSES OF USERS\\nDATA\\nQUERIES\\nUSERSDATA \\nWAREHOUSE\\nComplex\\n• User types\\n\\x7f Skill levels\\n\\x7f Number of users\\x7f User information   \\nneeds\\x7f Query types\\n\\x7f Query templates\\n\\x7f Complexity\\n\\x7f Predefined queries\\x7f Data content\\n\\x7f Concurrency\\x7f Volumes\\n\\x7f Responsiveness\\nFigure 14-8 Query processing arena.Simple\\nComplex', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b22ec4d8-a7ce-4ae9-a492-1f51a9c87e02', embedding=None, metadata={'page_label': '351', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='What can you say about the overall defining aspects of a managed reporting environment?\\nConsider the following brief list.\\n/L50539The information is pushed to the user, not pulled by the user as in the case of\\nqueries. Reports are published and the user subscribes to what he or she needs.\\n/L50539Compared to queries, reports are inflexible and predefined.\\n/L50539Most of the reports are preformatted and, therefore, rigid.\\n/L50539The user has less control over the reports received than the queries he or she can for-\\nmulate.\\n/L50539A proper distribution system must be established.\\n/L50539Report production normally happens on the server machine. \\nWhile constructing the reporting environment for your data warehouse, use the follow-\\ning as guidelines:\\nSet of preformatted reports. Provide a library of preformatted reports with clear de-\\nscriptions of the reports. Make it easy for users to browse through the library andselect the reports they need.\\nParameter-driven predefined reports. These give the users more flexibility than the\\npreformatted ones. Users must have the capability to set their own parameters andask for page breaks and subtotals.\\nEasy-to-use report development. When users need new reports in addition to prefor-\\nmatted or predefined reports, they must be able to develop their own reports easilywith a simple report-writer facility. \\nExecution on the server. Run the reports on the server machine to free the client ma-\\nchines for other modes of information delivery.\\nReport scheduling. Users must be able to schedule their reports at a specified time or\\nbased on designated events.\\nPublishing and subscribing. Users must have options to publish the reports they have\\ncreated and allow other users to subscribe and receive copies.\\nDelivery options. Provide various options to deliver reports including mass distribu-\\ntion, e-mail, the Web, automatic fax, and so on. Allow users to choose their ownmethods for receiving the reports. \\nMultiple data manipulation options. Allow the users to ask for calculated metrics,\\npivoting of results by interchanging the column and row variables, adding subtotalsand final totals, changing the sort orders, and showing stoplight-style thresholds. \\nMultiple presentation options. Provide a rich variety of options including graphs, ta-\\nbles, columnar formats, cross-tabs, fonts, styles, sizes, and maps. \\nAdministration of reporting environment. Ensure easy administration to schedule,\\nmonitor, and resolve problems. \\nAnalysis\\nWho are the users seriously interested in analysis? Business strategists, market re-\\nsearchers, product planners, production analysts—in short, all the users we have classifiedas explorers. Because of its rich historical data content, the data warehouse is very wellINFORMATION DELIVERY 333', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5ed425b2-2e55-4b9a-aba8-72bbadbad2a6', embedding=None, metadata={'page_label': '352', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='suited for analysis. It provides these users with the means to search for trends, find corre-\\nlations, and discern patterns. \\nIn one sense, an analysis session is nothing but a session of a series of related queries.\\nThe user might start off with an initial query: What are the first quarter sales totals for thisyear by individual product lines? The user looks at the numbers and is curious about thesag in the sales of two of these product lines. The user then proceeds to drill down by indi-vidual products in those two product lines. The next query is for a breakdown by regionsand then by districts. The analysis continues with comparison with the first quarterly salesof the two prior years. In analysis, there are no set predefined paths. Queries are formulat-ed and executed at the speed of thought. \\nWe have already covered the topic of query processing. Any provisions for query man-\\nagement apply to the queries executed as part of an analysis session. One significant dif-ference is that each query in an analysis session is linked to the previous one. The queriesin an analysis session form a linked series. Analysis is an interactive exercise.\\nAnalysis can become extremely complex, depending on what the explorer is after. The\\nexplorer may take several steps in a winding navigational path. Each step may call forlarge masses of data. The joins may involve several constraints. The explorer may want toview the results in many different formats and grasp the meaning of the results. Complexanalysis falls in the domain of online analytical processing (OLAP). The next chapter istotally devoted to OLAP . There we will discuss complex analysis in detail. \\nApplications\\nA decision support application in relation to the data warehouse is any downstream sys-\\ntem that gets its data feed from the data warehouse. In addition to letting the users accessthe data content of the warehouse directly, some companies create specialized applica-tions for specific groups of users. Companies do this for various reasons. Some of theusers may not be comfortable browsing through the data warehouse and looking for spe-cific information. If the required data is extracted from the data warehouse at periodic in-tervals and specialized applications are built using the extracted data, these users havetheir needs satisfied. \\nHow are the downstream applications different from an application driven with data\\nextracted directly from the operational systems? Building an application with data fromthe warehouse has one major advantage. The data in the data warehouse is already consol-idated, integrated, transformed, and cleansed. Any decision support applications built us-ing individual operational systems directly may not have the enterprise view of the data. \\nA downstream decision support application may just start out to be nothing more\\nthan a set of preformatted and predefined reports. Y ou add a simple menu for the usersto select and run the reports and you have an application that may very well be useful toa number of your users. Executive Information Systems (EIS) are good candidates fordownstream applications. EIS built with data from the warehouse proves to be superiorto its counterparts of more than a decade ago when EIS were based on data from oper-ational systems. \\nA more recent development is data mining, a major type of application that gets data\\nfeeds from the data warehouse. With more vendor products on the market to support datamining, this application becomes more and more prevalent. Data mining deals withknowledge discovery. Please refer to Chapter 17 for ample coverage of data mining ba-sics. 334 MATCHING INFORMATION TO THE CLASSES OF USERS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='847046ee-4c6a-40b2-8ebb-f2c7d530d974', embedding=None, metadata={'page_label': '353', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INFORMATION DELIVERY TOOLS\\nAs we have indicated earlier, the success of your data warehouse rides on the strengths of\\nthe information delivery tools. If the tools are effective, usable, and enticing, your userswill come to the data warehouse often. Y ou have to select the information delivery toolswith great care and thoroughness. We will discuss this very important consideration insufficient detail. \\nInformation delivery tools come in different formats to serve various purposes. The\\nprincipal class of tools comprises query or data access tools. This class of tools enablesthe users to define, formulate, and execute queries and obtain results. Other types are thereport writers or reporting tools for formatting, scheduling, and running reports. Othertools specialize in complex analysis. A few tools combine the different features so thatyour users may learn to use a single tool for queries and reports. More commonly, youwill find more than one information delivery tool used in a single data warehouse envi-ronment. \\nInformation delivery tools typically perform two functions: they translate the user re-\\nquests of queries or reports into SQL statements and send these to the DBMS; they re-ceive results from the data warehouse DBMS, format the result sets in suitable outputs,and present the results to the users. Usually, the requests to the DBMS retrieve and manip-ulate large volumes of data. Compared to the volumes of data retrieved, the result setscontain much lesser data. \\nThe Desktop Environment\\nIn the client–server computing architecture, information delivery tools run in the desktop\\nenvironment. Users initiate the requests on the client machines. When you select thequery tools for your information delivery component, you are choosing software to run onthe client workstations. What are the basic categories of information delivery tools?Grouping the tools into basic categories broadens your understanding of what types oftools are available and what types you need for your users.\\nLet us examine the array of information delivery tools you need to consider for selec-\\ntion. Please study Figure 14-9 carefully. This figure lists the major categories for the desk-top environment and summarizes the use and purpose of each category. Note the purposeof each category. The usage and functions of each category of tools help you match thecategories with the classes of users.\\nMethodology for Tool Selection\\nBecause of the enormous importance of the information delivery tools in a data ware-\\nhouse environment, you must have a well thought out, formalized methodology for select-ing the appropriate tools. A set of tools from certain vendors may be the best for a givenenvironment, but the same set of tools may be a total disaster in another data warehouseenvironment. There is no one-size-fits-all proposition in the tool selection. The tools foryour environment are for your users and must be the most suitable for them. Therefore,before formalizing the methodology for selection, do reconsider the requirements of yourusers.\\nWho are your users? At what organizational levels do they perform? What are the lev-\\nels of their computing proficiency? How do they expect to interact with the data ware-INFORMATION DELIVERY TOOLS 335', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='61f0300f-65d9-4b55-be9d-80a661fceb82', embedding=None, metadata={'page_label': '354', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='house? What are their expectations? How many tourists are there? Are there any explorers\\nat all? Ask all the pertinent questions and explore the answers.\\nAmong the best practices in data warehouse design and development, a formal\\nmethodology ranks among the top. A good methodology certainly includes your user rep-resentatives. Make your users part of the process. Otherwise your tool selection methodol-ogy is doomed to failure. Have the users actively involved in setting the criteria for thetools and also in the evaluation activity itself. Apart from considerations of user prefer-ences, technical compatibility with other components of the data warehouse must also betaken into account. Do not overlook technical aspects.\\nA good formal methodology promotes a staged approach. Divide the tool selection\\nprocess into well-defined steps. For each step, declare the purpose and state the activities.Estimate the time needed to complete each step. Proceed from one stage to the next stage.The activities in each stage depend on the successful completion of the activities in theprevious stage. Figure 14-10 illustrates the stages in the process for selecting informationdelivery tools. \\nThe formal methodology you come up for the selection of tools for your environment\\nmust define the activities in each stage of the process. Please examine the following listsuggesting the types of activities in each stage of the process. Use this list as a guide.\\nForm tool selection team. Include about four or five persons in the team. As informa-\\ntion delivery tools are important, ensure that the executive sponsor is part of theteam. User representatives from the primary subject areas must be on the team.They will provide the user perspective and act as subject matter experts. Havesomeone experienced with information delivery tools on the team. If the data ware-336 MATCHING INFORMATION TO THE CLASSES OF USERS\\nTOOL CATEGORY PURPOSE AND USAGE\\nAd Hoc Query\\nPreformatted \\nReporting\\nComplex \\nAnalysis\\nKnowledge \\nDiscoveryDSS \\nApplicationsEnhanced \\nReportingManaged Query\\nApplication \\nBuilderQuery templates and predefined queries. Users supply input parameters. \\nUsers can receive results on GUI screens or as reports.\\nUsers can define the information needs and compose their own queries. \\nMay use complex templates. Results on screen or reports.\\nUsers input parameters in predefined report formats and submit report jobs \\nto be run. Reports may be run as scheduled or on demand.\\nUsers can create own reports using report writer features. Used for special \\nreports not previously defined. Reports run on demand.\\nUsers write own complex queries. Perform interactive analysis usually in \\nlong sessions. Store intermediate results. Save queries for future use.\\nPre-designed standard decision support applications. May  be customized. \\nExample: Executive Information System. Data from the warehouse. \\nSoftware to build simple downstream applications for decision support \\napplications. Proprietary language component. Usually menu-driven.\\nSet of data mining techniques. Tools used to discover patterns and \\nrelationships not apparent or previously known. \\nFigure 14-9 Information delivery: the desktop environment.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fe598a3b-0d1f-4175-b537-0283fd8389e3', embedding=None, metadata={'page_label': '355', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='house administrator is experienced in this area, let that person lead the team and\\ndrive the selection process.\\nReassess user requirements. Review the user requirements, not in a general way, but\\nspecifically in relation to information delivery. List the classes of users and put eachpotential user in the appropriate class. Describe the expectations and needs of eachof your classes. Document the requirements so that you can match up the require-ments with the features of potential tools.\\nStipulate selection criteria. For each broad group of tools such as query tools or re-\\nporting tools, specify the criteria. Please see the following subsection on Tool Selec-tion Criteria.\\nResearch available tools and vendors. This stage can take a long time, so it is better to\\nget a head start on this stage. Obtain product literature from the vendors. Trade showscan help for getting the first glimpse of the potential tools. The Data WarehousingInstitute is another good source. Although there are a few hundred tools on the mar-ket, narrow the list down to about 25 or less for preliminary research. At this stage,primarily concentrate on the functions and features of the tools on your list.\\nPrepare a long list for consideration. This follows from the research stage. Y our re-\\nsearch will result in the preliminary or long list of potential tools for consideration.For each tool on the preliminary list, document the functions and features. Also,note how these functions and features would match with the requirements.\\nObtain additional information. In this stage, you want to do additional and more in-\\ntensive research on the tools on your preliminary list. Talk to the vendors. Contactthe installations the vendors suggest to you as references.INFORMATION DELIVERY TOOLS 337\\nSelect Team\\nReview Requirements\\nDefine Criteria\\nResearch Tools/Vendors\\nPrepare Long List\\nGet More Information\\nSelect Top Three\\nAttend Product Demos\\nIT to Complete Evaluation\\nUser to Complete Evaluation\\nMake Final SelectionBEGIN\\nFINISHCONTINUE\\nCONTINUE1\\n10\\n112\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nFigure 14-10 Information delivery tools: methodology for selection.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b32dbb9d-f017-463c-ac51-245355fe649e', embedding=None, metadata={'page_label': '356', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Select the top three tools. Select the top three tools as possible candidates. If you are\\nnot very sure of the final outcome, choose more, but not more than five, because ifyou have another long list, it will take a lot longer to go through the rest of the se-lection process. \\nAttend product demonstrations. Now you want to know as much as possible about\\nthe tools on the short list. Call the vendors in for product demonstrations. Y ou maygo to the vendor sites if your computing configuration is not ready for the selectedtools. Ask questions. During the demonstrations, constantly try to match the func-tions of the tools with the requirements of your users.\\nComplete evaluation by IT. IT makes a separate evaluation, mainly for technical\\ncompatibility with your computing environment. Test features such as connectivitywith your DBMS. Verify scalability.\\nComplete evaluation by users. This is a critical stage. User testing and acceptance is\\nvery important. Do not cut this stage short. This stage consists of a sufficient num-ber of hands-on sessions. If it is feasible to prototype the actual usage, do so by allmeans. Especially if two products match the requirements to about the same extent,prototyping may bring out the essential differences.\\nMake the final selection. Y ou are about ready to make the final selection. This stage\\ngives you a chance to reevaluate the tools that come very close to the requirements.Also, in this stage check out the vendors. The tools may be excellent. But the cur-rent vendor could have acquired the tool from another company and the technicalsupport may be inadequate. Or, the vendor may not be stable and enduring. Verifyall the relevant issues about the vendor. Make the final selection, keeping the usersin the loop all the while.\\nAs you might have already realized, the tool selection process could be intense and\\nmay take a considerable length of time. Nevertheless, it must not be taken lightly. It is ad-visable to proceed in distinct stages, keeping the users involved from the beginning to theend of the process.\\nLet us end this sub-section with the following practical tips:\\n/L50539Nominate an experienced member of the team or the data warehouse administrator\\nto lead the team and drive the process.\\n/L50539Keep your users totally involved in the process.\\n/L50539There is no substitute to hands-on evaluation. Do not be satisfied with just vendor\\ndemonstrations. Try the tools yourself.\\n/L50539Consider prototyping a few typical information delivery interactions. Will the tool\\nstand up to the load of multiple users?\\n/L50539It is not easy to combine tools from multiple vendors.\\n/L50539Remember, an information delivery tool must be compatible with the data ware-\\nhouse DBMS.\\n/L50539Continue to keep metadata considerations in the forefront.\\nTool Selection Criteria\\nFrom the discussions on the needs of each class of users, you must have understood the\\ncriteria for selecting information delivery tools. For example, we referred to explorers and338 MATCHING INFORMATION TO THE CLASSES OF USERS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='99d60ca2-1259-4b94-a263-35e39b391908', embedding=None, metadata={'page_label': '357', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='their need for complex analysis. This tells us that the tools for explorers must possess fea-\\ntures suitable for performing complex analysis. For tourists, the tools must be easy and in-tuitive. By now, you have a reasonable insight into the criteria for selecting the informa-tion delivery tools. Y ou have a good grasp of the criteria for selecting tools in the threemain areas of information delivery, namely, reporting, queries, and analysis.\\nLet us now bring our thoughts together and provide a list of general criteria for select-\\ning information delivery tools. This list is applicable to all the three areas. Y ou may use thelist as a guide and prepare your own checklist that is specific to your environment.\\nEase of use. This is perhaps the most important way to make your users happy. Ease of\\nuse is specifically required for query creation, report building, and presentationflexibility.\\nPerformance. Although system performance and response times are less critical in a\\ndata warehouse environment than in an OLTP system, still they rank high on the listof user requirements. Need for acceptable performance spans not only the informa-tion delivery system, but the entire environment.\\nCompatibility. The features of the information delivery tool must be exactly suited to\\nthe class of users it is intended for. For example, OLAP capability is not compatiblewith the class of users called tourists, nor are preformatted reports the precise re-quirement of the explorers.\\nFunctionality. This is an extension of compatibility. The profile of every user class de-\\nmands certain indispensable functions in the tool. For example, miners need a fullrange of functions from data capture through discovery of unknown patterns.\\nIntegrated. More commonly, querying, analyzing, and producing reports may be tied\\ntogether in one user session. The user may start with an initial query whose resultslead to drilling down or other forms of analysis. At the end of the session, the user islikely to capture the final result sets in the form of reports. If this type of usage iscommon in your environment, your information delivery tool must be able to inte-grate different functions.\\nTool administration. Centralized administration makes the task of the information de-\\nlivery administrator easy. The tool must come with utility functions to configureand control the information delivery environment. \\nWeb-enabled. The Internet has become our window into the world. Today’ s data ware-\\nhouses have a big advantage over the ones built before Web technology becamepopular. It is important for the information delivery tool to be able to publish Webpages over the Internet and your company’ s intranet.\\nData security. In most enterprises, safeguarding the warehouse data is as critical as\\nproviding security for data in operational systems. If your environment is a data-sensitive one, the information delivery tools must have security features. \\nData browsing capability. The user must be able to browse the metadata and review\\nthe data definitions and meanings. Also, the tool must present data sets as GUI ob-jects on the screen for the user to choose by clicking on the icons. \\nData selector ability. The tool must provide the user with the means for constructing\\nqueries without having to perform specific joins of tables using technical terminol-ogy and methods.\\nDatabase connectivity. The ability to connect to any of the leading database products\\nis an essential feature needed in the information delivery tool.INFORMATION DELIVERY TOOLS 339', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8b7d4b1f-f717-4000-b2fb-0fe81556a959', embedding=None, metadata={'page_label': '358', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Presentation feature. It is important for the tool to present the result sets in a variety\\nof formats including texts, tabular formats, charts, graphs, maps, and so on.\\nScalability. If your data warehouse is successful, you can be sure of a substantial in-\\ncrease in the number of users within a short time as well as a marked expansion inthe complexity of the information requests. The information delivery tool must bescalable to handle the larger volumes and extra complexity of the requests.\\nV endor dependability. As the data warehousing market matures you will notice many\\nmergers and acquisitions. Also, some companies are likely to go out of business.The selected tool may be the best suited one for your environment, but if the vendoris not stable, you may want to rethink your selection.\\nInformation Delivery Framework\\nIt is time for us to sum up everything we have discussed in this chapter. We classified the\\nusers and came up with standard classes of users. These are the classes into which you canfit every group of your users. Each class of users has specific characteristics when itcomes to information delivery from the data warehouse. Classification of the users leadsus to formalizing the information needs of each class. Once you understand what eachclass of users needs, you are able to derive the ways in which you must provide informa-tion to these classes. Our discussions moved on to the standard methods of informationdelivery and to the selection of information delivery tools. Now please refer to Figure 14-11, which summarizes all the discussions. The figure shows the classes of users. It indi-cates how the various users in the enterprise fit into this classification. The figure alsomatches each class of users with the common tool categories appropriate for the class.The figure brings everything together.340 MATCHING INFORMATION TO THE CLASSES OF USERS\\nFARMERSEXPLORERSOPERATORSMINERSTOURISTS\\nDATA\\nWAREHOUSEExecutives /\\nSr. Managers\\nSupport\\nstaff\\nManagers /\\nAnalystsSkilled\\nanalystsSpecial\\npurpose\\nanalysts\\nTools\\nAd Hoc\\nQuery /\\nComplex\\nAnalysis /\\nApp.\\nBuilderTools\\nManaged\\nQuery  /\\nPre-\\nformatted\\nReportingTools\\nAd Hoc\\nQuery /\\nEnhanced\\nReporting /\\nManaged\\nQueryTools\\nDSS\\nApplications\\n(EIS) /\\nManaged\\nQueryTools\\nKnowledge\\nDiscovery\\n(Data\\nMining)\\nFigure 14-11 Information delivery framework.Support\\nstaff', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cfe6b5ce-bf20-4a79-a2b7-5081d34f035f', embedding=None, metadata={'page_label': '359', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER SUMMARY\\n/L50539Information content and usage in a data warehouse differ greatly from those in oper-\\national systems.\\n/L50539The data warehouse possesses enormous information potential for the overall enter-\\nprise management as well as for individual business areas.\\n/L50539Y ou realize the information potential of the data warehouse through an effective\\nuser–information interface. \\n/L50539Who will use the information? Understand the various users and their needs. The\\nusers may be classified into interesting groups of tourists, operators, farmers, ex-plorers, and miners.\\n/L50539Provide information to each class according to its needs, skills, and business back-\\nground. \\n/L50539Queries, reports, analysis, and applications form the basis for information delivery.\\nEach class of users needs variations of these information delivery methods.\\n/L50539The success of your data warehouse rides on the effectiveness of the end-user infor-\\nmation delivery tools. Carefully choose the tools by applying proven selection crite-ria.\\nREVIEW QUESTIONS\\n1. How does the data warehouse differ from an operational system in usage and val-\\nue?\\n2. Explain briefly how the information from the data warehouse promotes customer\\nrelationship management.\\n3. What are the two basic modes of usage of information from the data warehouse?\\nGive an example for each mode.\\n4. List any five essential features necessary for the user–information interface.5. Who are the power users? How do power users expect to use the data warehouse?6. Who are the users classified as farmers? Name any three characteristics of this\\nclass of data warehouse users. \\n7. List any four essential features of a managed query environment.8. List any four essential features of a managed reporting environment.9. List and explain five criteria for selecting information delivery tools for your data\\nwarehouse. \\n10. Describe in less than four sentences your understanding of the information deliv-\\nery framework.\\nEXERCISES\\n1. Match the columns:\\n1. information discovery mode A. EIS\\n2. data warehouse tourists B. needs intuitive interfaceEXERCISES 341', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='570ffb19-9ab1-42f5-99b8-bd602eda7a98', embedding=None, metadata={'page_label': '360', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. query recasting C. plan, execute, assess\\n4. data warehouse explorers D. confirm hypothesis5. downstream application E. information pull technique6. overall enterprise management F . analyze large data volumes 7. verification mode G. need status of indicators8. casual user H. highly random access 9. data warehouse miners I. parse and improve query10. queries J. data mining \\n2. Compare the usage and value of information in the data warehouse with those in\\noperational systems. Explain the major differences. Discuss and give examples.\\n3. Examine the potential users of your data warehouse. Can you classify the users into\\ncasual users, regular users, and power users? If this method of simple classificationis inadequate, discuss how your users may be classified. \\n4. Among the potential users in your data warehouse, whom can you classify as\\ntourists? What are the characteristics of these tourists? How can you provide infor-mation for the tourists in your organization?\\n5. What do you understand about the information delivery framework in the data\\nwarehouse environment? As an end-user information delivery specialist, discusshow you will establish such a framework for a heathcare maintenance organization(HMO).342 MATCHING INFORMATION TO THE CLASSES OF USERS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5fb7fc3e-1831-46c5-8a45-1910fd1875b2', embedding=None, metadata={'page_label': '361', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 15\\nOLAP IN THE DATA WAREHOUSE \\nCHAPTER OBJECTIVES\\n/L50539Perceive the unqualified demand for online analytical processing (OLAP) and un-\\nderstand what drives this demand\\n/L50539Review the major features and functions of OLAP in detail \\n/L50539Grasp the intricacies of dimensional analysis and learn the meanings of hypercubes,\\ndrill-down and roll-up, and slice-and-dice\\n/L50539Examine the different OLAP models and determine which model is suitable for\\nyour environment\\n/L50539Consider OLAP implementation by studying the steps and the tools\\nIn the earlier chapters we mentioned online analytical processing (OLAP) in passing.\\nY ou had a glimpse of OLAP when we discussed the information delivery methods. Y ouhave some idea of what OLAP is and how it is used for complex analysis. As the name im-plies, OLAP has to do with the processing of data as it is manipulated for analysis. Thedata warehouse provides the best opportunity for analysis and OLAP is the vehicle forcarrying out involved analysis. The data warehouse environment is also best for data ac-cess when analysis is carried out.\\nWe now have the chance to explore OLAP in sufficient depth. In today’ s data ware-\\nhousing environment, with such tremendous progress in analysis tools from various ven-dors, you cannot have a data warehouse without OLAP . It is unthinkable. Therefore,throughout this chapter, look out for the important topics. \\nFirst, you have to perceive what OLAP is and why it is absolutely essential. This will\\nhelp you to better understand the features and functions of OLAP . We will discuss the ma-jor features and functions so that your grasp of OLAP may be firmed up. There are twomajor models for OLAP . Y ou should know which model is most suitable for your comput-ing and user environments. We will highlight the significance of each model, learn how to\\n343Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4dfce857-c692-4a7f-8155-0976910007a6', embedding=None, metadata={'page_label': '362', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='implement OLAP in your data warehouse environment, investigate OLAP tools, and find\\nout how to evaluate and get them for your users. Finally, we will discuss the implementa-tion steps for OLAP .\\nDEMAND FOR ONLINE ANALYTICAL PROCESSING\\nRecall our discussions in Chapter 2 of the top-down and bottom-up approaches for build-\\ning a data warehouse. In the top-down approach, you build the overall corporate-widedata repository using the entity-relationship (E-R) modeling technique. This enterprise-wide data warehouse feeds the departmental data marts that are designed using the dimen-sional modeling technique. In the bottom-up approach, you build several data marts usingthe dimensional modeling technique and the collection of these data marts forms the datawarehouse environment for your company. Each of these two approaches has its advan-tages and shortcomings.\\nY ou also learned about a practical approach to building a conglomeration of super-\\nmarts with conformed and standardized data content. While adopting this approach, firstyou plan and define the requirements at the corporate level, build the infrastructure for thecomplete warehouse, and then implement one supermart at a time in a priority sequence.The supermarts are designed using the dimensional modeling technique. \\nAs we have seen, a data warehouse is meant for performing substantial analysis using\\nthe available data. The analysis leads to strategic decisions that are the major reasons forbuilding data warehouses in the first place. For performing meaningful analysis, datamust be cast in a way suitable for analysis of the values of key indicators over time alongbusiness dimensions. Data structures designed using the dimensional modeling techniquesupport such analysis. \\nIn all the three approaches referred to above, the data marts rest on the dimensional\\nmodel. Therefore, these data marts must be able to support dimensional analysis. In prac-tice, these data marts seem to be adequate for basic analysis. However, in today’ s businessconditions, we find that users need to go beyond such basic analysis. They must have thecapability to perform far more complex analysis in less time. Let us examine how the tra-ditional methods of analysis provided in a data warehouse are not sufficient and perceivewhat exactly is demanded by the users to stay competitive and to expand. \\nNeed for Multidimensional Analysis\\nLet us quickly review the business model of a large retail operation. If you just look at dai-\\nly sales, you soon realize that the sales are interrelated to many business dimensions. Thedaily sales are meaningful only when they are related to the dates of the sales, the prod-ucts, the distribution channels, the stores, the sales territories, the promotions, and a fewmore dimensions. Multidimensional views are inherently representative of any businessmodel. Very few models are limited to three dimensions or less. For planning and makingstrategic decisions, managers and executives probe into business data through scenarios.For example, they compare actual sales against targets and against sales in prior periods.They examine the breakdown of sales by product, by store, by sales territory, by promo-tion, and so on. \\nDecision makers are no longer satisfied with one-dimensional queries such as “How\\nmany units of Product A did we sell in the store in Edison, New Jersey?” Consider the fol-344 OLAP IN THE DATA WAREHOUSE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='47e1097d-5c5f-42bd-84ba-10038c8ecd51', embedding=None, metadata={'page_label': '363', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='lowing more useful query: How much revenue did the new Product X generate during the\\nlast three months, broken down by individual months, in the South Central territory, by in-dividual stores, broken down by promotions, compared to estimates, and compared to theprevious version of the product? The analysis does not stop with this single multidimen-sional query. The user continues to ask for further comparisons to similar products, com-parisons among territories, and views of the results by rotating the presentation betweencolumns and rows.\\nFor effective analysis, your users must have easy methods of performing complex\\nanalysis along several business dimensions. They need an environment that presents amultidimensional view of data, providing the foundation for analytical processing througheasy and flexible access to information. Decision makers must be able to analyze dataalong any number of dimensions, at any level of aggregation, with the capability of view-ing results in a variety of ways. They must have the ability to drill down and roll up alongthe hierarchies of every dimension. Without a solid system for true multidimensionalanalysis, your data warehouse is incomplete. \\nIn any analytical system, time is a critical dimension. Hardly any query is executed\\nwithout having time as one of the dimensions along which analysis is performed. Further,time is a unique dimension because of its sequential nature—November always comes af-ter October. Users monitor performance over time, as for example, performance thismonth compared to last month, or performance this month compared with performancethe same month last year. \\nAnother point about the uniqueness of the time dimension is the way in which the hier-\\narchies of the dimension work. A user may look for sales in March and may also look forsales for the first four months of the year. In the second query for sales for the first fourmonths, the implied hierarchy at the next higher level is an aggregation taking into ac-count the sequential nature of time. No user looks for sales of the first four stores or thelast three stores. There is no implied sequence in the store dimension. True analytical sys-tems must recognize the sequential nature of time.\\nFast Access and Powerful Calculations\\nWhether a user’ s request is for monthly sales of all products along all geographical re-\\ngions or for year-to-date sales in a region for a single product, the query and analysis sys-tem must have consistent response times. Users must not be penalized for the complexityof their analysis. Both the size of the effort to formulate a query or the amount of time toreceive the result sets must be consistent irrespective of the query types. \\nLet us take an example to understand how speed of the analysis process matters to\\nusers. Imagine a business analyst looking for reasons why profitability dipped sharply inthe recent months in the entire enterprise. The analyst starts this analysis by querying forthe overall sales for the last five months for the entire company, broken down by individ-ual months. The analyst notices that although the sales do not show a drop, there is a sharpreduction in profitability for the last three months. The analysis proceeds further when theanalyst wants to find out which countries show reductions. The analyst requests a break-down of sales by major worldwide regions and notes that the European region is responsi-ble for the reduction in profitability. Now the analyst senses that clues are becoming morepronounced and looks for a breakdown of the European sales by individual countries. Theanalyst finds that the profitability has increased for a few countries, decreased sharply forsome other countries, and been stable for the rest. At this point, the analyst introduces an-DEMAND FOR ONLINE ANALYTICAL PROCESSING 345', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='28f918ba-e2b2-4c7c-98fe-8136879d6cfc', embedding=None, metadata={'page_label': '364', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='other dimension into the analysis. Now the analyst wants the breakdown of profitability\\nfor the European countries by country, month, and product. This step brings the analystcloser to the reason for the decline in the profitability. The analyst observes that the coun-tries in the European Union (EU) show very sharp declines in profitability for the last twomonths. Further queries reveal that manufacturing and other direct costs remain at theusual levels but the indirect costs have shot up. The analyst is now able to determine thatthe decline is due to the additional tax levies on some products in the EU. The analyst hasalso determined the exact effect of the levies so far. Strategic decisions follow on how todeal with the decline in profitability.\\nNow please look at Figure 15-1 showing the steps through the single analysis session.\\nHow many steps are there? Many steps, but a single analysis session and train of thought.Each step in this train of thought constitutes a query. The analyst formulates each query,executes it, waits for the result set to appear on the screen, and studies the result set. Eachquery is interactive because the result set from one query forms the basis for the nextquery. In this manner of querying, the user cannot maintain the train of thought unless themomentum is preserved. Fast access is absolutely essential for an effective analytical pro-cessing environment.\\nDid you notice that none of the queries in the above analysis session included any seri-\\nous calculations? This is not typical. In a real-world analysis session, many of the queriesrequire calculations, sometimes complex calculations. What is the implication here? Aneffective analytical processing environment must not only be fast and flexible, but it mustalso support complex and powerful calculations. \\nWhat follows is a list of typical calculations that get included in the query requests:346 OLAP IN THE DATA WAREHOUSE\\nSharp\\nenterprise -wide\\nprofitability dipSales OK,\\nprofitability\\ndown, last 3\\nmonths\\nSharp\\nreduction in\\nEuropean\\nregion\\nIncrease in a\\nfew countries,\\nflat in others,\\nsharp decline in\\nsome\\nSharp decline in\\nEU countries, last\\n2 monthsDirect costs\\nOK, indirect\\ncosts upAdditional tax\\non some\\nproducts in EU.Countrywide monthly sales for\\nlast 5 months ???\\nDisplay of direct and indirect\\ncosts for Euro Union countriesBreakdown of European Sales\\nby countries, by products ???Breakdown of European sales\\nby countries ???Monthly sales breakdown by\\nworldwide regions ???Thought  process based on each query result Query sequence in the analysis session\\nBegin\\nanalysis\\nEnd\\nanalysis\\nFigure 15-1 Query steps in an analysis session.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8be07dd6-265c-4c55-a40c-7ac2d1182694', embedding=None, metadata={'page_label': '365', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Roll-ups to provide summaries and aggregations along the hierarchies of the dimen-\\nsions.\\n/L50539Drill-downs from the top level to the lowest along the hierarchies of the dimensions,\\nin combinations among the dimensions.\\n/L50539Simple calculations, such as computation of margins (sales minus costs).\\n/L50539Share calculations to compute the percentage of parts to the whole.\\n/L50539Algebraic equations involving key performance indicators.\\n/L50539Moving averages and growth percentages.\\n/L50539Trend analysis using statistical methods.\\nLimitations of Other Analysis Methods\\nY ou now have a fairly good grip on the types of requirements of users to execute queries\\nand perform analysis. First and foremost, the information delivery system must be able topresent multidimensional views of the data. Then the information delivery system mustenable the users to use the data by analyzing it along multiple dimensions and their hierar-chies in a myriad of ways. And this facility must be fast. It must be possible for the usersto perform complex calculations.\\nLet us understand why the traditional tools and methods are not up to the task when it\\ncomes to complex analysis and calculations. What information methods are we familiarwith? Of course, the earliest method was the medium of reports. Then came spreadsheetswith all their functionality and features. SQL has been the accepted interface for retriev-ing and manipulating data from relational databases. These methods are used in OLTPsystems and in data warehouse environments. Now, when we discuss multidimensionalanalysis and complex calculations, how suitable are these traditional methods?\\nFirst, let us look at the characteristics of the OLTP and data warehouse environments.\\nWhen we mention the data warehouse environment here, we are not referring to heavymultidimensional analysis and complex calculations. We are only referring to the environ-ment with simple queries and routine reports. Please see Figure 15-2 showing the charac-teristics of the OLTP and the basic data warehouse environments as they relate to infor-mation delivery needs.\\nNow consider information retrieval and manipulation in these two environments. What\\nare the standard methods of information delivery? Reports, spreadsheets, and online dis-plays. What is the standard data access interface? SQL. Let us review these and determineif they are adequate for multidimensional analysis and complex calculations. \\nReport writers provide two key functions: the ability to point and click for generating\\nand issuing SQL calls, and the capability to format the output reports. However, reportwriters do not support multidimensionality. With basic report writers, you cannot drilldown to lower levels in the dimensions. That will have to come from additional reports.Y ou cannot rotate the results by switching rows and columns. The report writers do notprovide aggregate navigation. Once the report is formatted and run, you cannot alter thepresentation of the result data sets. \\nIf report writers are not the tools or methods we are looking for, how about spread-\\nsheets for calculations and the other features needed for analysis? Spreadsheets, whenthey first appeared, were positioned as analysis tools. Y ou can perform “what if ” analysiswith spreadsheets. When you modify the values in some cells, the values in other relatedcells automatically change. What about aggregations and calculations? Spreadsheets withDEMAND FOR ONLINE ANALYTICAL PROCESSING 347', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eb8b85cf-21b6-4be4-8908-a42ba03ec6f5', embedding=None, metadata={'page_label': '366', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='their add-in tools can perform some forms of aggregations and also do a variety of calcu-\\nlations. Third party tools have also enhanced spreadsheet products to present data inthree-dimensional formats. Y ou can view rows, columns, and pages on spreadsheets. Forexample, the rows can represent products, the columns represent stores, and the pagesrepresent the time dimension in months. Modern spreadsheet tools offer pivot tables or n-way cross-tabs.\\nEven with enhanced functionality using add-ins, spreadsheets are still very cumbersome\\nto use. Take an analysis involving the four dimensions of store, product, promotion, andtime. Let us say each dimension contains an average of five hierarchical levels. Now try tobuild an analysis to retrieve data and present it as spreadsheets showing all the aggregationlevels and multidimensional views, and also using even simple calculations. Y ou can verywell imagine how much effort it would take for this exercise. Now what if your user wantsto change the navigation and do different roll-ups and drill-downs. The limitations ofspreadsheets for multidimensional analysis and complex calculations are quite evident.\\nLet us now turn our attention to SQL (Structured Query Language). Although it might\\nhave been the original goal of SQL to be the end-user query language, now everyoneagrees that the language is too abstruse even for sophisticated users. Third-party productsattempt to extend the capabilities of SQL and hide the syntax from the users. Users canformulate their queries through GUI point-and-click methods or by using natural lan-guage syntax. Nevertheless, SQL vocabulary is ill-suited for analyzing data and exploringrelationships. Even basic comparisons prove to be difficult in SQL.\\nMeaningful analysis such as market exploration and financial forecasting typically in-\\nvolve retrieval of large quantities of data, performing calculations, and summarizing thedata on the fly. Perhaps, even the detailed analysis may be achieved by using SQL for re-348 OLAP IN THE DATA WAREHOUSE\\nCHARACTERISTICS OLTP SYSTEMS DATA WAREHOUSE\\nAnalytical capabilities\\nData for a single session\\nSize of result set\\nResponse time\\nData granularity\\nData currency\\nAccess method\\nBasic motivation\\nData model\\nOptimization of database\\nUpdate frequency\\nScope of user interactionVery low\\nVery limited\\nSmall\\nVery fast\\nDetail\\nCurrent \\nPredefined\\nCollect and input data\\nDesign for data updates\\nFor transactions\\nVery frequent\\nSingle transactionsModerate\\nSmall to medium size\\nLarge\\nFast to moderate\\nDetail and summary\\nCurrent and historical\\nPredefined and ad hoc\\nProvide information\\nDesign for queries\\nFor analysis\\nGenerally read-only\\nThroughout data content\\nFigure 15-2 OLTP and data warehouse environments.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='63c346fb-b219-446b-8c9d-44fc05b7bdfa', embedding=None, metadata={'page_label': '367', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='trieval and spreadsheets for presenting the results. But here is the catch: in a real-world\\nanalysis session, many queries follow one after the other. Each query may translate into anumber of intricate SQL statements, with each of the statements likely to invoke full tablescans, multiple joins, aggregations, groupings, and sorting. Analysis of the type we arediscussing requires complex calculations and handling time series data. SQL is notablyweak in these areas. Even if you can imagine an analyst accurately formulating such com-plex SQL statements, the overhead on the systems would still be enormous and seriouslyimpact the response times.\\nOLAP is the Answer\\nUsers certainly need the ability to perform multidimensional analysis with complex calcu-\\nlations, but we find that the traditional tools of report writers, query products, spread-sheets, and language interfaces are distressfully inadequate. What is the answer? Clearly,the tools being used in the OLTP and basic data warehouse environments do not match upto the task. We need different set of tools and products that are specifically meant for seri-ous analysis. We need OLAP in the data warehouse.\\nIn this chapter, we will thoroughly examine the various aspects of OLAP . We will come\\nup with formal definitions and detailed characteristics. We will highlight all the featuresand functions. We will explore the different OLAP models. But now that you have an ini-tial appreciation for OLAP , let us list the basic virtues of OLAP to justify our proposition.\\n/L50539Enables analysts, executives, and managers to gain useful insights from the presen-\\ntation of data.\\n/L50539Can reorganize metrics along several dimensions and allow data to be viewed from\\ndifferent perspectives.\\n/L50539Supports multidimensional analysis.\\n/L50539Is able to drill down or roll up within each dimension.\\n/L50539Is capable of applying mathematical formulas and calculations to measures.\\n/L50539Provides fast response, facilitating speed-of-thought analysis.\\n/L50539Complements the use of other information delivery techniques such as data mining.\\n/L50539Improves the comprehension of result sets through visual presentations using\\ngraphs and charts.\\n/L50539Can be implemented on the Web. \\n/L50539Designed for highly interactive analysis.\\nEven at this stage, you will further appreciate the nature and strength of OLAP by\\nstudying a typical OLAP session (see Figure 15-3). The analyst starts with a query re-questing a high-level summary by product line. Next, the user moves to drilling down fordetails by year. In the following step, the analyst pivots the data to view totals by yearrather than totals by product line. Even in such a simple example, you observe the powerand features of OLAP .\\nOLAP Definitions and Rules\\nWhere did the term OLAP originate? We know that multidimensionality is at the core of\\nOLAP systems. We have also mentioned some other basic features of OLAP . Is it a vagueDEMAND FOR ONLINE ANALYTICAL PROCESSING 349', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='09a2128a-432a-47a3-8809-04a955a23811', embedding=None, metadata={'page_label': '368', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='collection of complex factors for serious analysis? Is there a formal definition and a set of\\nfundamental guidelines identifying OLAP systems? \\nThe term OLAP or online analytical processing was introduced in a paper entitled\\n“Providing On-Line Analytical Processing to User Analysts,” by Dr. E. F . Codd, the ac-knowledged “father” of the relational database model. The paper, published in 1993, de-fined 12 rules or guidelines for an OLAP system. Later, in 1995, six additional rules wereincluded. We will discuss these rules. Before that, let us look for a short and precise defi-nition for OLAP . Such a succinct definition comes from the OLAP council, which pro-vides membership, sponsors research, and promotes the use of OLAP . Here is the defini-tion:\\nOn-Line Analytical Processing (OLAP) is a category of software technology that enables an-\\nalysts, managers and executives to gain insight into data through fast, consistent, interactiveaccess in a wide variety of possible views of information that has been transformed from rawdata to reflect the real dimensionality of the enterprise as understood by the user.\\nThe definition from the OLAP council contains all the key ingredients. Speed, consis-\\ntency, interactive access, and multiple dimensional views—all of these are principal ele-ments. As one trade magazine described it in 1995, OLAP is a fancy term for multidimen-sional analysis. \\nThe guidelines proposed by Dr. Codd form the yardstick for measuring any sets of\\nOLAP tools and products. A true OLAP system must conform to these guidelines. When350 OLAP IN THE DATA WAREHOUSE\\nFigure 15-3 Simple OLAP session.LINE TOTAL SALES\\nClothing $12,836,450\\nElectronics $16,068,300\\nVideo $21,262,190\\nKitchen $17,704,400\\nAppliances $19,600,800\\nTotal $87,472,140\\nLINE 1998 1999 2000 TOTAL\\nClothing $3,457,000 $3,590,050 $5,789,400 $12,836,450\\nElectronics $5,894,800 $4,078,900 $6,094,600 $16,068,300\\nVideo $7,198,700 $6,057,890 $8,005,600 $21,262,190\\nKitchen $4,875,400 $5,894,500 $6,934,500 $17,704,400\\nAppliances $5,947,300 $6,104,500 $7,549,000 $19,600,800\\nTotal $27,373,200 $25,725,840 $34,373,100 $87,472,140\\nYEAR Clothing Electronics Video Kitchen Appliances TOTAL\\n1998 $3,457,000 $5,894,800 $7,198,700 $4,875,400 $5,947,300 $27,373,200\\n1999 $3,590,050 $4,078,900 $6,057,890 $5,894,500 $6,104,500 $25,725,840\\n2000 $5,789,400 $6,094,600 $8,005,600 $6,934,500 $7,549,000 $34,373,100\\nTotal $12,836,450 $16,068,300 $21,262,190 $17,704,400 $19,600,800 $87,472,140High level \\nsummary by \\nproduct lineDrill down \\nby year\\nRotate \\ncolumns to \\nrows1\\n2\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='64d3de8f-44bd-4d84-a935-0ffa6753faf8', embedding=None, metadata={'page_label': '369', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='your project team is looking for OLAP tools, it can prioritize these guidelines and select\\ntools that meet the set of criteria at the top of your priority list. First, let us consider theinitial twelve guidelines for an OLAP system:\\nMultidimensional Conceptual View. Provide a multidimensional data model that is\\nintuitively analytical and easy to use. Business users’ view of an enterprise is multi-dimensional in nature. Therefore, a multidimensional data model conforms to howthe users perceive business problems. \\nTransparency. Make the technology, underlying data repository, computing architec-\\nture, and the diverse nature of source data totally transparent to users. Such trans-parency, supporting a true open system approach, helps to enhance the efficiencyand productivity of the users through front-end tools that are familiar to them.\\nAccessibility. Provide access only to the data that is actually needed to perform the\\nspecific analysis, presenting a single, coherent, and consistent view to the users.The OLAP system must map its own logical schema to the heterogeneous physicaldata stores and perform any necessary transformations. \\nConsistent Reporting Performance. Ensure that the users do not experience any sig-\\nnificant degradation in reporting performance as the number of dimensions or thesize of the database increases. Users must perceive consistent run time, responsetime, or machine utilization every time a given query is run.\\nClient/Server Architecture. Conform the system to the principles of client/server ar-\\nchitecture for optimum performance, flexibility, adaptability, and interoperability.Make the server component sufficiently intelligent to enable various clients to be at-tached with a minimum of effort and integration programming.\\nGeneric Dimensionality. Ensure that every data dimension is equivalent in both struc-\\nture and operational capabilities. Have one logical structure for all dimensions. Thebasic data structure or the access techniques must not be biased toward any singledata dimension.\\nDynamic Sparse Matrix Handling. Adapt the physical schema to the specific analyt-\\nical model being created and loaded that optimizes sparse matrix handling. Whenencountering a sparse matrix, the system must be able to dynamically deduce thedistribution of the data and adjust the storage and access to achieve and maintainconsistent level of performance.\\nMultiuser Support. Provide support for end users to work concurrently with either the\\nsame analytical model or to create different models from the same data. In short,provide concurrent data access, data integrity, and access security.\\nUnrestricted Cross-dimensional Operations. Provide ability for the system to recog-\\nnize dimensional hierarchies and automatically perform roll-up and drill-down op-erations within a dimension or across dimensions. Have the interface language al-low calculations and data manipulations across any number of data dimensions,without restricting any relations between data cells, regardless of the number ofcommon data attributes each cell contains.\\nIntuitive Data Manipulation. Enable consolidation path reorientation (pivoting),\\ndrill-down and roll-up, and other manipulations to be accomplished intuitively anddirectly via point-and-click and drag-and-drop actions on the cells of the analyticalmodel. Avoid the use of a menu or multiple trips to a user interface. DEMAND FOR ONLINE ANALYTICAL PROCESSING 351', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='11709ba1-0be1-467d-83af-5944cd1d3be2', embedding=None, metadata={'page_label': '370', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Flexible Reporting. Provide capabilities to the business user to arrange columns,\\nrows, and cells in a manner that facilitates easy manipulation, analysis, and synthe-sis of information. Every dimension, including any subsets, must be able to be dis-played with equal ease. \\nUnlimited Dimensions and Aggregation Levels. Accommodate at least fifteen,\\npreferably twenty, data dimensions within a common analytical model. Each ofthese generic dimensions must allow a practically unlimited number of user-definedaggregation levels within any given consolidation path.\\nIn addition to these twelve basic guidelines, also take into account the following re-\\nquirements, not all distinctly specified by Dr. Codd.\\nDrill-through to Detail Level. Allow a smooth transition from the multidimensional,\\npreaggregated database to the detail record level of the source data warehouserepository.\\nOLAP Analysis Models. Support Dr. Codd’ s four analysis models: exegetical (or de-\\nscriptive), categorical (or explanatory), contemplative, and formulaic.\\nTreatment of Nonnormalized Data. Prohibit calculations made within an OLAP sys-\\ntem from affecting the external data serving as the source.\\nStoring OLAP Results. Do not deploy write-capable OLAP tools on top of transac-\\ntional systems.\\nMissing Values. Ignore missing values, irrespective of their source.\\nIncremental Database Refresh. Provide for incremental refreshes of the extracted\\nand aggregated OLAP data.\\nSQL Interface. Seamlessly integrate the OLAP system into the existing enterprise en-\\nvironment.\\nOLAP Characteristics\\nLet us summarize in simple terms what we have covered so far. We explored why the busi-\\nness users absolutely need online analytical processing. We examined why the other meth-ods of information delivery do not satisfy the requirements for multidimensional analysiswith powerful calculations and fast access. We discussed how OLAP is the answer to sat-isfy these requirements. We reviewed the definitions and authoritative guidelines for theOLAP system.\\nBefore we get into a more detailed discussion of the major features of OLAP systems,\\nlet us list the most fundamental characteristics in plain language. OLAP systems\\n/L50539let business users have a multidimensional and logical view of the data in the data\\nwarehouse,\\n/L50539facilitate interactive query and complex analysis for the users,\\n/L50539allow users to drill down for greater details or roll up for aggregations of metrics\\nalong a single business dimension or across multiple dimensions,\\n/L50539provide ability to perform intricate calculations and comparisons, and\\n/L50539present results in a number of meaningful ways, including charts and graphs.352 OLAP IN THE DATA WAREHOUSE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d1edce6d-9e89-4d25-a3d7-20f6fcdc6dd3', embedding=None, metadata={'page_label': '371', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='MAJOR FEATURES AND FUNCTIONS\\nVery often, you are faced with the question of whether OLAP is not just data ware-\\nhousing in a nice wrapper? Can you not consider online analytical processing as just aninformation delivery technique and nothing more? Is it not another layer in the datawarehouse, providing interface between the data and the users? In some sense, OLAP isan information delivery system for the data warehouse. But OLAP is much more thanthat. A data warehouse stores data and provides simpler access to the data. An OLAPsystem complements the data warehouse by lifting the information delivery capabilitiesto new heights. \\nGeneral Features\\nIn this section, we will pay special attention to a few major features and functions of\\nOLAP systems. Y ou will gain greater insight into dimensional analysis, find deeper mean-ings about the necessity for drill-downs and roll-ups during analysis sessions and gaingreater appreciation for the role of slicing and dicing operations in analysis. Before get-ting into greater details about these, let us recapitulate the general features of OLAP .Please go to Figure 15-4 and note the summary. Also note the distinction between basicfeatures and advanced features. The list shown in the figure includes the general featuresyou observe in practice in most OLAP environments. Please use the list as a quick check-list of features your project team must consider for your OLAP system.\\nDimensional Analysis\\nBy this time, you are perhaps tired of the term “dimensional analysis.” We had to use the\\nterm a few times so far. Y ou have been told that dimensional analysis is a strong suit in theMAJOR FEATURES AND FUNCTIONS 353\\nBASIC  FEATURES\\nADVANCED  FEATURESMultidimensional \\nanalysisConsistent \\nperformanceFast response times \\nfor interactive queries\\nDrill-down and     \\nroll-upNavigation in and out \\nof detailsSlice-and-dice or \\nrotation\\nMultiple view    \\nmodesEasy               \\nscalabilityTime intelligence (year-\\nto-date, fiscal period)\\nPowerful  \\ncalculationsCross-dimensional \\ncalculationsPre-calculation or \\npre-consolidation\\nDrill-through across \\ndimensions or detailsSophisticated \\npresentation & displaysCollaborative \\ndecision making\\nDerived data values \\nthrough formulasApplication of alert \\ntechnology Report generation with \\nagent technology\\nFigure 15-4 General features of OLAP .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c54f016d-2805-4672-9596-b0ab9e856863', embedding=None, metadata={'page_label': '372', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='arsenal of OLAP . Any OLAP system devoid of multidimensional analysis is utterly use-\\nless. So try to get a clear picture of the facility provided in OLAP systems for dimension-al analysis.\\nLet us begin with a simple STAR schema. This STAR schema has three business di-\\nmensions, namely, product, time, and store. The fact table contains sales. Please see Fig-ure 15-5 showing the schema and a three-dimensional representation of the model as acube, with products on the X-axis, time on the Y -axis, and stores on the Z-axis. What arethe values represented along each axis? For example, in the STAR schema, time is one ofthe dimensions and month is one of the attributes of the time dimension. Values of this at-tribute month are represented on the Y -axis. Similarly, values of the attributes productname and store name are represented on the other two axes. \\nThis schema with just three business dimensions does not even look like a star.\\nNevertheless, it is a dimensional model. From the attributes of the dimension tables,pick the attribute product name from the product dimension, month from the time di-mension, and store name from the store dimension. Now look at the cube representingthe values of these attributes along the primary edges of the physical cube. Go furtherand visualize the sales for coats in the month of January at the New Y ork store to be atthe intersection of the three lines representing the product: coats, month: January, andstore: New Y ork. \\nIf you are displaying the data for sales along these three dimensions on a spreadsheet,\\nthe columns may display the product names, the rows the months, and pages the dataalong the third dimension of store names. See Figure 15-6 showing a screen display of apage of this three-dimensional data. \\nThe page displayed on the screen shows a slice of the cube. Now look at the cube and\\nmove along a slice or plane passing through the point on the Z-axis representing store:New Y ork. The intersection points on this slice or plane relate to sales along product and354 OLAP IN THE DATA WAREHOUSE\\nProduct Key \\nTime Key      \\nStore Key \\nFixed Costs  \\nVariable Costs \\nIndirect Sales   \\nDirect Sales \\nProfit MarginSALES FACTSSTOREPRODUCT\\nTIMEStore Key      \\nStore Name \\nTerritory      \\nRegion\\nTime Key       \\nDate              \\nMonth        \\nQuarter           \\nYearProduct Key  \\nProduct Name \\nSub-category   \\nCategory    \\nProduct Line  \\nDepartment\\nMonthsStores\\nProductsCoats, January, New York\\n550\\nFigure 15-5    Simple STAR schema.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9b05195e-e395-4cd9-8f5f-fb7fd73afc31', embedding=None, metadata={'page_label': '373', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='time business dimensions for store: New Y ork. Try to relate these sale numbers to the slice\\non the cube representing store: New Y ork.\\nNow we have a way of depicting three business dimensions and a single fact on a two-\\ndimensional page and also on a three-dimensional cube. The numbers in each cell on thepage are the sale numbers. What could be the types of multidimensional analysis on thisparticular set of data? What types of queries could be run during the course of analysissessions? Y ou could get sale numbers along the hierarchies of a combination of the threebusiness dimensions of product, store, and time. Y ou could perform various types ofthree-dimensional analysis of sales. The results of queries during analysis sessions will bedisplayed on the screen with the three dimensions represented in columns, rows, andpages. The following is a sample of simple queries and the result sets during a multidi-mensional analysis session.\\nQuery\\nDisplay the total sales of all products for past five years in all stores.\\nDisplay of Results\\nRows: Y ear numbers 2000, 1999, 1998, 1997, 1996\\nColumns: Total Sales for all products\\nPage: One store per page\\nQuery\\nCompare total sales for all stores, product by product, between years 2000 and 1999. \\nDisplay of Results\\nRows: Y ear numbers 2000, 1999; difference; percentage increase or decrease\\nColumns: One column per product, showing all products\\nPage: All stores MAJOR FEATURES AND FUNCTIONS 355\\nCOLUMNS : PRODUCT dimensionProductsROWS : TIME dimension\\nMonthsStore: New York\\nPAGES : STORE dimension\\nHats Coats Jackets Dresses Shirts Slacks\\nJan 200 550 350 500 520 490\\nFeb 210 480 390 510 530 500\\nMar 190 480 380 480 500 470\\nApr 190 430 350 490 510 480\\nMay 160 530 320 530 550 520\\nJun 150 450 310 540 560 330\\nJul 130 480 270 550 570 250\\nAug 140 570 250 650 670 230\\nSep 160 470 240 630 650 210\\nOct 170 480 260 610 630 250\\nNov 180 520 280 680 700 260\\nDec 200 560 320 750 770 310\\nFigure 15-6 A Three-dimensional display.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4a8b8435-ac69-4233-baf2-0b1e5c172cf2', embedding=None, metadata={'page_label': '374', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Query\\nShow comparison of total sales for all stores, product by product, between years\\n2000 and 1999 only for those products with reduced sales. \\nDisplay of Results\\nRows: Y ear numbers 2000, 1999; difference; percentage decrease\\nColumns: One column per product, showing only the qualifying products\\nPage: All stores \\nQuery\\nShow comparison of sales by individual stores, product by product, between years\\n2000 and 1999 only for those products with reduced sales. \\nDisplay of Results\\nRows: Y ear numbers 2000, 1999; difference; percentage decrease\\nColumns: One column per product, showing only the qualifying products\\nPage: One store per page\\nQuery\\nShow the results of the previous query, but rotating and switching the columns with\\nrows. \\nDisplay of Results\\nRows: One row per product, showing only the qualifying products \\nColumns: Y ear numbers 2000, 1999; difference; percentage decrease\\nPage: One store per page\\nQuery\\nShow the results of the previous query, but rotating and switching the pages with\\nrows. \\nDisplay of Results\\nRows: One row per store \\nColumns: Y ear numbers 2000, 1999; difference; percentage decrease\\nPage: One product per page, displaying only the qualifying products.\\nThis multidimensional analysis can continue on until the analyst determines how many\\nproducts showed reduced sales and which stores suffered the most.\\nIn the above example, we had only three business dimensions and each of the di-\\nmensions could, therefore, be represented along the edges of a cube or the results dis-played as columns, rows, and pages. Now add another business dimension, promotion.That will bring the number of business dimensions to four. When you have three busi-ness dimensions, you are able to represent these three as a cube with each edge of thecube denoting one dimension. Y ou are also able to display the data on a spreadsheet withtwo dimensions as rows and columns and the third dimension as pages. But when youhave four dimensions or more, how can you represent the data? Obviously, a three-dimensional cube does not work. And you also have a problem when trying to displaythe data on a spreadsheet as rows, columns, and pages. So what about multidimension-al analysis when there are more than three dimensions? This leads us to a discussion ofhypercubes.356 OLAP IN THE DATA WAREHOUSE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6b8f7fa9-fa13-4b31-b449-b7f172706abd', embedding=None, metadata={'page_label': '375', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='What are Hypercubes?\\nLet us begin with the two business dimensions of product and time. Usually, business\\nusers wish to analyze not just sales but other metrics as well. Assume that the metrics tobe analyzed are fixed cost, variable cost, indirect sales, direct sales, and profit margin.These are five common metrics.\\nThe data described here may be displayed on a spreadsheet showing metrics as\\ncolumns, time as rows, and products as pages. Please see Figure 15-7 showing a samplepage of the spreadsheet display. In the figure, please also note the three straight lines, twoof which represent the two business dimensions and the third, the metrics. Y ou can inde-pendently move up or down along the straight lines. Some experts refer to this representa-tion of a multidimension as a multidimensional domain structure (MDS).\\nThe figure also shows a cube representing the data points along the edges. Relate the\\nthree straight lines to the three edges of the physical cube. Now the page you see in thefigure is a slice passing through a single product and the divisions along the other twostraight lines shown on the page as columns and rows. With three groups of data—twogroups of business dimensions and one group of metrics—we can easily visualize the dataas being along the three edges of a cube.\\nNow add another business dimension to the model. Let us add the store dimension.\\nThat results in three business dimensions plus the metrics data. How can you representthese four groups as edges of a three-dimensional cube? How do you represent a four-di-mensional model with data points along the edges of a three-dimensional cube? How doyou slice the data to display pages?MAJOR FEATURES AND FUNCTIONS 357\\nCOLUMNS : MetricsROWS : TIME dimensionPRODUCT: Coats\\nPAGES : PRODUCT dimension\\nMonthsProducts\\nMetricsCoatsJan \\nFeb  Mar Apr MayJunJul Aug Sep Oct Nov DecTIME\\nFixed Cost\\nVariable \\nCost\\nIndirect \\nSales\\nDirect \\nSales\\nProfit      \\nMarginMETRICS\\nHats  \\nCoats Jackets DressesShirtsSlacksPRODUCT\\nMultidimensional \\nDomain  StructureFixed Variable Indirect Direct Profit\\nCost Cost Sales Sales Margin\\nJan 340 110 230 320 100\\nFeb 270 90 200 260 100\\nMar 310 100 210 270 70\\nApr 340 110 210 320 80\\nMay 330 110 230 300 90\\nJun 260 90 150 300 100\\nJul 310 100 180 300 70\\nAug 380 130 210 360 60\\nSep 300 100 180 290 70\\nOct 310 100 170 310 70\\nNov 330 110 210 310 80\\nDec 350 120 200 360 90\\nFigure 15-7 Display of columns, rows, and pages.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='12a5e584-1a1c-4c52-8299-4d7a71f0e1e2', embedding=None, metadata={'page_label': '376', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is where an MDS diagram comes in handy. Now you need not try to perceive four-\\ndimensional data as along the edges of the three-dimensional cube. All you have to do isdraw four straight lines to represent the data as an MDS. These four lines represent thedata. Please see Figure 15-8. By looking at this figure, you realize that the metaphor of aphysical cube to represent data breaks down when you try to represent four dimensions.But, as you see, the MDS is well suited to represent four dimensions. Can you think of thefour straight lines of the MDS intuitively to represent a “cube” with four primary edges?This intuitive representation is a hypercube, a representation that accommodates morethan three dimensions. At a lower level of simplification, a hypercube can very well ac-commodate three dimensions. A hypercube is a general metaphor for representing multi-dimensional data. \\nY ou now have a way of representing four dimensions as a hypercube. The next question\\nrelates to display of four-dimensional data on the screen. How can you possibly show fourdimensions with only three display groups of rows, columns, and pages? Please turn yourattention to Figure 15-9. What do you notice about the display groups? How does the dis-play resolve the problem of accommodating four dimensions with only three displaygroups? By combining multiple logical dimensions within the same display group. Noticehow product and metrics are combined to display as columns. The displayed page repre-sents the sales for store: New Y ork.\\nLet us look at just one more example of an MDS representing a hypercube. Let us\\nmove up to six dimensions. Please study Figure 15-10 with six straight lines showing thedata representations. The dimensions shown in this figure are product, time, store, promo-tion, customer demographics, and metrics.\\nThere are several ways you can display six-dimensional data on the screen. Figure 15-\\n11 illustrates one such six-dimensional display. Please study the figure carefully. Noticehow product and metrics are combined and represented as columns, store and time arecombined as rows, and demographics and promotion as pages.\\nWe have reviewed two specific issues. First, we have noted a special method for repre-358 OLAP IN THE DATA WAREHOUSE\\nJan\\nFebMarAprMay\\nJun\\nJulAugSep\\nOct\\nNovDecTIME\\nFixed Cost\\nVariable \\nCost\\nIndirect \\nSales\\nDirect \\nSales\\nProfit      \\nMarginMETRICS\\nHats  \\nCoats Jackets DressesShirtsSlacksPRODUCT\\nMultidimensional \\nDomain  Structure\\nNew York   \\nSan Jose      Dallas          \\nDenver     Cleveland    \\nBostonSTORE\\nFigure 15-8 MDS for four dimensions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1676a700-7169-4e60-ba52-3b13c18e0fe1', embedding=None, metadata={'page_label': '377', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='MAJOR FEATURES AND FUNCTIONS 359\\nTIME\\nSales\\nCostMETRICS PRODUCTMultidimensional \\nDomain  Structure\\nNew York   \\nSan Jose      DallasSTORE\\nJan \\nFeb        MarHats      \\nCoats\\nJacketsPAGE : Store Dimension\\nROWS : Time Dimension\\nCOLUMNS : Product & Metrics\\ncombinedHOW  \\nDISPLAYED ON \\nA PAGE\\nNew York Store \\nHats:Sales Hats:Cost Coats:Sales Costs:Cost Jackets:Sales Jackets:Cost\\nJan 450 350 550 450 500 400\\nFeb 380 280 460 360 400 320\\nMar 400 310 480 410 450 400\\nFigure 15-9 Page displays for four-dimensional data.\\nJan \\nFeb  Mar Apr MayJunJul Aug Sep Oct Nov DecTIME\\nFixed Cost\\nVariable \\nCost\\nIndirect \\nSales\\nDirect \\nSales\\nProfit      \\nMarginMETRICS\\nHats  \\nCoats Jackets DressesShirtsSlacksPRODUCT\\nMultidimensional \\nDomain  Structure\\nMarital Status   \\nLife Style      Income Level          Home Owner    Credit Rating    Purch. HabitDEMO-\\nGRAPHICS\\nNew York   \\nSan Jose      Dallas          Denver     Cleveland    BostonSTORE\\nType \\nDisplay       Coupon           Media     Cost        StylePROMO-\\nTION\\nFigure 15-10 Six-dimensional MDS.Coats:Cost', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1ca0c369-9a23-4212-afed-80bd334ea554', embedding=None, metadata={'page_label': '378', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='senting a data model with more than three dimensions using an MDS. This method is an\\nintuitive way of showing a hypercube. A model with three dimensions can be representedby a physical cube. But a physical cube is limited to only three dimensions or less. Sec-ond, we have also discussed the methods for displaying the data on a flat screen when thenumber of dimensions is three or more. Building on the resolution of these two issues, letus now move on to two very significant aspects of multidimensional analysis. One ofthese is the drill-down and roll-up exercise; the other is the slice-and-dice operation.\\nDrill-Down and Roll-Up\\nReturn to Figure 15-5. Look at the attributes of the product dimension table of the STAR\\nschema. In particular, note these specific attributes of the product dimension: productname, subcategory, category, product line, and department. These attributes signify an as-cending hierarchical sequence from product name to department. A department includesproduct lines, a product line includes categories, a category includes subcategories, andeach subcategory consists of products with individual product names. In an OLAP sys-tem, these attributes are called hierarchies of the product dimension.\\nOLAP systems provide drill-down and roll-up capabilities. Try to understand what we\\nmean by these capabilities with reference to above example. Please see Figure 15-12 illus-trating these capabilities with reference to the product dimension hierarchies. Note thedifferent types of information given in the figure. It shows the rolling up to higher hierar-chical levels of aggregation and the drilling down to lower levels of detail. Also note thesales numbers shown alongside. These are sales for one particular store in one particularmonth at these levels of aggregation. The sale numbers you notice as you go down the hi-erarchy are for a single department, a single product line, a single category, and so on. Y oudrill down to get the lower level breakdown of sales. The figure also shows the drill-across360 OLAP IN THE DATA WAREHOUSE\\nTIME\\nSales\\nCostMETRICS PRODUCTMultidimensional \\nDomain  Structure\\nNew York   \\nSan JoseSTORE\\nJan \\nFebHats      CoatsPAGE\\n: Demographics &\\nPromotion Dimensions combined\\nROWS : Store &Time   \\nDimensions combined\\nCOLUMNS : Product & \\nMetrics combinedHOW  DISPLAYED \\nON A PAGE\\nType\\nCouponPROMO\\nLife Style   \\nIncomeDEMO\\nLife Style : Coupon \\nHats Hats Coats Coats\\nSales Cos t Sales Cos t\\nNew York Jan 220 170 270 220\\nFeb 190 140 230 180\\nBoston Jan 200 160 240 200\\nFeb 180 130 220 170\\nFigure 15-11 Page displays for six-dimensional data.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cc39da5b-e385-4e92-a27d-236b1f3cd4e9', embedding=None, metadata={'page_label': '379', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='to another OLAP summarization using a different set of hierarchies of other dimensions.\\nNotice also the drill-through to the lower levels of granularity, as stored in the source datawarehouse repository. Roll-up, drill-down, drill-across, and drill-through are extremelyuseful features of OLAP systems supporting multidimensional analysis.\\nOn more question remains. While you are rolling up or drilling down, how do the page\\ndisplays change on the spreadsheets? For example, return to Figure 15-6 and look at theMAJOR FEATURES AND FUNCTIONS 361\\nDATA  WAREHOUSEDetailed Data Detailed Data\\nSummary DataOLAP\\nAggregation\\nLevelsSales in one\\nmonth  in\\none store\\nDepartment\\nProduct Line\\nCategory\\nProductSub-category300,000\\n60,000\\n5,00015,000\\n1,200Another\\ninstance of\\nOLAP\\nDrill -\\ndown /\\nRollup\\nDrill -\\nthrough\\nto detailDrill -\\nacross to\\nanother\\nOLAP\\ninstance\\nFigure 15-12 Roll-up and drill-down features of OLAP .\\nCOLUMNS : PRODUCT dimensionSub-categoriesROWS : TIME dimension\\nMonthsStore: New York\\nPAGES : STORE dimension\\nO ute r Dre ss Ca sua l\\nJan 1,100 1,020 490\\nFeb 1,080 1,040 500\\nMar 1,050 980 470\\nApr 970 1,000 480\\nMay 1,010 1,080 520\\nJun 910 1,100 330\\nJul 880 1,120 250\\nAug 960 1,320 230\\nSep 870 1,280 210\\nOct 910 1,240 250\\nNov 980 1,380 260\\nDec 1,080 1,520 310\\nFigure 15-13 Three-dimensional display with roll-up.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8dc17660-22ee-4b33-b9ee-b5d3ccf0775e', embedding=None, metadata={'page_label': '380', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='page display on the spreadsheet. The columns represent the various products, the rows\\nrepresent the months, and the pages represent the stores. At this point, if you want to rollup to the next higher level of subcategory, how will the display in Figure 15-6 change?The columns on the display will have to change to represent subcategories instead ofproducts. Please see Figure 15-13 indicating this change.\\nLet us ask just one more question before we leave this subsection. When you have\\nrolled up to the subcategory level in the product dimension, what happens to the display ifyou also roll up to the next higher level of the store dimension, territory? How will thedisplay on the spreadsheet change? Now the spreadsheet will display the sales withcolumns representing subcategories, rows representing months, and the pages represent-ing territories. \\nSlice-and-Dice or Rotation\\nLet us revisit Figure 15-6 showing the display of months as rows, products as columns,\\nand stores as pages. Each page represents the sales for one store. The data model corre-sponds to a physical cube with these data elements represented by its primary edges. Thepage displayed is a slice or two-dimensional plane of the cube. In particular, this displaypage for the New Y ork store is the slice parallel to the product and time axes. Now beginto look at Figure 15-14 carefully. On the left side, the first part of the diagram shows thisalignment of the cube. For the sake simplicity, only three products, three months, andthree stores are chosen for illustration.362 OLAP IN THE DATA WAREHOUSE\\nHats Coats Jackets\\nJan 200 550 350\\nFeb 210 480 390\\nMar 190 480 380MonthsStores\\nProducts\\nProduct: HatsX-axis: Columns ; Y-axis: Rows ; Z-axis: PagesXYZ\\nXY\\nZX\\nY\\nZProducts\\nMonthsStores\\nMonthsProductsStores\\nStore: New York Month: January\\nJan Feb Mar\\nNew York 200 210 190\\nBoston 210 250 240\\nSan Jos e 130 90 70New York Boston San Jose\\nHats 200 210 130\\nCoats 550 500 200\\nJackets 350 400 100\\nFigure 15-14 Slicing and dicing.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='63c5f64f-6694-4232-a89d-da5186857584', embedding=None, metadata={'page_label': '381', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Now rotate the cube so that products are along the Z-axis, months are along the X-axis,\\nand stores are along the Y -axis. The slice we are considering also rotates. What happens tothe display page that represents the slice? Months are now shown as columns and stores asrows. The display page represents the sales of one product, namely product: hats. \\nY ou can go to the next rotation so that months are along the Z-axis, stores are along the\\nX-axis, and products are along the Y -axis. The slice we are considering also rotates. Whathappens to the display page that represents the slice? Stores are now shown as columnsand products as rows. The display page represents the sales of one month, namely month:January.\\nWhat is the great advantage of all of this for the users? Did you notice that with each\\nrotation, the users can look at page displays representing different versions of the slices inthe cube. The users can view the data from many angles, understand the numbers better,and arrive at meaningful conclusions. \\nUses and Benefits\\nAfter exploring the features of OLAP in sufficient detail, you must have already deduced\\nthe enormous benefits of OLAP . We have discussed multidimensional analysis as provid-ed in OLAP systems. The ability to perform multidimensional analysis with complexqueries sometimes also entails complex calculations.\\nLet us summarize the benefits of OLAP systems:\\n/L50539Increased productivity of business managers, executives, and analysts\\n/L50539Inherent flexibility of OLAP systems means that users may be self-sufficient in run-\\nning their own analysis without IT assistance\\n/L50539Benefit for IT developers because using software specifically designed for the sys-\\ntem development results in faster delivery of applications\\n/L50539Self-sufficiency of users, resulting in reduction in backlog\\n/L50539Faster delivery of applications following from the previous benefits\\n/L50539More efficient operations through reducing time on query executions and in net-\\nwork traffic\\n/L50539Ability to model real-world challenges with business metrics and dimensions\\nOLAP MODELS\\nHave you heard of the terms ROLAP or MOLAP? There is another variation, DOLAP . A\\nvery simple explanation of the variations relates to the way data is stored for OLAP . Theprocessing is still online analytical processing, only the storage methodology is different. \\nROLAP stands for relational online analytical processing and MOLAP stands for\\nmultidimensional online analytical processing. In either case, the information interfaceis still OLAP . DOLAP stands for desktop online analytical processing. DOLAP is meantto provide portability to users of online analytical processing. In the DOLAP methodol-ogy, multidimensional datasets are created and transferred to the desktop machine, re-quiring only the DOLAP software to exist on that machine. DOLAP is a variation ofROLAP .OLAP MODELS 363', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ada82a9f-26d6-44ca-81a8-d10392bbff1c', embedding=None, metadata={'page_label': '382', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Overview of Variations\\nIn the MOLAP model, online analytical processing is best implemented by storing the\\ndata multidimensionally, that is, easily viewed in a multidimensional way. Here the datastructure is fixed so that the logic to process multidimensional analysis can be based onwell-defined methods of establishing data storage coordinates. Usually, multidimensionaldatabases (MDDBs) are vendors’ proprietary systems. On the other hand, the ROLAPmodel relies on the existing relational DBMS of the data warehouse. OLAP features areprovided against the relational database.\\nSee Figure 15-15 contrasting the two models. Notice the MOLAP model shown on the\\nleft side of the figure. The OLAP engine resides on a special server. Proprietary multidi-mensional databases (MDDBs) store data in the form of multidimensional hypercubes.Y ou have to run special extraction and aggregation jobs to create these multidimensionaldata cubes in the MDDBs from the relational database of the data warehouse. The specialserver presents the data as OLAP cubes for processing by the users.\\nOn the right side of the figure you see the ROLAP model. The OLAP engine resides on\\nthe desktop. Prefabricated multidimensional cubes are not created beforehand and storedin special databases. The relational data is presented as virtual multidimensional datacubes. 364 OLAP IN THE DATA WAREHOUSE\\nDesktop\\nMDDB OLAP\\nServer\\nData\\nWarehouse\\nDatabase\\nServerMOLAP\\nData\\nWarehouse\\nDatabase\\nServerOLAP\\nServicesDesktopROLAP\\nFigure 15-15 OLAP models.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='791557eb-c454-4883-a47e-3628e4a89c90', embedding=None, metadata={'page_label': '383', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The MOLAP Model\\nAs discussed, in the MOLAP model, data for analysis is stored in specialized multidimen-\\nsional databases. Large multidimensional arrays form the storage structures. For example,to store sales number of 500 units for product ProductA, in month number 2001/01, instore StoreS1, under distributing channel Channel05, the sales number of 500 is stored inan array represented by the values (ProductA, 2001/01, StoreS1, Channel05). \\nThe array values indicate the location of the cells. These cells are intersections of the\\nvalues of dimension attributes. If you note how the cells are formed, you will realize thatnot all cells have values of metrics. If a store is closed on Sundays, then the cells repre-senting Sundays will all be nulls. \\nLet us now consider the architecture for the MOLAP model. Please go over each part\\nof Figure 15-16 carefully. Note the three layers in the multitier architecture. Precalculatedand prefabricated multidimensional data cubes are stored in multidimensional databases.The MOLAP engine in the application layer pushes a multidimensional view of the datafrom the MDDBs to the users. \\nAs mentioned earlier, multidimensional database management systems are proprietary\\nsoftware systems. These systems provide the capability to consolidate and fabricate sum-marized cubes during the process that loads data into the MDDBs from the main datawarehouse. The users who need summarized data enjoy fast response times from the pre-consolidated data. OLAP MODELS 365\\nMDDBMOLAP\\nEngine\\nData\\nWarehouse\\nRDBMS\\nServerMDBMS\\nServerDesktop\\nClient\\nAPPLICATION\\nLAYER\\nDATA\\nLAYERPRESENTATION\\nLAYER\\nFigure 15-16 The MOLAP model.Proprietary DataLanguage\\nCreate and Store\\nSummary Data Cubes', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d17d80c0-8245-41c3-8cc6-2110c07d4037', embedding=None, metadata={'page_label': '384', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The ROLAP Model\\nIn the ROLAP model, data is stored as rows and columns in relational form. This model\\npresents data to the users in the form of business dimensions. In order to hide the storagestructure to the user and present data multidimensionally, a semantic layer of metadata iscreated. The metadata layer supports the mapping of dimensions to the relational tables.Additional metadata supports summarizations and aggregations. Y ou may store the meta-data in relational databases.\\nNow see Figure 15-17. This figure shows the architecture of the ROLAP model. What\\nyou see is a three-tier architecture. The analytical server in the middle tier application lay-er creates multidimensional views on the fly. The multidimensional system at the presen-tation layer provides a multidimensional view of the data to the users. When the users is-sue complex queries based on this multidimensional view, the queries are transformedinto complex SQL directed to the relational database. Unlike the MOLAP model, staticmultidimensional structures are not created and stored.\\nTrue ROLAP has three distinct characteristics:\\n/L50539Supports all the basic OLAP features and functions discussed earlier\\n/L50539Stores data in a relational form\\n/L50539Supports some form of aggregation366 OLAP IN THE DATA WAREHOUSE\\nData\\nWarehouse\\nRDBMS\\nServerDesktop\\nClient\\nAnalytical\\nServer\\nAPPLICATION\\nLAYER\\nDATA\\nLAYERPRESENTATION\\nLAYERMultidimensional\\nview\\nFigure 15-17 The ROLAP model.Create Data CubesDynamically\\nUser Request\\nComplex SQL', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='18044887-7ca5-4547-86da-01632e0a6348', embedding=None, metadata={'page_label': '385', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Local hypercubing is a variation of ROLAP provided by vendors. This is how it works:\\n1. The user issues a query.\\n2. The results of the query get stored in a small, local, multidimensional database.3. The user performs analysis against this local database.4. If additional data is required to continue the analysis, the user issues another query\\nand the analysis continues.\\nROLAP VERSUS MOLAP\\nShould you use the relational approach or the multidimensional approach to provide on-\\nline analytical processing for your users? That depends on how important query perfor-mance is for your users. Again, the choice between ROLAP and MOLAP also depends onthe complexity of the queries from your users. Figure 15-18 charts the solution optionsbased on the considerations of query performance and complexity of queries. MOLAP isthe choice for faster response and more intensive queries. These are just two broad consid-erations. \\nAs part of the technical component of the project team, your perspective on the choice\\nis entirely different from that of the users. Users will get the functionality and benefits ofmultidimensionality from either model but are more concerned with questions relating tothe extent of business data made available for analysis, the acceptability of performance,and the justification of the cost.\\nLet us conclude the discussion on the choice between ROLAP and MOLAP with Fig-\\nure 15-19. This figure compares the two models based on the specific aspects of data stor-age, technologies, and features. This figure is important, for it pulls everything togetherand presents a balanced case.ROLAP VERSUS MOLAP 367\\nROLAPMOLAP\\nComplexity of AnalysisQuery Performance\\nFigure 15-18 ROLAP or MOLAP?', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9f67270a-6553-4afc-a75e-2764c074ae05', embedding=None, metadata={'page_label': '386', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='OLAP IMPLEMENTATION CONSIDERATIONS\\nBefore considering implementation of OLAP in your data warehouse, you have to take\\ninto account two key issues with regard to the MOLAP model running under MDDBMS.The first issue relates to the lack of standardization. Each vendor tool has its own clientinterface. Another issue is scalability. OLAP is generally good for handling summarydata, but not good for volumes of detailed data.\\nOn the other hand, highly normalized data in the data warehouse can give rise to pro-\\ncessing overhead when you are performing complex analysis. Y ou may reduce this by us-ing a STAR schema multidimensional design. In fact, for some ROLAP tools, the multidi-mensional representation of data in a STAR schema arrangement is a prerequisite. \\nConsider a few choices of architecture. Look at Figure 15-20 showing four architectur-\\nal options. \\nY ou have now studied the various implementation options for providing OLAP func-\\ntionality in your data warehouse. These are important choices. Remember, without OLAP ,your users have very limited means for analyzing data. Let us now examine some specificdesign considerations.\\nData Design and Preparation\\nThe data warehouse feeds data to the OLAP system. In the MOLAP model, separate pro-\\nprietary multidimensional databases store the data fed from the data warehouse in theform of multidimensional cubes. On the other hand, in the ROLAP model, although nostatic intermediary data repository exists, data is still pushed into the OLAP system with368 OLAP IN THE DATA WAREHOUSE\\nData stored as relational \\ntables in the warehouse.\\nDetailed and light \\nsummary data available.\\nVery large data volumes.\\nAll data access from the \\nwarehouse storage.Data Storage Underlying Technologies Functions and FeaturesROLAPMOLAPData stored as relational \\ntables in the warehouse.\\nVarious summary data kept \\nin proprietary databases \\n(MDDBs)\\nModerate data volumes.\\nSummary data access from \\nMDDB, detailed data \\naccess from warehouse.Use of complex SQL to \\nfetch data from \\nwarehouse.\\nROLAP engine in \\nanalytical server creates \\ndata cubes on the fly.\\nMultidimensional views \\nby presentation layer.\\nCreation of pre-fabricated \\ndata cubes by MOLAP \\nengine. Propriety \\ntechnology to store \\nmultidimensional views in \\narrays, not tables. High \\nspeed matrix data retrieval.\\nSparse matrix technology \\nto manage data sparsity in \\nsummaries.Faster access.\\nLarge library of functions \\nfor complex calculations. \\nEasy analysis irrespective \\nof the number of \\ndimensions.\\nExtensive drill-down and \\nslice-and-dice capabilities.  Known environment and \\navailability of many tools.\\nLimitations on complex \\nanalysis functions.\\nDrill-through to lowest \\nlevel easier. Drill-across \\nnot always easy. \\nFigure 15-19 ROLAP versus MOLAP .engine. Proprietary', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='93c6b8c7-f236-4297-b7c1-99b03474f3e3', embedding=None, metadata={'page_label': '387', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='cubes created dynamically on the fly. Thus, the sequence of the flow of data is from the\\noperational source systems to the data warehouse and from there to the OLAP system.\\nSometimes, you may have the desire to short-circuit the flow of data. Y ou may wonder\\nwhy you should not build the OLAP system on top of the operational source systemsthemselves. Why not extract data into the OLAP system directly? Why bother movingdata into the data warehouse and then into the OLAP system? Here are a few reasons whythis approach is flawed:\\n/L50539An OLAP system needs transformed and integrated data. The system assumes that\\nthe data has been consolidated and cleansed somewhere before it arrives. The dis-parity among operational systems does not support data integration directly. \\n/L50539The operational systems keep historical data only to a limited extent. An OLAP sys-\\ntem needs extensive historical data. Historical data from the operational systemsmust be combined with archived historical data before it reaches the OLAP system.\\n/L50539An OLAP system requires data in multidimensional representations. This calls for\\nsummarization in many different ways. Trying to extract and summarize data fromthe various operational systems at the same time is untenable. Data must be consol-idated before it can be summarized at various levels and in different combinations. \\n/L50539Assume there are a few OLAP systems in your environment. That is, one supports\\nthe marketing department, another the inventory control department, yet another thefinance department, and so on. To accomplish this, you have to build a separate in-terface with the operational systems for data extraction into each OLAP system.Can you imagine how difficult this would be?OLAP IMPLEMENTATION CONSIDERATIONS 369\\nFigure 15-20 OLAP architectural options.MDDB OLAP\\nServer\\nData\\nWarehouse\\nDatabase\\nServerOLAP\\nServicesMDDBData\\nMartThin\\nClientClient Client\\nMDDBData\\nMartOLAP\\nServerFat\\nClient\\nFOUR  ARCHITECTURAL\\nOPTIONS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='549dacb4-dbe4-4122-b6d7-fe119b505128', embedding=None, metadata={'page_label': '388', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='In order to help prepare the data for the OLAP system, let us first examine some sig-\\nnificant characteristics of data in this system. Please review the following list:\\n/L50539An OLAP system stores and uses much less data compared to a data warehouse.\\n/L50539Data in the OLAP system is summarized. Y ou will rarely find data at the lowest lev-\\nel of detail as in the data warehouse.\\n/L50539OLAP data is more flexible for processing and analysis partly because there is much\\nless data to work with.\\n/L50539Every instance of the OLAP system in your environment is customized for the pur-\\npose that instance serves. In order words, OLAP data tends to be more departmen-talized, whereas data in the data warehouse serves corporate-wide needs. \\nAn overriding principle is that OLAP data is generally customized. When you build the\\nOLAP system with system instances servicing different user groups, you need to keep thisin mind. For example, one instance or specific set of summarizations would be meant forone group of users, say the marketing department. Let us quickly go through the tech-niques for preparing OLAP data for a specific group of users or a particular department,for example, marketing.\\nDefine Subset. Select the subset of detailed data the marketing department is interest-\\ned in.\\nSummarize. Summarize and prepare aggregate data structures in the way the market-\\ning department needs for summarizing. For example, summarize products alongproduct categories as defined by marketing. Sometimes, marketing and accountingdepartments may categorize products in different ways.\\nDenormalize. Combine relational tables in exactly the same way the marketing depart-\\nment needs denormalized data. If marketing needs tables A and B joined, but fi-nance needs tables B and C joined, go with the join for tables A and B for the mar-keting OLAP subset.\\nCalculate and Derive. If some calculations and derivations of the metrics are depart-\\nment-specific in your company, use the ones for marketing.\\nIndex. Choose those attributes that are appropriate for marketing to build indexes.\\nWhat about data modeling for the OLAP data structure? The OLAP structure contains\\nseveral levels of summarization and a few kinds of detailed data. How do you model theselevels of summarization?\\nPlease see Figure 15-21 indicating the types and levels of data in OLAP systems.\\nThese types and levels must be taken into consideration while performing data modelingfor the OLAP systems. Pay attention to the different types of data in an OLAP system.When you model the data structures for your OLAP system, you need to provide for thesetypes of data.\\nAdministration and Performance\\nLet us now turn our attention to two important though not directly connected issues. 370 OLAP IN THE DATA WAREHOUSE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7fcf6ee0-cfce-4a6c-824f-3bab737374a7', embedding=None, metadata={'page_label': '389', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Administration. One of these issues is the matter of administration and management\\nof the OLAP environment. The OLAP system is part of the overall data warehouse en-vironment and, therefore, administration of the OLAP system is part of the data ware-house administration. Nevertheless, we must recognize some key considerations for ad-ministering and managing the OLAP system. Let us briefly indicate a few of theseconsiderations.\\n/L50539Expectations on what data will be accessed and how\\n/L50539Selection of the right business dimensions \\n/L50539Selection of the right filters for loading the data from the data warehouse\\n/L50539Methods and techniques for moving data into the OLAP system (MOLAP model)\\n/L50539Choosing the aggregation, summarization, and precalculation\\n/L50539Developing application programs using the proprietary software of the OLAP vendor\\n/L50539Size of the multidimensional database\\n/L50539Handling of the sparse-matrix feature of multidimensional structures\\n/L50539Drill down to the lowest level of detail\\n/L50539Drill through to the data warehouse or to the source systems\\n/L50539Drill across among OLAP system instances\\n/L50539Access and security privileges\\n/L50539Backup and restore facilities\\nPerformance. First you need to recognize that the presence of an OLAP system in\\nyour data warehouse environment shifts the workload. Some of the queries that usuallymust run against the data warehouse will now be redistributed to the OLAP system. TheOLAP IMPLEMENTATION CONSIDERATIONS 371\\nPERMANENT DETAILED DATA\\nDetailed data retrieved from the data warehouse \\nrepository and stored in the OLAP system.\\nTRANSIENT DETAILED DATA\\nDetailed data brought in from the data \\nwarehouse repository on temporary, one-time \\nbasis for special purposes.STATIC \\nSUMMARY DATADYNAMIC \\nSUMMARY DATA\\nMost of the OLAP \\nsummary data is \\nstatic. This is the \\ndata summarized \\nfrom the data \\nretrieved from the \\ndata warehouse.This type of \\nsummary data is \\nvery rare in the \\nOLAP environment \\nalthough this \\nhappens because of \\nnew business rules. \\nFigure 15-21 Data modeling considerations for OLAP .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f94c287f-bf96-45ae-a94d-70d8857768bb', embedding=None, metadata={'page_label': '390', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='types of queries that need OLAP are complex and filled with involved calculations. Long\\nand complicated analysis sessions consist of such complex queries. Therefore, when suchqueries get directed to the OLAP system, the workload on the main data warehouse be-comes substantially reduced.\\nA corollary of shifting the complex queries to the OLAP system is the improvement in\\nthe overall query performance. The OLAP system is designed for complex queries. Whensuch queries run in the OLAP system, they run faster. As the size of the data warehousegrows, the size of the OLAP system still remains manageable and comparably small.\\nMultidimensional databases provide a reasonably predictable, fast, and consistent re-\\nsponse to every complex query. This is mainly because OLAP systems preaggregate andprecalculate many, if not, all possible hypercubes and store these. The queries run againstthe most appropriate hypercubes. For instance, assume that there are only three dimen-sions. The OLAP system will calculate and store summaries as follows:\\n/L50539A three-dimensional low-level array to store base data\\n/L50539A two-dimensional array of data for dimension-1 and dimension-2\\n/L50539A 2-dimensional array of data for dimension-2 and dimension-3\\n/L50539A high-level summary array by dimension-1\\n/L50539A high-level summary array by dimension-2\\n/L50539A high-level summary array by dimension-3\\nAll of these precalculations and preaggregations result in faster response to queries at\\nany level of summarization. But this speed and performance do not come without anycost. Y ou pay the price to some extent in the load performance. OLAP systems are not re-freshed daily for the simple reason that load times for precalculating and loading all thepossible hypercubes are exhorbitant. Enterprises use longer intervals between refreshes oftheir OLAP systems. Most OLAP systems are refreshed once a month. \\nOLAP Platforms\\nWhere does the OLAP system physically reside? Should it be on the same platform as the\\nmain data warehouse? Should it be planned to be on a separate platform from the begin-ning? What about growth of the data warehouse and the OLAP system? How do thegrowth patterns affect the decision? These are some of the questions you need to answeras you provide OLAP capability to your users.\\nUsually, the data warehouse and the OLAP system start out on the same platform.\\nWhen both are small, it is cost-justifiable to keep both on the same platform. Within ayear, it is usual to find rapid growth in the main data warehouse. The trend normally con-tinues. As this growth happens, you may want to think of moving the OLAP system to an-other platform to ease the congestion. But how exactly would you know whether to sepa-rate the platforms and when to do so? Here are some guidelines:\\n/L50539When the size and usage of the main data warehouse escalate and reach the point\\nwhere the warehouse requires all the resources of the common platform, start actingon the separation.\\n/L50539If too many departments need the OLAP system, then the OLAP requires additional\\nplatforms to run.372 OLAP IN THE DATA WAREHOUSE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e76dc9a3-1652-48d5-81eb-c3d4bc0ff806', embedding=None, metadata={'page_label': '391', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Users expect the OLAP system to be stable and perform well. The data refreshes to\\nthe OLAP system are much less frequent. Although this is true for the OLAP sys-tem, daily application of incremental loads and full refreshes of certain tables areneeded for the main data warehouse. If these daily transactions applicable to thedata warehouse begin to disrupt the stability and performance of the OLAP system,then move the OLAP system to another platform.\\n/L50539Obviously, in decentralized enterprises with OLAP users spread out geographically,\\none or more separate platforms for the OLAP system become necessary.\\n/L50539If users of one instance of the OLAP system want to stay away from the users of an-\\nother, then separation of platforms needs to be looked into.\\n/L50539If the chosen OLAP tools need a configuration different from the platform of the\\nmain data warehouse, then the OLAP system requires a separate platform, config-ured correctly. \\nOLAP Tools and Products\\nThe OLAP market is becoming sophisticated. Many OLAP products have appeared and\\nmost of the recent products are quite successful. Quality and flexibility of the productshave improved remarkably. \\nBefore we provide a checklist to be used for evaluation of OLAP products, let us list a\\nfew broad guidelines:\\n/L50539Let your applications and the users drive the selection of the OLAP products. Do\\nnot be carried away by flashy technology. \\n/L50539Remember, your OLAP system will grow both in size and in the number of active\\nusers. Determine the scalability of the products before you choose.\\n/L50539Consider how easy it is to administer the OLAP product. \\n/L50539Performance and flexibility are key ingredients in the success of your OLAP sys-\\ntem. \\n/L50539As technology advances, the differences in the merits between ROLAP and MO-\\nLAP appear to be somewhat blurred. Do not worry too much about these two meth-ods. Concentrate on the matching of the vendor product with your users’ analyticalrequirements. Flashy technology does not always deliver.\\nNow let us get to the selection criteria for choosing OLAP tools and products. While\\nyou evaluate the products, use the following checklist and rate each product against eachitem on the checklist:\\n/L50539Multidimensional representation of data\\n/L50539Aggregation, summarization, precalculation, and derivations\\n/L50539Formulas and complex calculations in an extensive library\\n/L50539Cross-dimensional calculations\\n/L50539Time intelligence such as year-to-date, current and past fiscal periods, moving aver-\\nages, and moving totals\\n/L50539Pivoting, cross-tabs, drill-down, and roll-up along single or multiple dimensionsOLAP IMPLEMENTATION CONSIDERATIONS 373', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e72870d4-a78c-4587-9256-c8e3adf6d1ef', embedding=None, metadata={'page_label': '392', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Interface of OLAP with applications and software such as spreadsheets, proprietary\\nclient tools, third-party tools, and 4GL environments.\\nImplementation Steps\\nAt this point, perhaps your project team has been given the mandate to build and imple-\\nment an OLAP system. Y ou know the features and functions. Y ou know the significance.Y ou are also aware of the important considerations. How do you go about implementingOLAP? Let us summarize the key steps. These are the steps or activities at a very highlevel. Each step consists of several tasks to accomplish the objectives of that step. Y ou willhave to come up with the tasks based on the requirements of your environment. Here arethe major steps:\\n/L50539Dimensional modeling\\n/L50539Design and building of the MDDB\\n/L50539Selection of the data to be moved into the OLAP system\\n/L50539Data acquisition or extraction for the OLAP system\\n/L50539Data loading into the OLAP server\\n/L50539Computation of data aggregation and derived data\\n/L50539Implementation of application on the desktop\\n/L50539Provision of user training\\nCHAPTER SUMMARY\\n/L50539OLAP is critical because its multidimensional analysis, fast access, and powerful\\ncalculations exceed that of other analysis methods.\\n/L50539OLAP is defined on the basis of Codd’ s initial twelve guidelines.\\n/L50539OLAP characteristics include multidimensional view of the data, interactive and\\ncomplex analysis facility, ability to perform intricate calculations, and fast responsetime. \\n/L50539Dimensional analysis is not confined to three dimensions that can be represented by\\na physical cube. Hypercubes provide a method for representing views with more di-mensions.\\n/L50539ROLAP and MOLAP are the two major OLAP models. The difference between\\nthem lies in the way the basic data is stored. Ascertain which model is more suitablefor your environment. \\n/L50539OLAP tools have matured. Some RDBMSs include support for OLAP . \\nREVIEW QUESTIONS\\n1. Briefly explain multidimensional analysis.\\n2. Name any four key capabilities of an OLAP system.3. State any five of Dr. Codd’ s guidelines for an OLAP system, giving a brief de-\\nscription for each.374 OLAP IN THE DATA WAREHOUSE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0120ae92-f33b-4119-8e4f-560a43306bd2', embedding=None, metadata={'page_label': '393', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. What are hypercubes? How do they apply in an OLAP system?\\n5. What is meant by slice-and-dice? Give an example.6. What are the essential differences between the MOLAP and ROLAP models?\\nAlso list a few similarities.\\n7. What are multidimensional databases? How do these store data?8. Describe any one of the four OLAP architectural options.9. Discuss two reasons why feeding data into the OLAP system directly from the\\nsource operational systems is not recommended.\\n10. Name any four factors for consideration in OLAP administration. \\nEXERCISES\\n1. Indicate if true or false:\\nA. OLAP facilitates interactive queries and complex uses.\\nB. A hypercube can be represented by the physical cube.C. Slice-and-dice is the same as the rotation of the columns and rows in presenta-\\ntion of data.\\nD. DOLAP stands for departmental OLAP .E. ROLAP systems store data in a multidimensional, proprietary databases.F . The essential difference between ROLAP and MOLAP is in the way data is\\nstored.\\nG. OLAP systems need transformed and integrated data.H. Data in an OLAP system is rarely summarized.I. Multidimensional domain structure (MDS) can represent only up to six dimen-\\nsions.\\nJ. OLAP systems do not handle moving averages.\\n2. As a senior analyst on the project team of a publishing company exploring the op-\\ntions for a data warehouse, make a case for OLAP . Describe the merits of OLAPand how it will be essential in your environment.\\n3. Pick any six of Dr. Codd’ s initial guidelines for OLAP . Give your reasons why the\\nselected six are important for OLAP . \\n4. Y ou are asked to form a small team to evaluate the MOLAP and ROLAP models\\nand make your recommendations. This is part of the data warehouse project for alarge manufacturer of heavy chemicals. Describe the criteria your team will use tomake the evaluation and selection.\\n5. Y our company is the largest producer of chicken products, selling to supermarkets,\\nfast-food chains, and restaurants, and also exporting to many countries. The ana-lysts from many offices worldwide expect to use the OLAP system when imple-mented. Discuss how the project team must select the platform for implementingOLAP for the company. Explain your assumptions.EXERCISES 375', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1975dabd-ea56-44bb-b2cd-5d16808b78cf', embedding=None, metadata={'page_label': '394', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 16\\nDATA WAREHOUSING AND THE WEB\\nCHAPTER OBJECTIVES\\n/L50539Understand what Web-enabling the data warehouse means and examine the reasons\\nfor doing so\\n/L50539Appreciate the implications of the convergence of Web technologies and those of\\nthe data warehouse\\n/L50539Probe into all the facets of Web-based information delivery\\n/L50539Study how OLAP and the Web connect and learn the different approaches to con-\\nnecting them\\n/L50539Examine the steps for building a Web-enabled data warehouse\\nWhat is the most dominant phenomenon in computing and communication that started\\nin the 1990s? Undoubtedly, it is the Internet with the Worldwide Web. The impact of theWeb on our lives and businesses can be matched only by a very few other developmentsover the past years. \\nIn the 1970s, we experienced a major breakthrough when the personal computer was\\nushered in with its graphical interfaces, pointing devices, and icons. Today’ s breakthroughis the Web, which is built on the earlier revolution. Making the personal computer usefuland effective was our goal in the 1970s and 1980s. Making the Web useful and effective isour goal today. The growth of the Internet and the use of the Web have overshadowed theearlier revolution. At the beginning of the year 2000, about 50 million households world-wide were estimated to be using the Internet. By the end of 2005, this number is expectedto grow ten-fold. About 500 million households worldwide will be browsing the Web bythen.\\nThe Web changes everything, as they say. Data warehousing is no exception. In the\\n1980s, data warehousing was still being defined and growing. During the 1990s, it was\\n377Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a352992b-36c2-4277-af80-5cfae4382d59', embedding=None, metadata={'page_label': '395', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='maturing. Now, after the Web revolution of the 1990s, data warehousing has assumed a\\nprominent place in the Web movement. Why?\\nWhat is the one major benefit of the Web revolution? Dramatically reduced communi-\\ncation costs. The Web has sharply diminished the cost of delivering information. What isthe relevance of that? What is one major purpose of the data warehouse? It is the deliveryof strategic information. So they match perfectly. The data warehouse is for delivering in-formation; the Internet makes it cost-effective to do so. We have arrived at the concept ofa Web-enabled data warehouse or a “data Webhouse.” The Web forces us to rethink datawarehouse design and deployment. \\nIn Chapter 3, we briefly considered the Web-enabled data warehouse. Specifically, we\\ndiscussed two aspects of this topic. First, we considered how to use the Web as one of theinformation delivery channels. This is taking the warehouse to the Web, opening up thedata warehouse to more than the traditional set of users. This chapter focuses on this as-pect of the relationship between the Web and the data warehouse.\\nThe other aspect, briefly discussed in Chapter 3, deals with bringing the Web to the\\nwarehouse. This aspect relates to your company’ s e-commerce, where the click streamdata of your company’ s Web site is brought into the data Webhouse for analysis. In thischapter, we will bypass this aspect of the Web–warehouse connection. Many articles byseveral authors and practitioners, and a recent excellent book co-authored by Dr. RalphKimball do adequate justice to the topic of the Data Webhouse. Please see the Referencesfor more information. \\nWEB-ENABLED DATA WAREHOUSE\\nA Web-enabled data warehouse uses the Web for information delivery and collaboration\\namong users. As months go by, more and more data warehouses are being connected tothe Web. Essentially, this means an increase in the access to information in the data ware-house. Increase in information access, in turn, means increase in the knowledge level ofthe enterprise. It is true that even before connecting to the Web, you could give access forinformation to more of your users, but with much difficulty and a proportionate increasein communication costs. The Web has changed all that. It is now a lot easier to add moreusers. The communications infrastructure is already there. Almost all of your users haveWeb browsers. No additional client software is required. Y ou can leverage the Web that al-ready exists. The exponential growth of the Web, with its networks, servers, users, andpages, has brought about the adoption of the Internet, intranets, and extranets as informa-tion transmission media. The Web-enabled data warehouse takes center stage in the Webrevolution. Let us see why.\\nWhy the Web?\\nIt appears to be quite natural to connect the data warehouse to the Web. Why do we say\\nthis? For a moment, think of how your users view the Web. First, they view the Web as atremendous source of information. They find the data content useful and interesting. Y ourinternal users, customers, and business partners already use the Web frequently. Theyknow how to get connected. The Web is everywhere. The sun never sets on the Web. Theonly client software needed is the Web browser, and almost everyone, young and old, haslearned how to launch and use a browser. A large number of software vendors have al-ready made their products Web-ready.378 DATA WAREHOUSING AND THE WEB', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cd689191-e457-4e79-bac7-9497187f26a0', embedding=None, metadata={'page_label': '396', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Now consider your data warehouse in relation to the Web. Y our users need the data\\nwarehouse for information. Y our business partners can use some of the specific informa-tion from the data warehouse. What do all of these have in common? Familiarity with theWeb and ability to access it easily. These are strong reasons for a Web-enabled data ware-house.\\nHow do you exploit the Web technology for your data warehouse? How do you connect\\nthe warehouse to Web? Let us quickly review three information delivery mechanisms thatcompanies have adopted based on Web technology. In each case, users access informationwith Web browsers. \\nInternet. The first medium is, of course, the Internet, which provides low-cost trans-\\nmission of information. Y ou may exchange information with anyone within or outside thecompany. Because the information is transmitted over public networks, security concernsmust be addressed. \\nIntranet. From the time the term “intranet” was coined in 1995, this concept of a pri-\\nvate network has gripped the corporate world. An intranet is a private computer networkbased on the data communications standards of the public Internet. The applications post-ing information over the intranet all reside within the firewall and, therefore, are more se-cure. Y ou can have all the benefits of the popular Web technology. In addition, you canmanage security better on the intranet.\\nExtranet. The Internet and the intranet have been followed by the extranet. An extranet\\nis not completely open like the Internet, nor it is restricted just for internal use like an in-tranet. An extranet is an intranet that is open to selective access by outside parties. Fromyour intranet, in addition to looking inward and downward, you could look outward toyour customers, suppliers, and business partners.\\nFigure 16-1 illustrates how information from the data warehouse may be delivered over\\nthese information delivery mechanisms. Note how your data warehouse may be deployedover the Web. If you choose to restrict your data warehouse to internal users, then youadopt the intranet. If it has to be opened up to outside parties with proper authorization,you go with the extranet. In both cases, the information delivery technology and the trans-mission protocols are the same.\\nThe intranet and the extranet come with several advantages. Here are a few:\\n/L50539With a universal browser, your users will have a single point of entry for informa-\\ntion.\\n/L50539Minimal training is required to access information. Users already know how to use\\na browser.\\n/L50539Universal browsers will run on any systems. \\n/L50539Web technology opens up multiple information formats to the users. They can re-\\nceive text, images, charts, even video and audio.\\n/L50539It is easy to keep the intranet/extranet updated so that there will be one source of in-\\nformation.\\n/L50539Opening up your data warehouse to your business partners over the extranet fosters\\nand strengthens the partnerships.\\n/L50539Deployment and maintenance costs are low for Web-enabling your data warehouse.\\nPrimarily, the network costs are less. Infrastructure costs are also low.WEB-ENABLED DATA WAREHOUSE 379', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='227e2e29-82c1-46ad-ba52-a69f3e6c5e75', embedding=None, metadata={'page_label': '397', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Convergence of Technologies\\nThere is no getting away from the fact that Web technology and data warehousing have\\nconverged, and the bond is only getting stronger. If you do not Web-enable your datawarehouse, you will be left behind. From the middle of the 1990s, vendors have been rac-ing one another to release Web-enabled versions of their products. The Web offerings ofthe products are exceeding the client/server offerings for the first time since Web offer-ings began to appear. Indirectly, these versions are forcing the convergence of the Web andthe data warehouse even further. \\nRemember that the Web is more significant than the data warehouse. The Web and its\\nfeatures will lead and the data warehouse has to follow. The Web has already pegged theexpectations of the users at a high level. Users will therefore expect the data warehouse toperform at that high level. Consider some of the expectations promoted by the Web thatare now expected to be adopted by data warehouses:\\n/L50539Fast response, although some Web pages are comparatively slower\\n/L50539Extremely easy and intuitive to use\\n/L50539Up 24 hours a day, 7 days a week\\n/L50539More up-to-date content\\n/L50539Graphical, dynamic, and flexible user interfaces\\n/L50539Almost personalized display\\n/L50539Expectation to connect to anywhere and drill across\\nOver the last few years, the number of Web-enabled data warehouses has increased\\nsubstantially. How have these Web-enabled data warehouses fared so far? To understand380 DATA WAREHOUSING AND THE WEB\\nFigure 16-1 Data warehouse and the Web.SUPPLIERS CUSTOMERSDISTRIBUTORSDEALERS\\nEXECUTIVES  \\nMANAGERS  \\nANALYSTS      \\nSUPPORT STAFF  \\nIT STAFF              \\nWAREHOUSE   \\nADMINISTRATORSDATA \\nWAREHOUSEINTERNAL \\nWAREHOUSE \\nUSERSEXTERNAL \\nWAREHOUSE \\nUSERS\\nINTRANETINTERNET\\nFirewall', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f7a2046a-6e8a-4283-90d0-88e7600ee6f7', embedding=None, metadata={'page_label': '398', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='the effect of the convergence of the two technologies, we must consider the three orders of\\neffects of decreasing costs as documented by Thomas W . Malone and John F . Rockart inearly 1990s:\\nFirst-Order Effect: Simple substitution of the new technology for the old.\\nSecond-Order Effect: Increased demand for the functions provided by the new technol-\\nogy.\\nThird-Order Effect: The rise of new technology-intensive structures.\\nWhat has the convergence of the Web technology and data warehousing brought about\\nso far? Web warehousing appears to have passed through the first two stages. Companiesthat have Web-enabled data warehouses have reduced costs by the substitution of the newmethods of information delivery. Also, the demand for information has increased follow-ing the first stage. For most companies with Web-enabled data warehouses, progress stopswhen they reach the end of the second stage. \\nAdapting the Data Warehouse for the Web\\nMuch is expected of a Web-enabled data warehouse. That means you have to reinvent your\\ndata warehouse. Y ou have to carry out a number of tasks to adapt your data warehouse forthe Web. Let us consider the specific provisions for Web-enabling your data warehouse.\\nFirst, let us get back to the discussion of the three stages following the introduction of\\na new technology. Apart from reducing costs from the substitution, demand for data ware-house information has increased. Most companies seem to be stuck at the end of the sec-ond stage. Only a few companies have moved on to the next stage and have realized third-order results. What are these results? Some of such results include extranet and consumerdata marts, management by exception, and automated supply and value chains. When youadapt your data warehouse for the Web, make sure that you do not stay put at the secondstage. Make plans to exploit the potential of the Web and move on to the third stage wherethe real benefits are found.\\nStudy the following list of requisites for adapting the data warehouse to the Web.\\nInformation “Push” Technique. The data warehouse was designed and implemented\\nusing the “pull” technique. The information delivery system pulls information fromthe data warehouse based on requests, and then provides it to the users. The Web of-fers another technique. The Web can “push” information to users without their ask-ing for it every time. Y our data warehouse must be able to adopt the “push” tech-nique.\\nEase of Usage. With the availability of click stream data, you can very quickly check\\nthe behavior of the user at the site. Among other things, click stream data revealshow easy or difficult it is for the users to browse the pages. Ease of use appears atthe top of the list of requirements.\\nSpeedy Response. Some data warehouses allow jobs to run long to produce the de-\\nsired results. In the Web model, speed is expected and cannot be negotiated or com-promised. \\nNo Downtime. The Web model is designed so that the system is available all the time.\\nSimilarly, the Web-enabled data warehouse has no downtime.WEB-ENABLED DATA WAREHOUSE 381', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1d0a64d0-7849-4991-a23a-2a7353105753', embedding=None, metadata={'page_label': '399', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Multimedia Output. Web pages have multiple data types—textual, numeric, graphics,\\nsound, video, animation, audio, and maps. These types are expected to show as out-puts in the information delivery system of the Web-enabled data warehouse.\\nMarket of One. Web information delivery is tending to become highly personalized,\\nwith dynamically created XML pages replacing static HTML coding. Web-enableddata warehouses will have to follow suit.\\nScalability. More access, more users, and more data—these are the results of Web-en-\\nabling the data warehouse. Therefore, scalability becomes a primary concern.\\nThe Web as a Data Source\\nWhen you talk about Web-enabling the data warehouse, the first, and perhaps the only\\nthought that comes to mind is the use of Web technology as an information deliverymechanism. Ironically, it rarely crosses your mind that Web content is a valuable and po-tent data source for your data warehouse. Y ou may hesitate before extracting data from theWeb for your Web-enabled data warehouse. \\nInformation content on the Web is so disparate and fragmented. Y ou need to build a\\nspecial search and extract system to sift through the mounds of information and pick upwhat is relevant for your data warehouse. Assume that your project team is able to buildsuch an extraction system, then selection and extraction consists of a few distinct steps.Before extraction, you must verify the accuracy of the source data. Just because data wasfound on the Web, you cannot automatically assume it is accurate. Y ou can get clues to ac-curacy from the types of sources. Please refer to Figure 16-2 showing an arrangement ofcomponents for data selection and extraction from the Web.382 DATA WAREHOUSING AND THE WEB\\nThe Web\\nData\\nSelection\\nand\\nExtraction\\nDATA\\nWAREHOUSEOperational\\nSystemsExternal\\nDataQueries Reports\\nData Warehouse Users\\nFigure 16-2 Web data for the data warehouse.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bf795854-7f66-4d37-bd76-02d7c606c54c', embedding=None, metadata={'page_label': '400', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='How can you use Web content to enrich your data warehouse? Here are a few impor-\\ntant uses:\\n/L50539Add more descriptive attributes to the business dimensions.\\n/L50539Include nominal or ordinal data about a dimension so that more options are avail-\\nable for pivoting and cross-tabulations.\\n/L50539Add linkage data to a dimension so that correlation analysis with other dimensions\\ncan be performed.\\n/L50539Create a new dimension table.\\n/L50539Create a new fact table.\\nWhat we have discussed here is much beyond the usual use of the Web as an informa-\\ntion delivery medium. Data selection and data extraction from the Web is a radically newparadigm. If your project team is willing to try it in your data warehouse environment, theresults will be worthwhile.\\nWEB-BASED INFORMATION DELIVERY\\nWe have seen how the convergence of Web technology and data warehousing is inevitable.\\nThe two technologies deal with providing information. Web technology is able to deliverinformation more easily, around the clock. It is no wonder that companies want to Web-enable their data warehouses. \\nOther advantages and possibilities also emerge when you connect the warehouse to the\\nWeb. One such benefit is the ability to come up with newer ways of making the data ware-house more effective through extranet data marts and the like. We also looked at the possi-bility of using the Web as a data source for your warehouse.\\nNevertheless, better information delivery remains the most compelling reason for\\nadapting the data warehouse for the Web. The Web brings a new outlook on informationdelivery and revolutionizes the process. Let us, therefore, spend some time on Web-basedinformation delivery. How does the Web escalate the usage of the data warehouse? Whatare the benefits and what are the challenges? How do you deal with the dramatic changein information delivery brought about by the Web?\\nExpanded Usage\\nNo matter how you look at it, the advantages of connecting the data warehouse to the\\nWeb appear to be amazing. Users can use the browser to perform their queries easilyand at any time during the day. There are no headaches associated with synchronizingdistributed data warehouses present in client/server environments. The usage of the datawarehouse expands beyond internal users. Interested parties from outside can now begranted the usage of the warehouse content. As the usage expands, scalability no longerposes a serious problem. What about the cost of training the users? Of course, trainingcosts are minimal because of the usage of the Web browser. Everything looks good andusage expands.\\nLet us understand what happens to the growth. Initially, your Web-enabled data ware-\\nhouse may receive only 500 to 5000 hits a day, but depending on your audience, this num-WEB-BASED INFORMATION DELIVERY 383', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='156d59e2-ff74-428a-b3da-723dbaedc28d', embedding=None, metadata={'page_label': '401', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ber can skyrocket within a short time. What does more users imply? Simply, more data.\\nAlso, more data in a 24 × 7 environment. The Web never shuts down. \\nExamine this phenomenon of extraordinary growth. Universal access generates a\\nwhole set of challenges that must be faced. Primarily, this imposes additional strain onyour Web-enabled data warehouse. Although scalability to accommodate more users andrapid expansion in the usage is no longer as strenuous as in a client/server environment, itis still a basic challenge. \\nLet us understand the pattern of this growth in usage. Two distinct factors foster the\\ngrowth: first, a totally open window that never shuts, next, an easy and intuitive accessmechanism via the ubiquitous Web browser. As a result, you have two challenges to con-tend with. The first is the increase in the user population. The second is the rapid acceler-ation of the growth. If you have opened your data warehouse to customers and businesspartners through an extranet, you will notice the steep expansion curve even more. \\nLet us call this extraordinary growth “supergrowth” and inspect the phenomenon.\\nPlease refer to Figure 16-3, which charts this phenomenon. The striking feature of super-growth manifests itself in your inability to scale up in time to contain the growth. The userbase will grow faster than your ability to scale up your Web-enabled data warehouse tomeet the expanding usage requirements. Y ou cannot simply add processors, disk drives, ormemory fast enough to meet the escalating demand, so how do you deal with super-growth?\\nLet us propose an initial approach. Is it possible to anticipate the problem and avoid it\\naltogether in the first place? In other words, can you control the growth and thereby cometo grips with the problem. When you Web-enable your data warehouse, the first instinct isto become overenthusiastic and open the warehouse instantly to the public. This is likeopening of the floodgates without warning. But in order to control the growth, you have tocontain your zeal and open up the warehouse in well-defined stages. This becomes ab-solutely necessary if the usage pattern is not very clear. Do not open the warehouse to the384 DATA WAREHOUSING AND THE WEB\\n0100200300400500600700800900\\nJULAUG SEPOCTNOV DECJANFEBMAR APRMAYJUN\\nSUPERGROWTH CHARTNUMBER OF USERS\\nFigure 16-3 Supergrowth of Web-enabled data warehouse.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='311ae9ec-9ca4-42e2-9474-cec5b21d574a', embedding=None, metadata={'page_label': '402', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='public at all, initially. First, let a few of your internal users have access. Then add some\\nmore users to the group. Include more and more in staged increments. In this manner, youcan constantly monitor the growth pattern in usage. Adopt the same cautionary approachwhen it comes to opening up the data warehouse to the public in the second wave. \\nWhat if your circumstances warrant opening up the Web-enabled data warehouse in\\none swoop to all users, internal and external? What if the staged approach described aboveis not feasible in your case? What if you cannot avoid supergrowth? The key to answeringthese questions lies in a specific characteristic of the growth curve. Notice that super-growth occurs only in the very initial stages. After the initial stages, the usage curveseems to level off, or at least the rate of increase becomes manageable. With this fact inmind, let us see how we can deal with supergrowth.\\nThe secret lies in finding the point up to which the usage will grow and then level off.\\nGo back to Figure 16-3 showing the supergrowth curve. The graph shows that the hyper-growth stage lasts until early November and is expected to level off at 750 users. Even ifyou start with 100 users, you are likely to hit the level of 750 users very soon. So, even ifthe intention is to start with just 100 users, let your initial offering of the Web-enableddata warehouse have enough resources to accommodate 750 users. But how do you comeup with the number of 750 users and that the point of leveling off is early November?There are no industry-standard charts to predict the supergrowth pattern. The supergrowthpattern of your data warehouse depends entirely on the circumstances and conditions ofyour environment. Y ou will have to define the graph for your environment by using thebest techniques for estimation. \\nNew Information Strategies\\nWhen the data warehouse and the Web technology converge, what are the expectations\\nfrom the data warehouse? How should the user interface to the data warehouse be modi-fied and enhanced on the Web model? Until your data warehouse was Web-enabled, userexpectations were governed by a set of defined standards. Now, after Web-enabling, whenusers approach the warehouse using the same browser as they do for other Internet data,the yardstick differs. Now the users expect the same type of information interface as theyare used to in an Internet session. Understanding these expectations will assist you in de-veloping new information delivery strategies for your Web-enabled warehouse.\\nInformation Delivery Guidelines. Let us summarize the guidelines for formulating\\nnew information delivery strategies. Study the following list.\\nPerformance. Industry experts agree on a less than 10-second response time for a\\npage to deliver the first screen of useful contents. The page may keep loading for alonger time, provided useful contents can be seen within 10 seconds. Design for themodem with the lowest speed. Display navigation buttons immediately. Reveal con-tents in a planned order: the useful ones immediately, followed by the ones at thenext levels of usefulness. Reconsider slow and redundant graphics. Use pagecaching techniques. Ensure that the physical database design enables fast responsetimes.\\nUser Options. Users are conditioned to expect to see a number of standard options\\nwhen they arrive at a Web page. These include navigation selections. Navigationbuttons for a Web-enabled data warehouse include drill-down options, home page,WEB-BASED INFORMATION DELIVERY 385', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c8468fe1-c9f8-4a45-90c7-04d1a11cd2fa', embedding=None, metadata={'page_label': '403', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='major subjects, website map, searches, and website sponsor details. Also, include\\nwarehouse-specific selections, help menus, and choices to communicate with thesponsoring company. More specifically, for a data warehouse, the user options mustinclude navigation to a library of reports, selections to browse the business dimen-sions and the attributes within each dimension, and interface to business metadata. \\nUser Experience. Every Web designer’ s fondest goal is to make each page a pleasing\\nexperience for the user. Users are eager to visit and stay on a page that is nicely for-matted. If there are too many distractions and problems, users tend to shun thepages. Be judicious in the use of fonts, colors, blinking graphics, bold texts, under-lines, audio clips, and video segments.\\nProcesses. It is important that business processes be made to work smoothly and seam-\\nlessly on the Web. For the Web-enabled data warehouse, this requirement translatesinto streamlining Web interaction during an analysis session. Let the user be able tomove from one step to the next, easily and gracefully.\\nUser Support. In a lengthy process, a user must have the reassurance that nothing will\\nbe lost in the middle of the process. The user must know where he or she is in theprocess so that proceeding further will not be interrupted. During access to theWeb-enabled data warehouse, let the user know the intermediary status information.For example, in running reports, provide the status of the reports to the users. \\nResolving Problems. The user must be able to backtrack from their mistakes, make\\ncorrections, and then proceed. The user must also be able to report problems. \\nInformation in Context. Opening the data warehouse by means of the Web provides a\\nnew slant on how information may be viewed in broader contexts. Hitherto, querierscould find answers to straight questions of “how much” and “how often.” Answersto such questions were found within narrow limits of the company’ s framework.With the opening of business intelligence through the Web, the circle widens to in-clude the entire supply chain and some competitive concerns. Information may nowbe obtained within the larger strategic framework.\\nPersonalized Information. It is almost impossible to provide predefined queries that\\nwill satisfy the requirements of everyone in the value chain, so strive to provide adhoc capabilities for asking any type of question related to any type of data in thewarehouse. \\nSelf-service Access. As the Web opens up the data warehouse to more users, both in-\\nside and outside the enterprise, tools must provide autonomous access to informa-tion. Users must be able to navigate and drill out to information sources. It must bepractically an environment with self-service access where users may serve them-selves. \\nHTML Files. In the Web-enabled data warehouse environment, the standard HTML\\ndocument or file, also known as the Web page, is the principal means for communication.The Web page is the resource for displaying information on the Internet or an internal net-work. The information interface tool generates HTML files from the ad hoc user queriesor from stored procedures in the database.\\nY ou can generate an HTML file one time or on a regular basis using triggers. As you\\nknow, a trigger is a special kind of stored procedure that is automatically executed when aspecified data manipulation statement on a specific table is encountered. For example, atrigger program can automatically generate an exception report in the form of an HTML386 DATA WAREHOUSING AND THE WEB', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='614fcc48-19d7-4e0a-a433-5c4ae3e18619', embedding=None, metadata={'page_label': '404', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='file when some predefined threshold values are exceeded. The users can then view the lat-\\nest occurrence by calling up the corresponding Web page. \\nDatabase management systems offer publish-and-subscribe functions in their database\\nengine. A subscription enables HTML pages to be created from scratch or refreshedwhenever a trigger fires in the specified data source. A published HTML page can includefiltered data directly from one or more tables, the result of a query translated internallyinto SQL statements, or an output from a stored procedure. However, the subscription fa-cility is limited to the data within the particular database. \\nReporting As a Strategic Tool. Next let us turn to reporting as a method of Web-\\nbased information delivery. On the Web, you are able to publish or distribute files via e-mail. These features open up enormous possibilities for reporting as a strategic tool. Nowyou can integrate business partners into the supply chain. Managers and executives maydirect prescribed reports to be sent automatically to specific customers and suppliers.They may set thresholds on inventory levels and have reports sent only when the levels areoutside the limits. \\nReport management can cover both types of reports. Routine reports and exception re-\\nports may be scheduled for distribution. Y ou can create a number of parameter-driven re-ports that can be made available through the Web. Y ou may even label the reports withbusiness names and categorize them according to classes of users, depending on rank orsecurity authorization levels. \\nSeveral report management techniques are possible: parameter-driven reports, cus-\\ntomizable reports, exception reports, predefined reports, and so on. One technique canprovide for OLAP drill-down. In this technique, the user requests a first report showingresults at high summary levels. This report may be used to proceed further with the analy-sis. When the first report comes back, that report serves as the launch pad for furtheranalysis. The user changes the request parameters and drills down on the summary datafor additional details without having to create a new report. Another useful technique re-lates to on-demand pages. When the report with several pages comes back, the user cannavigate to the desired page via hyperlinks instead of paging through, one page at a time. \\nBrowser Technology for the Data Warehouse\\nWeb technology and browser technology are almost synonymous. Browsers are the com-\\nmon client software in a Web-enabled data warehouse environment. Y our users will accessinformation using a standard browser. Let us go over some details so that you can becomefamiliar with browser technology. A browser-based application comes with many bene-fits. The user interface—the browser—is practically free. Y ou need not configure and in-stall a browser-based application on the client; the application runs on a server. Rightaway, you observe that deployment of the application becomes easy even when there arehundreds or thousands of desktops. \\nAt present, four technologies are commonly used to build Web-enabled user interfaces.\\nThese are HTML, Java, ADO, and plug-ins. Look at Figure 16-4, which compares the fourtechnologies in terms of strengths and weaknesses.\\nPlease study the following brief descriptions of the four technologies:\\nHTML. HTML, the simplest and easiest to manage technology, works on any browser\\nregardless of the platform. Users can navigate by clicking on hyperlinks. HTML supportsWEB-BASED INFORMATION DELIVERY 387', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8e37e035-73b6-420a-88e9-61dc285a2b8f', embedding=None, metadata={'page_label': '405', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='graphics and forms. It is “stateless,” meaning that the context of the network link between\\nthe browser and the application is not maintained between applications. Y ou may simulateOLAP features such as pivoting and drill-down by generating new HTML pages. But youhave to pay the price of waiting for the result page to be generated and downloaded to thedesktop. HTML is good for static reports. It is very suitable when your application doesnot know the features of the target platform.\\nJava. Do you need advanced 3-D visualization, drill-through, drag and drop, or similar\\nhigh-end functionality? Then Java is the technology for you. Java is available on all majorclient platforms. As Java applets are not allowed to write to hard drives or print to localprinters, for some applications this could pose a problem. Because Java is an interpretivelanguage, it is somewhat slower than compiled languages. The desktop must be equippedwith a Java-enabled browser. As Java applets have to be downloaded from the server everytime, sometimes the long download times may not be acceptable. Java is suitable for aninteractive client, where long load times may not be a factor.\\nADO. This is Microsoft’ s solution for distributed Web-based systems. ADO, implement-\\ned as Microsoft DLLs or data link libraries, can be installed by downloading from a serverusing a browser. As expected, ADO runs only on Windows platforms, thereby excludingUNIX and Mac configurations. Being a compiled interface, ADO is faster than Java.ADO/MD, Microsoft’ s extension to ADO as part of Pivot-Table Services, can be used tocreate ActiveX controls in Visual Basic to manipulate data in OLAP services from a Webpage. ADO is restricted to Windows platforms where you have good control of DLLs.\\nPlug-ins. These are browser-specific programs that execute within the browser itself.\\nPlug-ins can be installed on the local drives. Because each browser needs its own plug-in,388 DATA WAREHOUSING AND THE WEB\\nWorks with any browser. \\nPlatform-independent. Open \\nstandard. Static graphics.Only moderately interactive. \\nStatic pages. Some platform \\nlimitations with dynamic HTML.\\nPlatform-independent. \\nIncreased popularity. Added \\nsecurity. Long load times. Interpretive \\nlanguage. Could be a problem \\nwith older browsers.\\nWindows environment. Widely \\nknown and used. Compiled \\ncode. Better performance.Exclusion of non-Windows \\nplatforms. Potential security \\nproblems and DLL glitches.\\nTightly coupled with the \\nbrowser. Compiled code and, \\ntherefore,  better performance. Sometimes plug-in size may be a \\nproblem for network traffic and \\ndownload times. Browser-specific. STRENGTHS WEAKNESSES\\nHTML\\nPlug-inADOJava\\nFigure 16-4 Web interface technologies.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9a449f4a-e2e5-49f1-aee2-55d937be1457', embedding=None, metadata={'page_label': '406', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='you may want to standardize the browser in your environment if you choose this ap-\\nproach. OLAP clients on multiple platforms, especially those using Java, may have aproblem because of limitations in bandwidth. \\nSecurity Issues\\nWithout a doubt, when you open up your Web-enabled data warehouse to users through-\\nout the enterprise via an intranet and to business partners outside via an extranet, you tendto maximize the value. Depending on your organization, you may even derive more valuewhen you take the next step and open the warehouse to the public on the Internet. Butthese actions raise serious security issues. Y ou may have to impose security restrictions atdifferent levels. \\nAt the network level, you may look into solutions that support data encryption and re-\\nstricted transfer mechanisms. Security at the network level is just one piece in the protec-tion scheme. Carefully institute a security system at the application level. At this level, thesecurity system must manage authorizations on who is allowed to get into the applicationand what each user is allowed to access.\\nHave you heard of the Information Terrorist? This disloyal or undependable employ-\\nee who has authorization to access secured information is a great threat to the securityof the warehouse. Plugging this hole is difficult and you need to address this aspect ofsecurity.\\nOLAP AND THE WEB\\nLarge amounts of time and money are invested in building data warehouses in the hope\\nthat the enterprise will get the information it needs to make strategic decisions of lastingvalue. For maximizing the value potential, you need to cater to as large a user group aspossible to tap into the potential of the warehouse. This includes the extension of OLAPcapabilities to a larger group of analysts. \\nEnterprise OLAP\\nThe early warehouses started as small-scale decision support systems for a selected hand-\\nful of interested analysts. Early mainframe decision support systems provided powerfulanalytical capabilities although quite incomparable to today’ s OLAP systems. Becausethose systems were difficult to use, they seldom reached beyond a small group of analystswho could plough through the difficulties. \\nThe next generation of decision support systems replaced complex mainframe comput-\\ning with easy-to-use GUIs and point-and-click interfaces. These second-generation sys-tems running on client/server architecture were gradually able to support OLAP in addi-tion to simple querying and reporting. Still, deployment and maintenance costs preventedextension of decision support to larger number of users. OLAP and OLAP-like capabili-ties were still limited to a small number of users. \\nThe Web has put a dramatically different slant on information delivery. Web-enabled\\ndata warehouses can open their doors to a large group of users both within and outside theenterprise, and OLAP services be extended to more than a select group of analysts. Thequestion arises: Can OLAP systems scale up to support a large number of concurrentOLAP AND THE WEB 389', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d3113a43-7255-41b4-8e19-87271fc185f0', embedding=None, metadata={'page_label': '407', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='users performing complex queries and intensive calculations? How can your project team\\nensure that OLAP is successful in your Web-enabled data warehouse?\\nWeb-OLAP Approaches\\nThe underlying combination for a successful implementation is comprised of Web tech-\\nnology, the data warehouse with its OLAP system, and a thin-client architecture. How doyou implement OLAP in such an environment? How will the OLAP system work in yourWeb-enabled data warehouse? What kind of client and Web architecture will produce theoptimum results? Y ou may approach these questions in three different ways.\\n1.Browser Plug-ins. In the first approach, you use plug-ins or browser extensions.\\nThis is just a slightly modified version of fat-client Windows implementation ex-cept that the client configuration is more towards that of a thin client. Support is-sues creep in and this approach has scalability problems.\\n2.Precreated HTML Documents. In the next approach, you provide precreated\\nHTML documents along with the navigation tools to find these. The documents areresult sets of analytical operations. This approach takes advantage of Web technolo-gy and thin-client economy, but users are confined to using predefined reports. Theapproach is devoid of on-demand analysis; users cannot do typical online analyticalprocessing. \\n3.OLAP in the Server. The best approach is to use the server to do all online analyti-\\ncal processing and present the results on a true thin-client information interface.This approach realizes the economic benefits of the Web and thin-client architec-ture. At the same time, it provides an integrated server environment irrespective ofthe client machines. Maintenance is minimized because applications and logic arecentralized on the server. Version control is also consistent. Everyone shares thesame components: server, metadata, and reports. This approach works well in pro-duction environments. \\nOLAP Engine Design\\nWhen the data warehouse is Web-enabled and the level of OLAP operations raised, the de-\\nsign of the OLAP engine determines possibilities for scaling up. In the OLAP product youchoose for your Web-enabled data warehouse, OLAP engine design ranks high in critical-ity. A properly designed engine produces a performance curve that stays linear as thenumber of concurrent users increases. Let us consider some options:\\nDependence on the RDBMS. The OLAP engine relies completely on the RDBMS\\nto perform multidimensional processing, generating complex, multipass SQL to accesssummary data. Joins, aggregations, and calculations are all done within the database, pos-ing serious problems for Web-enabled systems. A large number of temporary tables be-come necessary. The overhead for creating, inserting, dropping, allocating disk space,checking permissions, and modifying system tables for each calculation is enormous. Justfive concurrent users may bring the OLAP system to its knees.\\nDependence on the Engine. Here the engine generates SQL to access summary\\ndata and performs all processing on a middle tier. Y ou will observe two problem areas in390 DATA WAREHOUSING AND THE WEB', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4dc9eacc-1f97-42bb-9397-8640d15b05f0', embedding=None, metadata={'page_label': '408', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='this approach. Heavy network traffic and large memory requirements make this approach\\nundesirable. Y ou may get a linear performance curve, but the curve is likely to be too steepbecause the potential of the DBMS is not being used.\\nIntelligent OLAP Engine. The engine has the intelligence to determine the type of\\nrequest and where it will be performed optimally. Because of its intelligence, the engine isable to distribute the joins, aggregations, and calculations between the engine componentand the RDBMS. In this model, you are able to separate the presentation, logic, and datalayers both logically and physically. Therefore, system processing is balanced and networktraffic is optimized. Currently, this seems to be the best approach, achieving a perfor-mance curve that remains linear with a gradual inclination as the number of concurrentusers increases.\\nBUILDING A WEB-ENABLED DATA WAREHOUSE\\nLet us summarize what we have covered so far. We perceived how the Web has changed\\neverything including the design and deployment of data warehouses. We understood howWeb technology and data warehousing have converged, opening up remarkable possibili-ties. The primary purpose of the data warehouse is to provide information, and the Webmakes the providing of information easy. What a nice combination of technologies! Nowthe value of your data warehouse can be extended to a wider array of users. \\nWhen we match up the features of the data warehouse with the characteristics of the\\nWeb, we observe that we have to do a number of things to the design and deploymentmethods to adapt the warehouse to the Web. We went through most of the tasks. The Webhas changed the way information is delivered from the data warehouse. Web-based infor-mation delivery is more inclusive, much easier to use, but also different from the tradi-tional methods. We have spent some time on Web-based information delivery. We alsotouched on OLAP in relation to the Web. So, where are we now? We are now ready to re-view considerations for building a Web-enabled data warehouse. \\nNature of the Data Webhouse\\nIn mid-1999, Dr. Ralph Kimball popularized a new term, “data Webhouse,” which includ-\\ned the notion of a Web-enabled data warehouse. He declared that the data warehouse istaking central stage in the Web revolution. He went on to state that this requires restatingand adjusting our data warehouse thinking. How true! \\nIn attempting to formulate the principles for building a Web-enabled data warehouse,\\nlet us first review the nature of the data Webhouse. We will use this knowledge to definethe implementation considerations. Before going over the main features, look at Figure16-5, which gives you a broad overview of a data Webhouse. Now let us review the fea-tures. Here is a list of the principal features of the data Webhouse:\\n/L50539It is a fully distributed system. Many independent nodes make up the whole. As Dr.\\nKimball would say, there is no center to the data Webhouse.\\n/L50539It is a Web-enabled system; it is beyond a client/server system. The distribution of\\ntasks and the arrangement of the components are radically different.BUILDING A WEB-ENABLED DATA WAREHOUSE 391', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='33525fee-9e3a-4310-ae01-7dd38aa04f15', embedding=None, metadata={'page_label': '409', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539The Web browser is the key to information delivery. The system delivers the results\\nof requests for information through remote browsers.\\n/L50539Because of its openness, security is a serious concern.\\n/L50539The Web supports all data types including textual, numeric, graphical, photograph-\\nic, audio, video, and more. It therefore follows that the data Webhouse supportsmany forms of data.\\n/L50539The system provides results to information requests within reasonable response\\ntimes.\\n/L50539User interface design is of paramount importance for ease of use and for effective\\npublication on the Web. Unlike the interfaces in other configurations, the Web has adefinite method to measure the effectiveness of the user interface. The analysis ofthe clickstream data tells you how good the interface is.\\n/L50539By nature, the data Webhouse necessitates a nicely distributed architecture compris-\\ning small-scale data marts.\\n/L50539Because the arrangement of the components is based on a “bus” architecture of\\nlinked data marts, it is important to have fully conformed dimensions and complete-ly conformed or standardized facts.\\n/L50539The Web never sleeps. Y our data Webhouse is expected to be up all the time.\\n/L50539Finally, remember that the data Webhouse is meant to be open to all groups of users,\\nboth inside and outside the enterprise—employees, customers, suppliers, and otherbusiness partners.392 DATA WAREHOUSING AND THE WEB\\nEnterprise \\nData \\nWarehouse\\nInternal Users within the EnterpriseINTERNETEXTRANET\\nINTRANETExternal Business Partners\\nPublic Visitors & Customers\\nFigure 16-5 Data Webhouse: a broad overview.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9bf7960f-0300-4309-b70f-c6390e38fdf5', embedding=None, metadata={'page_label': '410', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Implementation Considerations\\nThe major features described above lead us to factors you need to consider for implement-\\ning a Web-enabled data warehouse. Each feature listed above demands readjustment ofthe implementation principles. Mostly, by going through a list of features, you can derivewhat is required. We want to highlight just a few implementation considerations that arecrucial. \\nIf the data Webhouse is expected to be widely distributed, how could you manage it?\\nHow could you make all the components of the architecture work together and makesense? Do you not feel that without something in the middle, it seems impossible to makeit work? In the real world, many of the connected groups may be using different technolo-gies and different platforms. How could you tie them all together? From where?\\nPlease study the following observations carefully:\\n/L50539In order to achieve basic architectural coherence among the distributed units, fer-\\nvently adopt dimensional modeling as the basic modeling technique. \\n/L50539Use the data warehouse bus architecture. This architecture, with its fully conformed\\ndimensions and completely standardized facts, is conducive to the flow of correctinformation. \\n/L50539In a distributed environment, who conforms the dimensions and facts? In earlier\\nchapters, we have already discussed the meaning of conforming the dimensions andfacts. Basically, the implication is to have the same definitions throughout. Onesuggestion is to centralize the definitions of the conformed dimensions and the con-formed facts. This need not be physical centralization; logical centralization willwork. This centralization gives the semblance of a center to the data Webhouse.\\n/L50539Still the question remains: who actually conforms the dimensions and facts? The an-\\nswer depends on what will work for your environment. If feasible, assign the task ofconforming the dimensions and facts to the local groups of participants. Each groupgets the responsibility to come up with the definitions for a dimension or a set offacts.\\n/L50539Well, how do all units become aware of the complete set of definitions for all di-\\nmensions and facts? This is where the Web comes in handy. Y ou can have the defin-itions published on the Web; they then become the standards for conformed dimen-sions and facts.\\n/L50539How do you physically implement the conformed dimension tables and the con-\\nformed fact tables? Dimension tables are often physically duplicated. Again, seewhat is feasible in your environment. Total physical centralization of all the dimen-sion tables may not be practical, but the conformed fact tables are rarely duplicated.Generally, fact tables are very large in comparison to the dimension tables.\\n/L50539One last consideration. Now we understand the data Webhouse as a distributed set\\nof dimensions and facts based on possibly dissimilar database technologies. Howcan you make such a distributed collection work as a cohesive whole? This is whatthe query tool or the report writer is required to do in such a distributed configura-tion. Let us say one of the remote users executes a specific query. The query toolmust establish connections to each of the required fact table providers and retrieveresult sets that are constrained by the conformed dimensions. Then the tool mustcombine all the retrieved result sets in the application server using a single-passBUILDING A WEB-ENABLED DATA WAREHOUSE 393', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bb98055b-710a-4aeb-aa24-5ff065de8bcc', embedding=None, metadata={'page_label': '411', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='sort-merge. The combination will produce the correct final result set simply be-\\ncause the dimensions are all conformed. \\nPutting the Pieces Together\\nIn this subsection, let us go over the various components that need to be pulled together to\\nmake up the Web-enabled data warehouse. Consider the following list:\\n/L50539The data Webhouse configuration is beyond client/server computing. The usual\\ntwo-tier or three-tier technology is inadequate. As the number of users rise upsharply, new servers must be added without any difficulty. Therefore, consider adistributed component architecture. \\n/L50539With the user nodes spread out, you must strive for minimum administration on the\\nclient side. True thin-client technology like Java is likely to provide a zero-adminis-tration client setup. \\n/L50539The client technology is expected to be a combination of thin clients and full clients.\\nEnsure complete metadata integration. Both IT and the variety of user types willbenefit from unified metadata.\\n/L50539Select the right database to support the distributed environment. As you are likely to\\nuse Java, a RDBMS with the Java engine in the database would prove useful.\\n/L50539In many Web applications, the HTTP server becomes a point of congestion as all\\ndata from a session is fed to the browser through this server. Y ou will find scalabili-ty to be difficult unless you implement a CORBA model. CORBA provides distrib-uted object computing and scalability because the server and the client communi-cate via CORBA. \\n/L50539Ensure that you pay enough attention to administration and maintenance. This\\nshould include identification of the dimensions, hierarchies within dimensions,facts, and summaries. Summary management could be difficult.\\n/L50539The Web interface consists of a browser, search engine, groupware, push technolo-\\ngies, home pages, hypertext links, and downloaded Java and ActiveX applets.\\n/L50539Tools supporting HTML can be universally deployed. However, for complex analy-\\nsis, HTML is cumbersome. Use HTML as much as possible and reserve Java orplug-ins for complex ad hoc analysis.\\nWeb Processing Model\\nFirst let us look at a Web architecture configuration. See Figure 16-6 showing the overall\\narrangement. Notice that the architecture is more complex that a two-tier or three-tierclient/server architecture. Y ou need additional tiers to accommodate the requirements ofWeb computing. At a minimum, you need to have a Web server between the browserclients and the database. Also, note the firewall to protect your corporate applicationsfrom outside intrusions.\\nThis covers the overall architecture. See Figure 16-7 showing a model for delivering\\ninformation. The model illustrates how HTML pages are translated into SQL queriespassed on to the DBMS using CGI scripts. This model shows the components for infor-mation delivery through HTML pages. The model may be generalized to illustrate othertechnologies.394 DATA WAREHOUSING AND THE WEB', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='93c57bde-6cc0-43c1-88c1-b965b858849e', embedding=None, metadata={'page_label': '412', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BUILDING A WEB-ENABLED DATA WAREHOUSE 395\\nInternet \\nClient\\nMDDBOLAP \\nServer\\nData \\nWarehouse\\nDatabase \\nServerFirewall\\nWeb Server\\nSource \\nSystemsInternet\\nNetwork  Backbone\\nFigure 16-6 Architecture of a Web-enabled data warehouse.\\nWeb \\nServerWeb \\nBrowser\\nQuery \\nEngineStructured \\nData Content\\nUnstructured \\nData ContentHTMLHTML\\nSQLCGI\\nFigure 16-7 Web processing model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3011db4f-28f8-49e1-805e-695dadb341dd', embedding=None, metadata={'page_label': '413', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER SUMMARY\\n/L50539The Web is the most dominant computing phenomenon of the 1990s and its technol-\\nogy and data warehousing are converging to produce dramatic results. \\n/L50539A Web-enabled data warehouse adapts the Web for information delivery and collab-\\noration among the users.\\n/L50539Adapting the data warehouse to the Web means including features such as the infor-\\nmation “push” technique, ease of usage, speedy response, no downtime, multimediaoutput, and scalability. \\n/L50539Web-based information delivery expands the data warehouse usage and opens up\\nnew information strategies.\\n/L50539The combination of OLAP and Web technologies produces great benefits for users.\\n/L50539Because of the open nature of the Web, adapting the data warehouse to the Web\\ncalls for serious implementation considerations.\\nREVIEW QUESTIONS\\n1. Describe briefly two major features of the Web-enabled data warehouse.\\n2. How do the Internet, intranet, and extranet apply to the data warehouse?3. What are the expectations of users from a Web-enabled data warehouse?4. How can you use the Web as a data source for your data warehouse? What types of\\ninformation can you get from the Web?\\n5. Name any four standard options on a Web page delivering information from the\\ndata warehouse.\\n6. What are the four common technologies to build Web-enabled user interfaces for\\nyour data warehouse?\\n7. Why is data security a major concern for a Web-enabled data warehouse?8. List any four features of the data Webhouse.9. Name any two approaches for an OLAP system to function in a Web-enabled data\\nwarehouse.\\n10. What is a data warehouse bus architecture? How does it fit in a Web-enabled data\\nwarehouse?\\nEXERCISES\\n1. Indicate if true or false:\\nA. The extranet excludes outside business partners from accessing the company’ s\\ndata warehouse.\\nB. Web technology in a data warehouse opens up multiple information formats to\\nusers.\\nC. Better information delivery to users is the only reason for Web-enabling your\\ndata warehouse.396 DATA WAREHOUSING AND THE WEB', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0238f432-6730-4ef5-a789-709177f23419', embedding=None, metadata={'page_label': '414', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='D. Supergrowth is a rare phenomenon in a Web-enabled data warehouse environ-\\nment.\\nE. The Web promotes “self-service” access.F . The data Webhouse necessitates a distributed architecture.G. Web technology makes it easier to add more users to the data warehouse.H. The Web and its features are not compatible with the data warehouse.I. For its information interface, the Web-enabled data warehouse is unable to use\\nboth “push” and “pull” techniques.\\nJ. In a Web-enabled data warehouse, the standard HTML or XML file is the prin-\\ncipal means for communication.\\n2. Web-based information in a data warehouse results in “supergrowth.” Discuss this\\nphenomenal growth and describe how you will provide for supergrowth.\\n3. Y our project team has been directed to provide all report generation and delivery\\nover the Internet. Create a plan for adopting the Web technology for all reportingfrom your data warehouse. Discuss all the implications. \\n4. The Web-enabled data warehouse has no center. It is a distributed environment. An-\\nalyze the implications of these statements. From this perspective, what are the con-siderations for maintaining the dimension and fact tables?\\n5. As the Web specialist on the project team, prepare a document highlighting the ma-\\njor considerations for Web-enabling your data warehouse. Just list the considera-tions, not the implementation techniques.EXERCISES 397', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='75b1ea13-342f-49cc-aaa7-e9b85526d529', embedding=None, metadata={'page_label': '415', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 17\\nDATA MINING BASICS \\nCHAPTER OBJECTIVES\\n/L50539Learn what exactly data mining is and examine its features\\n/L50539Compare data mining with OLAP and understand the relationships and differences\\n/L50539Notice the place for data mining in a data warehouse environment\\n/L50539Carefully go through the important data mining techniques and understand how\\neach works\\n/L50539Study a few data mining applications in different industries and perceive the appli-\\ncation of the technology to your environment\\nY ou have most certainly heard about data mining. Most of you know that the technolo-\\ngy has something to do with discovering knowledge. Some of you possibly know that datamining is used in applications such as marketing, sales, credit analysis, and fraud detec-tion. All of you know vaguely that data mining is somehow connected to data warehous-ing. Data mining is used in just about every area of business from sales and marketing tonew product development to inventory management and human resources. \\nThere are perhaps as many variations in the definition of data mining as there are ven-\\ndors and proponents. Some experts include a whole range of tools and techniques fromsimple query mechanisms to statistical analysis in the definition. Others restrict the defin-ition to knowledge discovery techniques. A workable data warehouse, although not a pre-requisite, will give a practical boost to the data mining process.\\nWhy is data mining being put to use in more and more businesses? Here are some ba-\\nsic reasons:\\n/L50539In today’ s world, an organization generates more information in a week than most\\npeople can read in a lifetime. It is humanly impossible to study, decipher, and inter-pret all that data to find useful information. \\n399Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3ee9eac7-7357-4f32-b383-cff9442ff36f', embedding=None, metadata={'page_label': '416', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539A data warehouse pools all the data after proper transformation and cleansing into\\nwell-organized data structures. Nevertheless, the sheer volume of data makes it im-possible for anyone to use analysis and query tools to discern useful patterns. \\n/L50539In recent times, many data mining tools suitable for a wide range of applications\\nhave appeared in the market. We are seeing the maturity of the tools and products. \\n/L50539Data mining needs substantial computing power. Parallel hardware, databases, and\\nother powerful components are becoming very affordable.\\n/L50539As you are aware, organizations are placing enormous emphasis on building sound\\ncustomer relationships, and for good reasons. Companies want to know how theycan sell more to existing customers. Organizations are interested in determiningwhich of their customers will prove to be of long-term value to them. Companiesneed to discover any existing natural classifications among their customers so thatthe classifications may be properly targeted with products and services. Data min-ing enables companies to find answers and discover patterns in their customer data.\\n/L50539Finally, competitive considerations weigh heavily on your company to get into data\\nmining. Perhaps your company’ s competition is already using data mining.\\nWHAT IS DATA MINING?\\nBefore providing some formal definitions of data mining, let us try to understand the\\ntechnology in a business context. Like all decision support systems, data mining deliversinformation. Please refer to Figure 17-1 showing the progression of decision support.Note the earliest approach, when primitive types of decision support systems existed.Next came database systems providing more useful decision support information. In the400 DATA MINING BASICS\\nEarly File -\\nbased SystemsData\\nWarehousesDatabase\\nSystemsOLAP\\nSystemsData Mining\\nApplications\\nBasic\\naccounting\\ndataOperating\\nsystems\\ndataSelected\\nand extracted\\ndataData for\\nmulti -\\ndimensional\\nAnalysisData for\\nDecision\\nSupport\\nNo Decision\\nSupportPrimitive\\nDecision\\nSupportTrue\\nDecision\\nSupportComplex\\nAnalysis &\\nCalculationsKnowledge\\nDiscovery\\nSpecial\\nReportsQueries\\nand\\nReportsAd Hoc\\nQueries /\\nAnalysisDiscovered\\nPatterns /\\nRelationships\\nUSER -DRIVEN DATA -DRIVEN\\nFigure 17-1 Decision support progresses to data mining.Operational', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='890dbdf8-76aa-4a7f-8d3d-df4aad7343c5', embedding=None, metadata={'page_label': '417', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1990s, data warehouses with query and report tools to assist users in retrieving the types\\nof decision support information they need began to be the primary and valuable source ofdecision support information. For more sophisticated analysis, OLAP tools became avail-able. Up to this point, the approach for obtaining information was driven by the users. Butthe sheer volume of data renders it impossible for anyone to use analysis and query toolsto discern useful patterns. For example, in marketing analysis, it is almost impossible tothink through all the probable associations and gain insights by querying and drillingdown into the data warehouse. Y ou need a technology that can learn from past associationsand results, and predict customer behavior. Y ou need a tool that will accomplish the dis-covery of knowledge by itself. Y ou want a data-driven approach and not a user-driven one.This is where data mining steps in and takes over from the users.\\nProgressive organizations gather enterprise data from the source operational systems,\\nmove the data through a transformation and cleansing process, and store the data in datawarehouses in a form suitable for multidimensional analysis. Data mining takes theprocess a giant step further.\\nData Mining Defined\\nAs an analogy, imagine a very wide and very deep pit densely packed with some impor-\\ntant material. Y ou use a set of sophisticated drilling tools to dig and unravel the contents.Y ou do not know what exactly you hope to get from your effort. Nothing may turn up, oryou may be fortunate to find some real gold nuggets. Y ou may discover this valuable trea-sure that you never knew was there in the first place. Y ou were not specifically looking fornuggets. Y ou did not know they were there or if they ever existed. Please refer to Figure17-2 crudely depicting this scenario.\\nNow, as a change of scenery, replace the very wide and very deep pit with your dataWHAT IS DATA MINING? 401\\nData Mining \\nToolSelected, \\nextracted, and \\nprepared dataNuggets of \\ninformation \\ndiscovered (patterns \\n/ relationships)\\nFigure 17-2 Mining for nuggets.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ff51d2ad-58df-4a71-aedd-2e34a91faded', embedding=None, metadata={'page_label': '418', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='warehouse. Replace the material in the pit with the massive data content in your data\\nwarehouse and replace the drilling tools by data mining tools. The gold nuggets are thevaluable pieces of information, such as patterns or relationships, you never knew existedin the data. In fact, you had applied the data mining tools to find something worthwhileyou did not know existed. This is one aspect of data mining. Data mining is synonymouswith knowledge discovery—discovering some aspect of knowledge you never suspectedto exist.\\nIf you do not know that some pattern or relationship even exists, how do you direct the\\ndata mining tool to find it? For a mortgage bank, how does the data mining tool know thata nugget of information exists indicating that most homeowners who are likely to defaulton their mortgages belong to a certain classification of customers? \\nIf knowledge discovery is one aspect of data mining, prediction is the other. Here you\\nare looking for a specific association with regard to an event or condition. Y ou know thatsome of your customers are likely to buy upscale if they are properly campaigned. Y ouwould like to predict the tendency to upscale. Y our customer data may contain interestingassociations between tendency to upscale and age, income level, and marital status. Y ouwould like to discover the factors leading to the tendency to upscale and predict which ofyour customers are likely to move up in their buying patterns. Prediction is the other as-pect of data mining.\\nSo, what is data mining? It is a knowledge discovery process. Data mining helps you\\nunderstand the substance of the data in a special unsuspected way. It unearths patterns andtrends in the raw data you never knew existed. \\n“Data mining,” writes Joseph P . Bigus in his book, Data Mining with Neural Networks,\\nis the efficient discovery of valuable, non-obvious information from a large collection ofdata.” Data mining centers around the automated discovery of new facts and relationshipsin data. With traditional query tools, you search for known information. Data mining toolsenable you to uncover hidden information. The assumption is that more useful knowledgelies hidden beneath the surface. \\nThe Knowledge Discovery Process\\nIn the above discussion, we have described data mining as the process of knowledge dis-\\ncovery. Data mining discovers knowledge or information that you never knew was presentin your data. What about this knowledge? How does it show up? Usually, the uncoveredhidden knowledge manifests itself as relationships or patterns. Try to understand the kindsof relationships or patterns that are discovered.\\nRelationships. Take the example of your going to the nearby supermarket on the way\\nhome to pick up bread, milk, and a few other “things.” What other things? Y ou are notsure. While you fetch the milk container, you happen to see a pack of assorted cheesesclose by. Y es, you want that. Y ou pause to look at the next five customers behind you. Toyour amazement, you notice that three of those customers also reach for the cheese pack.Coincidence? Within a space of five minutes, you have seen milk and cheese bought to-gether. \\nNow on to the bread shelf. As you get your bread, a bag of barbecue potato chips catch-\\nes your eye. Why not get that bag of potato chips? Now the customer behind you alsowants bread and chips. Coincidence? Not necessarily. It is possible that this supermarketis part of a national chain that uses data mining. The data mining tools have discovered the402 DATA MINING BASICS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ab83b2a7-2fcd-489d-8582-006857366b95', embedding=None, metadata={'page_label': '419', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='relationships between bread and chips and between milk and cheese packs, especially in\\nthe evening rush hour. So the items must have been deliberately placed in close proximity. \\nData mining discovers relationships of this type. The relationships may be between two\\nor more different objects along with the time dimension. Sometimes, the relationshipsmay be between the attributes of the same object. Whatever they may be, discovery of re-lationships is a key result of data mining.\\nPatterns. Pattern discovery is another outcome of data mining operations. Consider a\\ncredit card company trying to discover the pattern of usage that usually warrants increasein the credit limit or a card upgrade. They would like to know which of their customersmust be lured with a card upgrade and when. The data mining tools mine the usage pat-terns of thousands of card-holders and discover the potential pattern of usage that willproduce results in a marketing campaign. \\nBefore you engage in data mining, you must determine clearly what you want the tool\\nto accomplish. At this stage, we are not trying to predict the knowledge you expect to dis-cover, but to define the business objectives of the engagement. Let us walk through themajor phases and steps. First look at Figure 17-3 indicating four major phases, then readthe following brief descriptions of the detailed steps.\\nStep 1: Define Business Objectives. This step is similar to any information system\\nproject. First of all, determine whether you really need a data mining solution. State yourobjectives. Are you looking to improve your direct marketing campaigns? Do you want todetect fraud in credit card usage? Are you looking for associations between products thatsell together? In this step, define expectations. Express how the final results will be pre-sented and used in the operational systems.\\nStep 2: Prepare Data. This step consists of data selection, preprocessing of data, and\\ndata transformation. Select the data to be extracted from the data warehouse. Use the busi-WHAT IS DATA MINING? 403\\nFigure 17-3 Knowledge discovery phases.DETERMINATION \\nOF BUSINESS \\nOBJECTIVESSELECTION AND \\nPREPARATION \\nOF DATAAPPLICATION OF \\nSUITABLE DATA MINING \\nTECHIQUESEVALUATION AND \\nAPPLICATION OF \\nRESULTS\\nPercentage of Total Effort20%45%15%20%\\nAPPLICATION OF\\nSUITABLE DATA MINING\\nTECHNIQUES', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ce015f06-1303-4590-82cc-14a5874531b2', embedding=None, metadata={'page_label': '420', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ness objectives to determine what data has to be selected. Include appropriate metadata\\nabout the selected data. By now, you also know what type of mining algorithm you will beusing. The mining algorithm has a bearing on data selection. The variables selected fordata mining are also known as active variables. \\nPreprocessing is meant to improve the quality of selected data. When you select from the\\ndata warehouse, it is assumed that the data is already cleansed. Preprocessing could also in-volve enriching the selected data with external data. In the preprocessing substep, removenoisy data, that is, data blatantly out of range. Also ensure that there are no missing values.\\nClearly, if the data for mining is selected from the data warehouse, it is again assumed\\nthat all the necessary data transformations have already been completed. Make sure thatthis really is the case.\\nStep 3: Perform Data Mining. Obviously, this is the crucial step. The knowledge dis-\\ncovery engine applies the selected algorithm to the prepared data. The output from thisstep is a set of relationships or patterns. However, this step and the next step of evaluationmay be performed in an iterative manner. After an initial evaluation, you may adjust thedata and redo this step. The duration and intensity of this step depend on the type of datamining application. If you are segmenting the database, not too many iterations are need-ed. If you are creating a predictive model, the models are repeatedly set up and tested withsample data before testing with the real database. \\nStep 4: Evaluate Results. Y ou are actually seeking interesting patterns or relation-\\nships. These help you in the understanding of your customers, products, profits, and mar-kets. In the selected data, there are potentially many patterns or relationships. In this step,you examine all the resulting patterns. Y ou will apply a filtering mechanism and selectonly the promising patterns to be presented and applied. Again, this step also depends onthe specific kind of data mining algorithm applied.\\nStep 5: Present Discoveries. Presentation of knowledge discoveries may be in the\\nform of visual navigation, charts, graphs, or free-form texts. Presentation also includesstoring of interesting discoveries in the knowledge base for repeated use.\\nStep 6: Incorporate Usage of Discoveries. The goal of any data mining operation is\\nto understand the business, discern new patterns and possibilities, and also turn this un-derstanding into actions. This step is for using the results to create actionable items in thebusiness. Y ou assemble the results of the discovery in the best way so that they can be ex-ploited to improve the business. \\nThese major phases and their detailed steps are shown in Figure 17-4. Please study the\\nfigure carefully and note each step. Also note the data elements used in the steps. This fig-ure illustrates the knowledge discovery process from beginning to end. \\nOLAP Versus Data Mining\\nAfter reading the chapter on OLAP , you must now be an expert on that topic. As you\\nknow, with OLAP queries and analyses, users are able to obtain results from complexqueries and derive interesting patterns. Data mining also enables users to uncover interest-ing patterns, but there is an essential difference in the way the results are obtained. Pleaserefer to Figure 17-5. It shows the basic difference by means of a simple diagram.404 DATA MINING BASICS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7f7cab87-72bf-4842-b825-6cf7817cb406', embedding=None, metadata={'page_label': '421', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='When an analyst works with OLAP in an analysis session, he or she has some prior\\nknowledge of what he or she is looking for. The analyst starts with assumptions deliber-ately considered and thought out. Whereas in the case of data mining, the analyst has noprior knowledge of what the results are likely to be. Users drive OLAP queries. Eachquery may lead to a more complex one and so on. The user needs prior knowledge of theexpected results. The process is completely different in data mining. Whereas OLAPWHAT IS DATA MINING? 405\\nDETERMINATION\\nOF OBJECTIVESPREPARATION\\nOF DATAAPPLICATION OF DATA\\nMINING TECHIQUESEVALUATION AND\\nAPPLICATION OF\\nRESULTS\\nObjec -\\ntivesDefine\\nObjectivesSelect\\nDataExtract\\nDataPreprocess\\nDataMine\\nDataReview\\nResultsSelect\\nPromising\\nPatternsPresent\\nResults\\n(text/charts)Apply\\nResults\\nEnterprise\\nData\\nWarehouseEnterprise\\nOperational\\nSystemsSelected /\\nExtracted\\nData\\nPreprocessed\\nDataSelected\\nResultsAll\\nResultsResults\\nPresen -\\ntation\\nFigure 17-4 Knowledge discovery process.\\nFigure 17-5 OLAP and data mining.OLAP DATA  MINING\\nMultidimensional\\nData CubesPreprocessed\\nData\\nDeliberately\\nthough -out\\nassumptions\\nbased on prior\\nknowledge\\nInteractiveAnalysisLaunch DataMining Tool\\nHidden and\\nUnforseen\\nPatternsDiscoveredDETERMINATION PREPARATION APPLICATION OF DATA EVALUATION AND\\nOF OBJECTIVES OF DATA MINING TECHNIQUES APPLICATION OF\\nRESULTS\\nDeliberately\\nthought-out\\nassumptions\\nbased on prior\\nknowledge', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b5c4190c-c94a-439e-9b75-2455b8c6e676', embedding=None, metadata={'page_label': '422', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='helps the user to analyze the past and gain insights, data mining helps the user predict the\\nfuture. To amplify this statement, Figure 17-6 lists the sets of questions the two method-ologies can answer.\\nNotice how OLAP is able to give you answers to questions on past performance. Of\\ncourse, from these answers you can gain a good understanding on what happened in thepast. Y ou may make guesses about the future from these answers about past performance.In contrast, notice what data mining can do. It can uncover specific patterns and relation-ships to predict the future.\\nWe have said that whereas OLAP analyzes the past, data mining predicts the future.\\nY ou must have guessed that there must be more to it than just this broad statement. Let uslook at all the different ways data mining differs from OLAP . For a comprehensive list ofdifferences between OLAP and data mining, please study Figure 17-7.\\nIn another sense, OLAP and data mining are complementary. Y ou may say that data\\nmining picks up where OLAP leaves off. The analyst drives the process while usingOLAP tools. In data mining, the analyst prepares the data and “sits back” while the toolsdrive the process. \\nData Mining and the Data Warehouse\\nThe enterprise data warehouse, either as a centralized repository feeding dependent data\\nmarts or as a conglomerate of conformed data marts on a bus structure, forms a very use-ful source of data for data mining. It contains all the significant data you have extractedfrom the various source operational systems. This data has been cleansed and trans-formed, and stored in your data warehouse repository.406 DATA MINING BASICS\\nWhich 100 customers offer the best \\nprofit potential?\\nWhich customers are likely to be bad \\ncredit risks?\\nWhat are the anticipated sales by \\nterritory and region for next year?\\nWhich salespersons are expected to \\nexceed their quotas next year?\\nFor the next two years, which stores \\nare likely to have best performance?\\nWhat is the expected returns for next \\nyear’s promotions?\\nWhich customers are likely to switch \\nto the competition next year?Who are our top 100 best customers \\nfor the last three years?\\nWhich customers defaulted on their \\nmortgages last two years?\\nWhat were the sales by territory last \\nquarter compared to the targets?\\nWhich salespersons sold more than \\ntheir quota during last four quarters?\\nLast year, which stores exceeded the \\ntotal prior year sales?\\nLast year, which were the top five \\npromotions that performed well?\\nWhich customers switched to other \\nphone companies last year?OLAP: Report on the past Data Mining: Predict the future\\nFigure 17-6 OLAP is used to analyze the past; data mining is used to predict the future.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3f5fb2b6-f3b6-409f-abf9-7bff609ac72f', embedding=None, metadata={'page_label': '423', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Data mining fits well and plays a significant role in the data warehouse environment. A\\nclean and complete data warehouse forms the bedrock for data mining and the data ware-house enables data mining operations to take place. The two technologies support eachother. The following are some of the major factors of this relationship.\\n/L50539Data mining algorithms need large amounts of data, more so at the detailed level.\\nMost data warehouses contain data at the lowest level of granularity. \\n/L50539Data mining flourishes on integrated and cleansed data. If your ETL functions were\\ncarried out properly, your data warehouse contains such data, very suitable for datamining.\\n/L50539The infrastructure for data warehouses is already robust, with parallel processing\\ntechnology and powerful relational database systems. Because such scalable hard-ware is already in place, no new investment is needed to support data mining. \\nLet us point out one difference in the way data from the data warehouse is used for tra-\\nditional analysis and data mining. When an analyst wants to perform an analysis, say withan OLAP tool, he or she begins with summary data at a high level, then continues throughthe lower levels by means of drill-down techniques. On many occasions, the analyst neednot go down to the detail levels. This is because he or she finds the suitable subsets for de-riving conclusions at the higher levels. But data mining is different. As the data mining al-gorithm is searching for trends and patterns, it deals with lots of detailed data. For exam-ple, if the data mining algorithm is looking for customer buying patterns, it certainlyneeds detailed data at the level of the individual customer. \\nSo what is a compromise approach? What is the level of granularity you need to pro-\\nvide in the data warehouse? Unless it is a huge burden to keep detailed data at the lowestWHAT IS DATA MINING? 407\\nFigure 17-7 OLAP and data mining: basic differences.Motivation for \\ninformation request\\nData           \\ngranularity\\nNumber of business \\ndimensions\\nNumber of \\ndimension attributes\\nSizes of datasets for \\nthe dimensions\\nAnalysis              \\napproach\\nAnalysis          \\ntechniques\\nState of the \\ntechnologyWhat is happening in the \\nenterprise?Predict the future based on why \\nthis is happening.\\nSummary                                 \\ndata.Detailed                           \\ntransaction-level data.\\nLimited number                         \\nof dimensions.Large number                         \\nof dimensions.\\nSmall number                                \\nof attributes.Many dimension              \\nattributes.\\nNot large for \\neach dimension.Usually very large                      \\nfor each dimension.\\nUser-driven,                       \\ninteractive analysis.Data-driven automatic \\nknowledge discovery. \\nMultidimensional, drill-down, \\nand slice-and-dice.Prepare data, launch                 \\nmining tool and sit back.\\nMature and                          \\nwidely used.Still emerging; some parts           \\nof the technology more mature.OLAP DATA MINING Features', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f48b6873-f1e5-4e35-b798-38175823d9b2', embedding=None, metadata={'page_label': '424', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='level of granularity, strive to store detailed data. Otherwise, for data mining engagements,\\nyou may have to extract detailed data directly from the operational systems. This calls foradditional steps of data consolidation, cleansing, and transformation. Y ou may also keeplight summaries in the data warehouse for traditional queries. Most of the summarizeddata along various sets of dimensions may reside in the OLAP systems. \\nThe data warehouse is a valuable and easily available data source for data mining oper-\\nations. Data extractions that data mining tools work on come from the data warehouse.Figure 17-8 illustrates how data mining fits in the data warehouse environment. Noticehow the data warehouse environment supports data mining. Note the levels of data held inthe data warehouse and the OLAP system. Also observe the flow of data from the datawarehouse for the knowledge discovery process.\\nMAJOR DATA MINING TECHNIQUES\\nNow that we are at the point of getting introduced to data mining techniques, we quickly\\nrealize that there are many different ways of classifying the techniques. Someone new todata mining may be totally confused by the names and descriptions of the techniques.Even among practicing data mining consultants, no uniform terminology seems to exist.Even though no consistent set of terms seems to be available, let us try to use the morepopular ones.\\nMany data mining practitioners seem to agree on a set of data mining functions that\\ncan be used in specific application areas. Various data mining techniques are applicable toeach type of function. These techniques consist of the specific algorithms that can be usedfor each function. Let us review these terms again. But before doing that, please see Fig-ure 17-9 showing the application areas, examples of mining functions, mining processes,408 DATA MINING BASICS\\nSource\\nOperational\\nSystemsDATA STAGING AREA\\nFlat files\\nwith\\nextracted\\nand\\ntransformed\\ndataOLAP\\nSystem\\nEnterprise\\nData\\nWarehouseLoad image\\nfiles ready\\nfor loading\\nthe data\\nwarehouse\\nData selected,\\nextracted,\\ntransformed, and\\nprepared for miningData\\nMiningOPTIONS\\nFOR DATA\\nEXTRACTION\\nFigure 17-8 Data mining in the data warehouse environment.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d72d828c-dc82-427b-8776-5a56ec90b5b0', embedding=None, metadata={'page_label': '425', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='and mining techniques. Using the figure, try to understand the connections. Please study\\nthe following statements:\\n/L50539Data mining algorithms are part of data mining techniques.\\n/L50539Data mining techniques are used to carry out data mining functions. While perform-\\ning specific data mining functions, you are applying data mining processes.\\n/L50539A specific data mining function is generally suitable to a given application area. \\n/L50539Each application area is a major area in business where data mining is actively used\\nnow.\\nWe will devote the rest of this section to discussing the highlights of the major functions,\\nthe processes used to carry out the functions, and the data mining techniques themselves.\\nData mining covers a broad range of techniques. This is not a textbook on data mining\\nand a detailed discussion of the data mining algorithms is not within its scope. There are anumber of well-written books in the field and you may refer to them to pursue your interest. \\nLet us explore the basics here. We will select six of the major techniques for our dis-\\ncussion. Our intention is to understand these techniques broadly without getting down totechnical details. The main goal is for you to get an overall appreciation of data miningtechniques. \\nCluster Detection\\nClustering means forming groups. Take the very ordinary example of how you do your\\nlaundry. Y ou group the clothes into whites, dark-colored clothes, light-colored clothes,MAJOR DATA MINING TECHNIQUES 409\\nMining \\nTechniquesMining \\nProcessesExamples of \\nMining FunctionsApplication \\nArea\\nFraud \\nDetection\\nRisk \\nAssessment\\nMarket AnalysisCredit card frauds\\nInternal audits\\nWarehouse pilferage\\nCredit card upgrades\\nMortgage Loans\\nCustomer Retention\\nCredit Ratings\\nMarket basket analysis\\nTarget marketing\\nCross selling\\nCustomer Relationship \\nMarketingDetermination of \\nvariations from norms\\nDetection and analysis \\nof links\\nPredictive Modeling\\nDatabase segmentationCluster Detection\\nDecision Trees\\nLink Analysis\\nGenetic AlgorithmsDecision Trees\\nMemory-based \\nReasoningData Visualization \\nMemory-based \\nReasoning\\nFigure 17-9 Data mining functions and application areas.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8bf786a3-55fa-42c3-b939-86bc3538ae71', embedding=None, metadata={'page_label': '426', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='permanent press, and the ones to be dry-cleaned. Y ou have five distinct clusters. Each\\ncluster has a meaning and you can use the meaning to get that cluster cleaned properly.The clustering helps you take specific and proper action for the individual pieces thatmake up the cluster. Now think of a specialty store owner in a resort community whowants to cater to the neighborhood by stocking the right type of products. If he has dataabout the age group and income level of each of the people who frequent the store, usingthese two variables, the store owner can probably put the customers into four clusters.These clusters may be formed as follows: wealthy retirees staying in resorts, middle-agedweekend golfers, wealthy young people with club memberships, and low-income clientswho happen to stay in the community. The information about the clusters helps the storeowner in his marketing. \\nClustering or cluster detection is one of the earliest data mining techniques. This tech-\\nnique is designated as undirected knowledge discovery or unsupervised learning. What dowe mean by this statement? In the cluster detection technique, you do not search preclas-sified data. No distinction is made between independent and dependent variables. For ex-ample, in the case of the store’ s customers, there are two variables: age group and incomelevel. Both variables participate equally in the functioning of the data mining algorithm. \\nThe cluster detection algorithm searches for groups or clusters of data elements that\\nare similar to one another. What is the purpose of this? Y ou expect similar customers orsimilar products to behave in the same way. Then you can take a cluster and do somethinguseful with it. Again, in the example of the specialty store, the store owner can take themembers of the cluster of wealthy retirees and target products specially interesting tothem. \\nNotice one important aspect of clustering. When the mining algorithm produces a clus-\\nter, you must understand what that cluster means exactly. Only then you will be able to dosomething useful with that cluster. The store owner has to understand that one of the clus-ters represents wealthy retirees residing in resorts. Only then can the store owner do some-thing useful with that cluster. It is not always easy to discern the meaning of every clusterthe data mining algorithm forms. A bank may get as many as twenty clusters but be ableto interpret the meanings of only two. But the return for the bank from the use of justthese two clusters may be enormous enough so that they may simply ignore the othereighteen clusters. \\nIf there are only two or three variables or dimensions, it is fairly easy to spot the clus-\\nters, even when dealing with many records. But if you are dealing with 500 variables from100,000 records, you need a special tool. How does the data mining tool perform the clus-tering function? Without getting bogged down in too much technical detail, let us studythe process. First, some basics. If you have two variables, then points in a two-dimension-al graph represent the values of sets of these two variables. Please refer to Figure 17-10,which shows the distribution of these points.\\nLet us consider an example. Suppose you want the data mining algorithm to form clus-\\nters of your customers, but you want the algorithm to use 50 different variables for eachcustomer, not just two. Now we are discussing a 50-dimensional space. Imagine each cus-tomer record with different values for the 50 dimensions. Each record is then a vectordefining a “point” in the 50-dimensional space.\\nLet us say you want to market to the customers and you are prepared to run marketing\\ncampaigns for 15 different groups. So you set the number of clusters as 15. This number isKin the K-means clustering algorithm, a very effective one for cluster detection. Fifteen\\ninitial records (called “seeds”) are chosen as the first set of centroids based on best guess-410 DATA MINING BASICS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2bd3c3e1-362c-42cc-adc0-b5bdda43a51f', embedding=None, metadata={'page_label': '427', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='es. One seed represents one set of values for the 50 variables chosen from the customer\\nrecord. In the next step, the algorithm assigns each customer record in the database to acluster based on the seed to which it is closest. Closeness is based on the nearness of thevalues of the set of 50 variables in a record to the values in the seed record. The first set of15 clusters is now formed. Then the algorithm calculates the centroid or mean for each ofthe first set of 15 clusters. The values of the 50 variables in each centroid are taken to rep-resent that cluster.\\nThe next iteration then starts. Each customer record is rematched with the new set of\\ncentroids and cluster boundaries are redrawn. After a few iterations the final clustersemerge. Now please refer to Figure 17-11 illustrating how centroids are determined andcluster boundaries redrawn.\\nHow does the algorithm redraw the cluster boundaries? What factors determine that\\none customer record is near one centroid and not the other? Each implementation of thecluster detection algorithm adopts a method of comparing the values of the variables in in-dividual records with those in the centroids. The algorithm uses these comparisons to cal-culate the distances of individual customer records from the centroids. After calculatingthe distances, the algorithm redraws the cluster boundaries.\\nDecision Trees\\nThis technique applies to classification and prediction. The major attraction of decision\\ntrees is their simplicity. By following the tree, you can decipher the rules and understandwhy a record is classified in a certain way. Decision trees represent rules. Y ou can usethese rules to retrieve records falling into a certain category. Please examine Figure 17-12showing a decision tree representing the profiles of men and women buying a notebookcomputer. MAJOR DATA MINING TECHNIQUES 411Number of years as customer\\nTotal Value to the enterprise\\n25 years\\n$ 1 million\\nFigure 17-10 Clusters with two variables.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8f13647e-b543-439c-a745-fc53a10b032e', embedding=None, metadata={'page_label': '428', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='412 DATA MINING BASICS\\n1\\n32\\n1Initial cluster boundaries \\nbased on initial seeds\\n3Cluster boundaries redrawn \\nat each iteration2Centroids of new clusters \\ncalculated\\nInitial seed Calculated centroid\\nFigure 17-11 Centroids and cluster boundaries.\\nPortability\\nLightMedium\\nSpeed Speed\\nStorageCostPentium IIISlower\\nPentium IIISlower\\nComfortableKeyboard Keyboard KeyboardW\\nW MM\\nMW MMCostCost Cost\\nStorage Storage Storage Storage<$2,500More\\n<$2,500<$2,500<$2,500MoreMoreMore\\n10GBLess\\n10GB 10GB10GB10GBLessLessLess Less\\nAverage\\nComfortableComfortableComfortableAverageAverage Average\\nWM KeyboardM\\nFigure 17-12 Decision tree for notebook computer buyers.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9ccb2faa-ec51-408a-8ca5-4777a507d210', embedding=None, metadata={'page_label': '429', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='In some data mining processes, you really do not care how the algorithm selected a cer-\\ntain record. For example, when you are selecting prospects to be targeted in a marketingcampaign, you do not need the reasons for targeting them. Y ou only need the ability topredict which members are likely to respond to the mailing. But in some other cases, thereasons for the prediction are important. If your company is a mortgage company andwants to evaluate an application, you need to know why an application must be rejected.Y our company must be able to protect itself from any lawsuits of discrimination. Wherev-er the reasons are necessary and you must be able to trace the decision paths, decisiontrees are suitable. \\nAs you have seen from Figure 17-12, a decision tree represents a series of questions.\\nEach question determines what follow-up question is best to be asked next. Good ques-tions produce a short series. Trees are drawn with the root at the top and the leaves at thebottom, an unnatural convention. The question at the root must be the one that best differ-entiates among the target classes. A database record enters the tree at the root node. Therecord works its way down until it reaches a leaf. The leaf node determines the classifica-tion of the record. \\nHow can you measure the effectiveness of a tree? In the example of the profiles of\\nbuyers of notebook computers, you can pass the records whose classifications are al-ready known. Then you can calculate the percentage of correctness for the knownrecords. A tree showing a high level of correctness is more effective. Also, you must payattention to the branches. Some paths are better than others because the rules are better.By pruning the incompetent branches, you can enhance the predictive effectiveness ofthe whole tree. \\nHow do the decision tree algorithms build the trees? First, the algorithm attempts to\\nfind the test that will split the records in the best possible manner among the wanted clas-sifications. At each lower level node from the root, whatever rule works best to split thesubsets is applied. This process of finding each additional level of the tree continues. Thetree is allowed to grow until you cannot find better ways to split the input records. \\nMemory-Based Reasoning\\nWould you rather go to an experienced doctor or to a novice? Of course, the answer is ob-\\nvious. Why? Because the experienced doctor treats you and cures you based on his or herexperience. The doctor knows what worked in the past in several cases when the symp-toms were similar to yours. We are all good at making decisions on the basis of our expe-riences. We depend on the similarities of the current situation to what we know from pastexperience. How do we use the experience to solve the current problem? First, we identifysimilar instances in the past, then we use the past instances and apply the informationabout those instances to the present. The same principles apply to the memory-based rea-soning (MBR) algorithm. \\nMBR uses known instances of a model to predict unknown instances. This data mining\\ntechnique maintains a dataset of known records. The algorithm knows the characteristicsof the records in this training dataset. When a new record arrives for evaluation, the algo-rithm finds neighbors similar to the new record, then uses the characteristics of the neigh-bors for prediction and classification. \\nWhen a new record arrives at the data mining tool, first the tool calculates the “dis-\\ntance” between this record and the records in the training dataset. The distance function ofMAJOR DATA MINING TECHNIQUES 413', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4e5399b5-5f25-4649-830d-aad4a50e4adf', embedding=None, metadata={'page_label': '430', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='the data mining tool does the calculation. The results determine which data records in the\\ntraining dataset qualify to be considered as neighbors to the incoming data record. Next,the algorithm uses a combination function to combine the results of the various distancefunctions to obtain the final answer. The distance function and the combination functionare key components of the memory-based reasoning technique.\\nLet us consider a simple example to observe how MBR works. This example is about\\npredicting the last book read by new respondents based on a dataset of known responses.For the sake of keeping the example quite simple, assume there are four recent bestsellers.The students surveyed have read these books and have also mentioned which they hadread last. The results of four surveys are shown in Figure 17-13. Look at the first part ofthe figure. Here you see the scatterplot of known respondents. The second part of the fig-ure contains the unknown respondents falling in place on the scatterplot. From where eachunknown respondent falls on the scatterplot, you can determine the distance to the knownrespondents and then find the nearest neighbor. The nearest neighbor predicts the lastbook read by each unknown respondent.\\nFor solving a data mining problem using MBR, you are concerned with three critical\\nissues:\\n1. Selecting the most suitable historical records to form the training or base dataset\\n2. Establishing the best way to compose the historical record3. Determining the two essential functions, namely, the distance function and the com-\\nbination function414 DATA MINING BASICS\\n15 35 30 25 20\\nAge of studentsFour groups of respondents\\nBest Sellers\\nAge of studentsFour groups of respondents?\\n??\\n?\\n15 35 30 25 20nearest \\nneighbor nearest \\nneighbor\\nnearest \\nneighbor\\nnearest \\nneighbor\\nTimeline\\nThe Greatest GenerationThe Last PrecinctThe O’Reilly Factor\\nFigure 17-13 Memory-based reasoning.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d7568153-e476-413c-acec-622e5824907c', embedding=None, metadata={'page_label': '431', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Link Analysis\\nThis algorithm is extremely useful for finding patterns from relationships. If you look at\\nthe business world closely, you clearly notice all types of relationships. Airlines link citiestogether. Telephone calls connect people and establish relationships. Fax machines con-nect with one another. Physicians prescribing treatments have links to the patients. In asale transaction at a supermarket, many items bought together in one trip are all linked to-gether. Y ou notice relationships everywhere.\\nThe link analysis technique mines relationships and discovers knowledge. For exam-\\nple, if you look at the supermarket sale transactions for one day, why are skim milk andbrown bread found in the same transaction about 80% of the time? Is there a strong rela-tionship between the two products in the supermarket basket? If so, can these two prod-ucts be promoted together? Are there more such combinations? How can we find suchlinks or affinities? \\nPursue another example, casually mentioned above. For a telephone company, finding\\nout if residential customers have fax machines is a useful proposition. Why? If a residen-tial customer uses a fax machine, then that customer may either want a second line orwant to have some kind of upgrade. By analyzing the relationships between two phonenumbers established by the calls along with other stipulations, the desired information canbe discovered. Link analysis algorithms discover such combinations. Depending upon thetypes of knowledge discovery, link analysis techniques have three types of applications:associations discovery, sequential pattern discovery, and similar time sequence discovery.Let us briefly discuss each of these applications.\\nAssociations Discovery. Associations are affinities between items. Association dis-\\ncovery algorithms find combinations where the presence of one item suggests the pres-ence of another. When you apply these algorithms to the shopping transactions at a super-market, they will uncover affinities among products that are likely to be purchasedtogether. Association rules represent such affinities. The algorithms derive the associationrules systematically and efficiently. Please see Figure 17-14 presenting an association ruleand the annotated parts of the rule. The two parts—support factor and the confidence fac-tor—indicate the strength of the association. Rules with high support and confidence fac-tor values are more valid, relevant, and useful. Simplicity makes association discovery apopular data mining algorithm. There are only two factors to be interpreted and even thesetend to be intuitive for interpretation. Because the technique essentially involves countingthe combinations as the dataset is read repeatedly each time new dimensions are added,scaling does pose a major problem.\\nSequential Pattern Discovery. As the name implies, these algorithms discover pat-\\nterns where one set of items follows another specific set. Time plays a role in these pat-terns. When you select records for analysis, you must have date and time as data items toenable discovery of sequential patterns. \\nLet us say you want the algorithm to discover the buying sequence of products. The\\nsale transactions form the dataset for the data mining operation. The data elements in thesale transaction may consist of date and time of transaction, products bought during thetransaction, and the identification of the customer who bought the items. A sample set ofthese transactions and the results of applying the algorithm are shown in Figure 17-15.Notice the discovery of the sequential pattern. Also notice the support factor that gives anindication of the relevance of the association.MAJOR DATA MINING TECHNIQUES 415', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3dc6ed69-928e-4223-87d8-3d26ef58ffe1', embedding=None, metadata={'page_label': '432', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='416 DATA MINING BASICS\\nA customer in a super-\\nmarket also buys milk in \\n65% \\nof the cases\\nwhenever the customer \\nbuys bread, this \\nhappening for                         \\n20%                                    \\nof all purchases.   Association rule head\\nAssociation rule bodyConfidence \\nFactor\\nSupport \\nFactor\\nFigure 17-14 An association rule.\\nFigure 17-15 Sequential pattern discovery.NAME OF CUSTOMER PRODUCT SEQUENCE FOR CUSTOMER\\nJohn Brown Desktop PC, MP3 Player, Digital Camera\\nCindy Silverman Desktop PC, MP3 Player, Digital Camera, Tape Backup Drive\\nRobert Stone Laptop PC, Digital Camera\\nTerry Goldsmith Laptop PC, Digital CameraRichard McKeown Desktop PC, MP3 Player\\nSEQUENTIAL PATTERNS (Support Factor > 60%) SUPPORTING CUSTOMERS\\nDesktop PC, MP3 Player John Brown, Cindy Silverman, Richard McKeownSequential \\nPattern \\nDiscovery with \\nSupport \\nFactors\\nSEQUENTIAL PATTERNS (Support Factor > 40%) SUPPORTING CUSTOMERS\\nDesktop PC, MP3 Player, Digital Camera John Brown, Cindy Silverman\\nLaptop PC, Digital Camera Robert Stone, Terry GoldsmithSALE DATE NAME OF CUSTOMER PRODUCTS PURCHASED\\nNov. 15, 2000 John Brown Desktop PC, MP3 PlayerNov. 15, 2000 Cindy Silverman Desktop PC, MP3 Player, Digital CameraNov. 15, 2000 Robert Stone Laptop PCDec. 19, 2000 Terry Goldsmith Laptop PCDec. 19, 2000 John Brown Digital CameraDec. 19, 2000 Terry Goldsmith Digital CameraDec. 19, 2000 Robert Stone Digital CameraDec. 20, 2000 Cindy Silverman Tape Backup Drive\\nDec. 20, 2000 Richard McKeown Desktop PC, MP3 PlayerTransaction \\nData File\\nSequential Patterns --\\nCustomer Sequence', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1aab26b2-63ba-483f-b0e3-65f7a863f062', embedding=None, metadata={'page_label': '433', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Typical discoveries include associations of the following types:\\n/L50539Purchase of a digital camera is followed by purchase of a color printer 60% of the\\ntime\\n/L50539Purchase of a desktop is followed by purchase of a tape backup drive 65% of the time\\n/L50539Purchase of window curtains is followed by purchase of living room furniture 50%\\nof the time \\nSimilar Time Sequence Discovery. This technique depends on the availability of\\ntime sequences. In the previous technique, the results indicate sequential events over time.This technique, however, finds a sequence of events and then comes up with other similarsequences of events. For example, in retail department stores, this data mining techniquecomes up with a second department that has a sales stream similar to the first. Findingsimilar sequential price movements of stock is another application of this technique.\\nNeural Networks\\nNeural networks mimic the human brain by learning from a training dataset and applying\\nthe learning to generalize patterns for classification and prediction. These algorithms areeffective when the data is shapeless and lacks any apparent pattern. The basic unit of anartificial neural network is modeled after the neurons in the brain. This unit is known as anode and is one of the two main structures of the neural network model. The other struc-ture is the link that corresponds to the connection between neurons in the brain. Please seeFigure 17-16 illustrating the neural network model. \\nLet us consider a simple example to understand how a neural network makes a predic-MAJOR DATA MINING TECHNIQUES 417\\nVALUES FOR INPUT VARIABLESDISCOVERED VALUE FOR OUTPUT VARIABLE\\nNodesLinksOutput \\nfrom node\\nInput to \\nnext nodeInput \\nvalues \\nweightedINPUT OUTPUT\\nFigure 17-16 Neural network model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5d57753b-b0b7-405e-a9ca-18e3de1ce637', embedding=None, metadata={'page_label': '434', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='tion. The neural network receives values of the variables or predictors at the input nodes.\\nIf there are 15 different predictors, then there are 15 input nodes. Weights may be appliedto the predictors to condition them properly. Now please look at Figure 17-17 indicatingthe working of a neural network. There may be several inner layers operating on the pre-dictors and they move from node to node until the discovered result is presented at theoutput node. The inner layers are also known as hidden layers because as the input datasetis running through many iterations, the inner layers rehash the predictors over and overagain.\\nGenetic Algorithms\\nIn a way, genetic algorithms have something in common with neural networks. This tech-\\nnique also has its basis in biology. It is said that evolution and natural selection promotethe survival of the fittest. Over generations, the process propagates the genetic material inthe fittest individuals from one generation to the next. Genetic algorithms apply the sameprinciples to data mining. This technique uses a highly iterative process of selection,cross-over, and mutation operators to evolve successive generations of models. At each it-eration, every model competes with everyone other by inheriting traits from previous onesuntil only the most predictive model survives. \\nLet us try to understand the evolution of successive generations in genetic algorithms\\nby using a very popular example used by many authors. This is the problem to be solved:Y our company is doing a promotional mailing and wants to include free coupons in themailing. Remember, this is a promotional mailing with the goal of increasing profits. Atthe same time, the promotional mailing must not produce the opposite result of lost rev-enue. This is the question: What is the optimum number of coupons to be placed in eachmailer to maximize profits?\\nAt first blush, it looks like mailing out as many coupons as possible might be the solu-\\ntion. Will this not enable the customers to use all the available coupons and maximizeprofits? However, some other factors seem to complicate the problem. First, the morecoupons in the mailer, the higher the postal costs are going to be. The increased mailing418 DATA MINING BASICS\\nAge \\n35\\nIncome \\n$75,000Upgrade to \\nGold Credit \\nCard -- Pre-\\napproved0.35\\n0.75Weight = 0.9\\nWeight = 1.01.065Neural Network for pre-\\napproval of Gold Credit Card\\n[Upgrade pre-\\napproved if output \\nvalue >1.0] \\nFigure 17-17 How a neural network works.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8b4a998a-f348-44df-bb51-208a8dad6960', embedding=None, metadata={'page_label': '435', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='costs will eat into the profits. Second, if you do not send enough coupons, every coupon\\nnot in the mailer is a coupon that is not used. This is lost opportunity and potential loss inrevenue. Finally, too many coupons in a mailer may turn the customer off and he or shemay not use any at all. All these factors reinforce the need to arrive at an optimum numberof coupons in each mailer. Now look at Figure 17-18 showing the first three generationsof the evolution represented by the genetic algorithm applied to the problem.\\nLet us examine the figure. Each simulated organism has a gene that indicates the or-\\nganism’ s best guess at the number of coupons per mailer. Notice the four organisms in thefirst generation. For two of the organisms, the gene or the estimated number of coupons isabnormal. Therefore, these two organisms do not survive. Remember, only the fittest sur-vive. Note how these two instances are crossed out. Now the remaining two surviving or-ganisms reproduce similar replicas of themselves with distinct genes. Again, rememberthat genes represent the numbers of potential coupons in a mailer. The norm is reset atevery generation and the process of evolution continues. In every generation, the fittestorganisms survive and the evolution continues until there is only one final survivor. Thathas the gene representing the optimal number of coupons per mailer.\\nOf course, the above example is too simplistic. We have not explained how the num-\\nbers are generated in each generation. Also, we have not indicated how the norms are setand how you eliminate the abnormal organisms. There are complex calculations for per-forming these functions. Nevertheless, the example gives you a fairly good overview ofthe technique.\\nMoving into Data Mining\\nY ou now have sufficient knowledge to look in the right direction and help your company\\nget into data mining and reap the benefits. What are the initial steps? How should yourMAJOR DATA MINING TECHNIQUES 419\\n1500 \\ncoupons13 \\ncoupons\\n36 \\ncoupons3  \\ncouponsThird Generation Second Generation First Generation\\n31 \\ncoupons11 \\ncoupons16 \\ncoupons\\n39 \\ncoupons19 \\ncoupons\\n15 \\ncoupons\\n10 \\ncoupons\\n13 \\ncoupons\\nFigure 17-18 Genetic algorithm generations.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='89451f26-b19a-4774-9c30-468333794ec2', embedding=None, metadata={'page_label': '436', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='company get started in this attractive technology? First of all, remember that your data\\nwarehouse is going to feed the data mining processes. Whatever your company plans touse data mining technology for, the data source is your data warehouse. Before gettinginto data mining, a sound and solid data warehouse will put the data mining operation ona strong foundation.\\nAs mentioned earlier, data mining techniques produce good results when large vol-\\numes of data are available. Almost all the algorithms need data at the lowest grain. Con-sider having data at the detailed level in your data warehouse. Another important pointrefers to the quality of the data. Data mining is about discovering patterns and relation-ships from data. Mining dirty data leads to inaccurate discoveries. Actions taken based ondubious discoveries will produce seriously wrong consequences. Data mining projects canrun up the project costs. Y ou cannot afford to launch into the technology if the data is notclean enough. Ensure that the data warehouse holds high-quality data.\\nWhen you apply a data mining technique, it is nice to discover a few interesting pat-\\nterns and relationships. But what is your company going to do with the discoveries? If thediscovered patterns and relationships are not actionable, it is a wasted effort. Before em-barking on a data mining project, have clear ideas of the types of problems you expect tosolve and the types of benefits you expect to obtain. After firming up the objectives, whatnext? Y ou need a way of comparing the data mining algorithms and selecting the tool mostappropriate for your specific requirements.\\nIn the previous section, we covered the major data mining techniques. Y ou learnt about\\neach individual technique, how it works, and how it discovers knowledge. But the discus-sion dealt with one technique at a time. Is there a framework to compare the techniques?Is there a comparison method to help you in the selection of your data mining tool? Pleaselook at Figure 17-19.\\nThe model structure refers to how the technique is perceived, not how it is actually im-\\nplemented. For example, a decision tree model may actually be implemented throughSQL statements. In the framework, the basic process is the process performed by the par-ticular data mining technique. For example, decision trees perform the process of splittingat decision points. How a technique validates the model is important. In the case of neuralnetworks, the technique does not contain a validation method to determine termination.The model calls for processing the input records through the different layers of nodes andterminate the discovery at the output node.\\nWhen you are looking for a tool, a data mining tool supporting more than one technique\\nis worth consideration. Y our organization may not presently need a composite tool withmany techniques. A multitask tool opens up more possibilities. Moreover, many data min-ing analysts desire to cross-validate discovered patterns using several techniques. The mostavailable techniques supported by vendor tools in the market today include the following:\\n/L50539Cluster detection\\n/L50539Decision trees\\n/L50539Link analysis\\n/L50539Data visualization\\nBefore we get into a detailed list of criteria for selecting data mining tools, let us make\\na few general but important observations about tool selection. Please consider these tipscarefully:420 DATA MINING BASICS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aa0e7f44-08ca-4f28-a5dc-7b09518f11f1', embedding=None, metadata={'page_label': '437', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539The tool must be able to integrate well with your data warehouse environment by\\naccepting data from the warehouse and be compatible with the overall metadataframework.\\n/L50539The patterns and relationships discovered must be as accurate as possible. Discover-\\ning erratic patterns is more dangerous than not discovering any patterns at all. \\n/L50539In most cases, you would need an explanation for the working of the model and\\nknow how the results were produced. The tool must be able to explain the rules andhow the patterns were discovered.\\nLet us complete this section with a list of criteria for evaluating data mining tools. The\\nlist is by no means exhaustive, but it covers the essential points. \\nData Access. The data mining tool must be able to access data sources such as the data\\nwarehouse and quickly bring over the required datasets to its environment. On manyoccasions you may need data from other sources to augment the data extracted fromthe data warehouse. The tool must be capable of reading other data sources and in-put formats.\\nData Selection. While selecting and extracting data for mining, the tool must be able\\nto perform its operations according to a variety of criteria. Selection abilities mustinclude filtering out of unwanted data and deriving new data items from existingones.MAJOR DATA MINING TECHNIQUES 421\\nData Mining \\nTechniqueUnderlying \\nStructureBasic     \\nProcessValidation \\nMethod\\nCross validation to \\nverify accuracyGrouping of values \\nin the same \\nneighborhoodDistance calculations \\nin n-vector spaceCluster \\nDetection\\nCross validation Splits at decision \\npoints based on \\nentropy Binary Tree\\nCross validation Association of \\nunknown instances \\nwith known instancesPredictive structure \\nbased on distance and \\ncombination functions \\nNot applicable Discover links \\namong variables by \\ntheir valuesBased on linking of \\nvariables\\nNot applicable Weighted inputs of \\npredictors at each \\nnode Forward propagation \\nnetwork\\nMostly cross \\nvalidationSurvival of the fittest \\non mutation of \\nderived valuesNot applicableDecision Trees\\nMemory-based \\nReasoning\\nLink Analysis\\nNeural \\nNetworks\\nGenetic \\nAlgorithms\\nFigure 17-19 Framework for comparing techniques.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1b2877e8-c6e2-4757-ac3a-9ec569071d91', embedding=None, metadata={'page_label': '438', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sensitivity to Data Quality. Because of its importance, data quality is worth mention-\\ning again. The data mining tool must be sensitive to the quality of the data it mines.The tool must be able to recognize missing or incomplete data and compensate forthe problem. The tool must also be able to produce error reports.\\nData Visualization. Data mining techniques process substantial data volumes and pro-\\nduce a wide range of results. Inability to display results graphically and diagram-matically diminishes the value of the tool severely. Select tools with good data visu-alization capabilities. \\nExtensibility. The tool architecture must be able to integrate with the data warehouse\\nadministration and other functions such as data extraction and metadata manage-ment.\\nPerformance. The tool must provide consistent performance irrespective of the\\namount of data to be mined, the specific algorithm applied, the number of variablesspecified, and the level of accuracy demanded. \\nScalability. Data mining needs to work with large volumes of data to discover mean-\\ningful and useful patterns and relationships. Therefore, ensure that the tool scales upto handle huge data volumes. \\nOpenness. This is a desirable feature. Openness refers to being able to integrate with\\nthe environment and other types of tools. Look for the ability of the tool to con-nect to external applications where users could gain access to data mining algo-rithms from other applications. The tool must be able to share the output withdesktop tools such as graphical displays, spreadsheets, and database utilities. Thefeature of openness must also include availability of the tool on leading serverplatforms.\\nSuite of Algorithms. Select a tool that provides a few different algorithms rather than\\none that supports only a single data mining algorithm.\\nDATA MINING APPLICATIONS\\nY ou will find a wide variety of applications benefiting from data mining. The technology\\nencompasses a rich collection of proven techniques that cover a wide range of applica-tions in both the commercial and noncommercial realms. In some cases, multiple tech-niques are used, back to back, to greater advantage. Y ou may apply a cluster detectiontechnique to identify clusters of customers. Then you may follow with a predictive algo-rithm applied to some of the identified clusters and discover the expected behavior of thecustomers in those clusters.\\nNoncommercial use of data mining is strong and pervasive in the research area. In oil\\nexploration and research, data mining techniques discover locations suitable for drillingbecause of potential mineral and oil deposits. Pattern discovery and matching techniqueshave military applications in assisting to identify targets. Medical research is a field ripefor data mining. The technology helps researchers with discoveries of correlations be-tween diseases and patient characteristics. Crime investigation agencies use the technolo-gy to connect criminal profiles to crimes. In astronomy and cosmology, data mining helpspredict cosmic events.\\nThe scientific community makes use of data mining to a moderate extent, but the tech-\\nnology has widespread applications in the commercial arena. Most of the tools target the422 DATA MINING BASICS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1a2d04b2-041c-4d14-8539-49bf09ece4ea', embedding=None, metadata={'page_label': '439', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='commercial sector. Please review the following list of a few major applications of data\\nmining in the business area:\\nCustomer Segmentation. This is one of the most widespread applications. Businesses\\nuse data mining to understand their customers. Cluster detection algorithms discov-er clusters of customers sharing the same characteristics.\\nMarket Basket Analysis. This is a very useful application for retail. Link analysis al-\\ngorithms uncover affinities between products that are bought together. Other busi-nesses such as upscale auction houses use these algorithms to find customers towhom they can sell higher-value items.\\nRisk Management. Insurance companies and mortgage businesses use data mining to\\nuncover risks associated with potential customers.\\nFraud Detection. Credit card companies use data mining to discover abnormal spend-\\ning patterns of customers. Such patterns can expose fraudulent use of the cards. \\nDelinquency Tracking. Loan companies use the technology to track customers who\\nare likely to default on repayments.\\nDemand Prediction. Retail and other businesses use data mining to match demand\\nand supply trends to forecast demand for specific products.\\nBenefits of Data Mining\\nBy now you are convinced of the strengths and usefulness of data mining technology.\\nWithout data mining, useful knowledge lying buried in the mountains of data in many or-ganizations would never be discovered and the benefits from using the discovered patternsand relationships would not be realized. What are the types of such benefits? We have al-ready touched upon the applications of data mining and you have grasped the impliedbenefits.\\nJust to appreciate the enormous utility of data mining, let us enumerate the types of\\nbenefits. Please go through the following list indicating the types of benefits actually real-izable in real-world situations:\\n/L50539In a large company manufacturing consumer goods, the shipping department regu-\\nlarly short-ships orders and hides the variations between the purchase orders and thefreight bills. Data mining detects the criminal behavior by uncovering patterns oforders and premature inventory reductions.\\n/L50539A mail order company improves direct mail promotions to prospects through more\\ntargeted campaigns.\\n/L50539A supermarket chain improves earnings by rearranging the shelves based on discov-\\nery of affinities of products that sell together.\\n/L50539An airlines company increases sales to business travelers by discovering traveling\\npatterns of frequent flyers.\\n/L50539A department store hikes the sales in specialty departments by anticipating sudden\\nsurges in demand.\\n/L50539A national health insurance provider saves large amounts of money by detecting\\nfraudulent claims.\\n/L50539A major banking corporation with investment and financial services increases theDATA MINING APPLICATIONS 423', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7dd2c36f-918e-4d1a-9245-78e163a048c2', embedding=None, metadata={'page_label': '440', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='leverage of direct marketing campaigns. Predictive modeling algorithms uncover\\nclusters of customers with high lifetime values.\\n/L50539A manufacturer of diesel engines increases sales by forecasting sales of engines\\nbased on patterns discovered from historical data of truck registrations.\\n/L50539A major bank prevents loss by detecting early warning signs for attrition in its\\nchecking account business.\\n/L50539A catalog sales company doubles its holiday sales from the previous year by predict-\\ning which customers would use the holiday catalog. \\nApplications in the Retail Industry\\nLet us very briefly discuss how the retail industry makes use of data mining and benefits\\nfrom it. Fierce competition and narrow profit margins have plagued the retail industry.Forced by these factors, the retail industry adopted data warehousing earlier than mostother industries. Over the years, these data warehouses have accumulated huge volumesof data. The data warehouses in many retail businesses are mature and ripe. Also, throughthe use of scanners and cash registers, the retail industry has been able to capture detailedpoint of sale data.\\nThe combination of the two features—huge volumes of data and low-granularity\\ndata—is ideal for data mining. The retail industry was able to begin using data miningwhile others were just making plans. All types of businesses in the retail industry, includ-ing grocery chains, consumer retail chains, and catalog sales companies, use direct mar-keting campaigns and promotions extensively. Direct marketing happens to be quite criti-cal in the industry. All companies depend heavily on direct marketing.\\nDirect marketing involves targeting campaigns and promotions to specific customer\\nsegments. Cluster detection and other predictive data mining algorithms provide customersegmentation. As this is a crucial area for the retail industry, many vendors offer data min-ing tools for customer segmentation. These tools can be integrated with the data ware-house at the back end for data selection and extraction. At the front end, these tools workwell with standard presentation software. Customer segmentation tools discover clustersand predict success rates for direct marketing campaigns.\\nRetail industry promotions necessarily require knowledge of which products to pro-\\nmote and in what combinations. Retailers use link analysis algorithms to find affinitiesamong products that usually sell together. As you already know, this is market basketanalysis. Based on the affinity grouping, retailers can plan their special sale items andalso the arrangement of products on the shelves.\\nApart from customer segmentation and market basket analysis, retailers use data min-\\ning for inventory management. Inventory for a retailer encompasses thousands of prod-ucts. Inventory turnover and management are significant concerns for these businesses.Another area of use for data mining in the retail industry relates to sales forecasting. Re-tail sales are subject to strong seasonal fluctuations. Holidays and weekends also make adifference. Therefore, sales forecasting is critical for the industry. The retailers turn to thepredictive algorithms of data mining technology for sales forecasting.\\nWhat are the other types of data mining uses in the retail industry? What are the ques-\\ntions and concerns the industry is interested in? Here is a short list:\\n/L50539Customer long-term spending patterns424 DATA MINING BASICS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f211e4fb-04a9-483a-a6e0-c79ca7d561fa', embedding=None, metadata={'page_label': '441', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Customer purchasing frequency\\n/L50539Best types of promotions\\n/L50539Store plan and arrangement of promotional displays\\n/L50539Planning mailers with coupons\\n/L50539Customer types buying special offerings\\n/L50539Sale trends, seasonal and regular\\n/L50539Manpower planning based on busy times\\n/L50539Most profitable segments in the customer base\\nApplications in the Telecommunications Industry\\nThe next industry we want to look at for data mining applications is telecommunications.\\nThis industry was deregulated in the 1990s. In the United States, the cellular alternativechanged the landscape dramatically, although the wave had already hit Europe and fewpockets in Asia earlier. Against this background of an extremely competitive marketplace,the companies scrambled to find methods to understand their customers. Customer reten-tion and customer acquisition have become top priorities in their marketing. Telecommu-nications companies compete with one another to design the best offerings and entice cus-tomers. No wonder this climate of competitive pressures has driven telecommunicationcompanies to data mining. All the leading companies have already adopted the technologyand are reaping many benefits. Several data mining vendors and consulting companiesspecialize in the problems of this industry. \\nCustomer churn is of serious concern. How many times a week do you get cold calls\\nfrom telemarketing representatives in this industry? Many data mining vendors offerproducts to contain customer churn. The newer cellular phone market experiences thehighest churn rate. Some experts estimate the total cost of acquiring a single new cus-tomer is as high as $500. \\nProblem areas in the communications network are potential disasters. In today’ s com-\\npetitive market, customers are tempted to switch at the slightest problem. Customer reten-tion under such circumstances becomes very fragile. A few data mining vendors special-ize in data visualization products for the industry. These products flash alert signs on thenetwork maps to indicate potential problem areas, enabling the responsible employees totake preventive action. \\nBelow is a general list of questions and concerns of the industry where data mining ap-\\nplications are helping:\\n/L50539Retention of customers in the face of enticing competition\\n/L50539Customer behavior indicating increased line usage in the future\\n/L50539Discovery of profitable service packages\\n/L50539Customers most likely to churn\\n/L50539Prediction of cellular fraud\\n/L50539Promotion of additional products and services to existing customers\\n/L50539Factors that increase the customer’ s propensity to use the phone\\n/L50539Product evaluation compared to the competitionDATA MINING APPLICATIONS 425', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='483812ae-37a4-4520-9784-b32f0a4f7418', embedding=None, metadata={'page_label': '442', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Applications in Banking and Finance\\nThis is another industry where you will find heavy usage of data mining. Banking has\\nbeen reshaped by regulations in the past few years. Mergers and acquisitions are morepronounced in banking and banks have been expanding the scope of their services. Fi-nance is an area of fluctuation and uncertainty. The banking and finance industry is fertileground for data mining. Banks and financial institutions generate large volumes of de-tailed transaction data. Such data is suitable for data mining.\\nData mining applications at banks are quite varied. Fraud detection, risk assessment of\\npotential customers, trend analysis, and direct marketing are the primary data mining ap-plications at banks.\\nIn the financial area, requirements for forecasting dominate. Forecasting of stock\\nprices and commodity prices with a high level of approximation can mean large profits.Forecasting of potential financial disaster can prove to be very valuable. Neural networkalgorithms are used in forecasting, options and bond trading, portfolio management, andin mergers and acquisitions. \\nCHAPTER SUMMARY\\n/L50539Decision support systems have progressed to data mining.\\n/L50539Data mining, which is knowledge discovery, is data-driven, whereas other analysis\\ntechniques such as OLAP are user-driven.\\n/L50539The knowledge discovery process in data mining uncovers relationships and pat-\\nterns not readily known to exist.\\n/L50539Six distinct steps comprise the knowledge discovery process. \\n/L50539In information retrieval and discovery, OLAP and data mining can be considered to\\nbe complementary as well as different.\\n/L50539The data warehouse is the best source of data for a data mining operation.\\n/L50539Major common data mining techniques are cluster detection, decision trees, memo-\\nry-based reasoning, link analysis, neural networks, and genetic algorithms.\\nREVIEW QUESTIONS\\n1. Give three broad reasons why you think data mining is being used in today’ s busi-\\nnesses.\\n2. Define data mining in two or three sentences.3. Name the major phases of a data mining operation. Out of these phases, pick two\\nand describe the types of activities in these two phases.\\n4. How is data mining different from OLAP? Explain briefly.5. Is the data warehouse a prerequisite for data mining? Does the data warehouse\\nhelp data mining? If so, in what ways?\\n6. Briefly describe the cluster detection technique. 7. How does the memory-based reasoning (MBR) technique work? What is the un-\\nderlying principle?\\n8. Name the three common applications of the link analysis technique.426 DATA MINING BASICS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0c89d456-011a-4d76-9c0f-9edac8089f2d', embedding=None, metadata={'page_label': '443', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9. Do neural networks and genetic algorithms have anything in common? Point out a\\nfew differences.\\n10. What is market basket analysis? Give two examples of this application in business.\\nEXERCISES\\n1. Match the columns:\\n1. knowledge discovery process A. reveals reasons for the discovery\\n2. OLAP B. neural networks3. cluster detection C. distance function4. decision trees D. feeds data for mining5. link analysis E. data-driven6. hidden layers F . fraud detection7. genetic algorithms G. user-driven8. data warehouse H. forms groups9. MBR I. highly iterative\\n10. banking application J. associations discovery\\n2. As a data mining consultant, you are hired by a large commercial bank that provides\\nmany financial services. The bank already has a data warehouse that it rolled outtwo years ago. The management wants to find the existing customers who are mostlikely to respond to a marketing campaign offering new services. Outline theknowledge discovery process, list the phases, and indicate the activities in eachphase.\\n3. Describe how decision trees work. Choose an example and explain how this knowl-\\nedge discovery process works.\\n4. What are the basic principles of genetic algorithms? Give an example. Use the ex-\\nample to describe how this technique works.\\n5. In your project you are responsible for analyzing the requirements and selecting a\\ntoolset for data mining. Make a list of the criteria you will use for the toolset selec-tion. Briefly explain why each criterion is necessary.EXERCISES 427', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3291de46-4f11-40e6-b4e3-1b65eb7196dd', embedding=None, metadata={'page_label': '444', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 18\\nTHE PHYSICAL DESIGN PROCESS \\nCHAPTER OBJECTIVES\\n/L50539Distinguish between physical design and logical design as applicable to the data\\nwarehouse\\n/L50539Study the steps in the physical design process in detail\\n/L50539Understand physical design considerations and know the implications\\n/L50539Grasp the role of storage considerations in physical design\\n/L50539Examine indexing techniques for the data warehouse environment\\n/L50539Review and summarize all performance enhancement options\\nAs an IT professional, you are familiar with logical and physical models. Y ou have\\nprobably worked with the transformation of a logical model into a physical model. Y oualso know that completing the physical model has to be tied to the details of the platform,the database software, hardware, and any third-party tools.\\nAs you know, in an OLTP system you have to perform a number of tasks for complet-\\ning the physical model. The logical model forms the primary basis for the physical model.But, in addition, a number of factors must be considered before you can get to the physi-cal model. Y ou must determine where to place the database objects in physical storage.What is the storage medium and what are its features? This information helps you definethe storage parameters. Then you have to plan for indexing, an important consideration.On which columns in each table must the indexes be built? Y ou need to look into othermethods for improving performance. Y ou have to examine the initialization parameters inthe DBMS and decide how to set them. Similarly, in the data warehouse environment, youneed to consider many different factors to complete the physical model.\\nWe have considered the logical model for the data warehouse in sufficient detail. Y ou\\nhave mastered the dimensional modeling technique that helps you design the logical mod-el. In this chapter, we will use the logical model of a data warehouse to develop and com-\\n429Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='32e324a4-b9bc-4e86-a176-154e12af3505', embedding=None, metadata={'page_label': '445', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='plete the physical model. Physical design gets the work of the project team closer to im-\\nplementation and deployment. Every task so far has brought the project to the grand logi-cal model. Now, physical design moves it to the next significant phase. \\nPHYSICAL DESIGN STEPS\\nFigure 18-1 is a pictorial representation of the steps in the physical design process for a\\ndata warehouse. Note the steps indicated in the figure. In the following subsections, wewill broadly describe the activities within these steps. Y ou will understand how at the endof the process you arrive at the completed physical model. After the end of this section,the rest of the chapter elaborates on all the crucial aspects of the physical design.\\nDevelop Standards\\nMany companies invest a lot of time and money to prescribe standards for information sys-\\ntems. The standards range from how to name the fields in the database to how to conductinterviews with the user departments for requirements definition. A group in IT is desig-nated to keep the standards up-to-date. In some companies, every revision must be updatedand authorized by the CIO. Through the standards group, the CIO ensures that the standardsare followed correctly and strictly. Now the practice is to publish the standards on the com-pany’ s intranet. If your IT department is one of the progressive ones giving due attention tostandards, then be happy to embrace and adapt the standards for the data warehouse.\\nIn the data warehouse environment, the scope of the standards expands to include addi-\\ntional areas. Standards ensure consistency across the various areas. If you have the sameway of indicating names of the database objects, then you are leaving less room for ambi-430 THE PHYSICAL DESIGN PROCESS\\nDevelop\\nStandards Create\\nAggregates\\nPlanDetermine\\nData\\nPartitioning\\nEstablish\\nClustering\\nOptions\\nPrepare\\nIndexing\\nStrategy\\nAssign Storage\\nStructuresComplete\\nPhysical\\nModel\\nFigure 18-1 The physical design process.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='818c856f-cd81-4752-85d5-cd022c6978a6', embedding=None, metadata={'page_label': '446', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='guity. Let us say the standards in your company require the name of an object to be a con-\\ncatenation of multiple words separated by dashes and that the first word in the group indi-cates the business subject. With these standards, as soon as someone reads an objectname, that person can know the business subject. \\nStandards take on greater importance in the data warehouse environment. This is be-\\ncause the usage of the object names is not confined to the IT department. The users willalso be referring to the objects by names when they formulate and run their own queries.As standards are quite significant, we will come back to them a little later in this chapter.Now let us move on to the next step in the physical design.\\nCreate Aggregates Plan\\nLet us say that in your environment more than 80% of the queries ask for summary infor-\\nmation. If your data warehouse stores data only at the lowest level of granularity, everysuch query has to read through all the detailed records and sum them up. Consider a querylooking for total sales for the year, by product, for all the stores. If you have detailedrecords keeping sales by individual calendar dates, by product, and by store, then thisquery needs to read a large number of detailed records. So what is the best method to im-prove performance in cases like this? If you have higher levels of summary tables of prod-ucts by store, the query could run faster. But how many such summary tables must youcreate? What is the limit? \\nIn this step, review the possibilities for building aggregate tables. Y ou get clues from\\nthe requirements definition. Look at each dimension table and examine the hierarchicallevels. Which of these levels are more important for aggregation? Clearly assess the trade-off. What you need is a comprehensive plan for aggregation. The plan must spell out theexact types of aggregates you must build for each level of summarization. It is possiblethat many of the aggregates will be present in the OLAP system. If OLAP instances arenot for universal use by all users, then the necessary aggregates must be present in themain warehouse. The aggregate database tables must be laid out and included in the phys-ical model. We will have some more to say about summary levels in a later section.\\nDetermine the Data Partitioning Scheme\\nConsider the data volumes in the warehouse. What about the number of rows in a fact\\ntable? Let us make some rough calculations. Assume there are four dimension tables with50 rows each on average. Even with this limited number of dimension table rows, the po-tential number of fact table rows exceeds six million. Fact tables are generally very large.Large tables are not easy to manage. During the load process, the entire table must beclosed to the users. Again, back up and recovery of large tables pose difficulties becauseof their sheer sizes. Partitioning divides large database tables into manageable parts. \\nAlways consider partitioning options for fact tables. It is not just the decision to parti-\\ntion that counts. Based on your environment, the real decision is about how exactly to par-tition the fact tables. Y our data warehouse may be a conglomerate of conformed datamarts. Y ou must consider partitioning options for each fact table. Should some be parti-tioned vertically and the others horizontally? Y ou may find that some of your dimensiontables are also candidates for partitioning. Product dimension tables are especially large.Examine each of your dimension tables and determine which of these must be partitioned.\\nIn this step, come up with a definite partitioning scheme. The scheme must include:PHYSICAL DESIGN STEPS 431', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6e8c887a-4bb6-4af9-93e7-feda38eecb2b', embedding=None, metadata={'page_label': '447', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539The fact tables and the dimension tables selected for partitioning\\n/L50539The type of partitioning for each table—horizontal or vertical\\n/L50539The number of partitions for each table\\n/L50539The criteria for dividing each table (for example, by product groups)\\n/L50539Description of how to make queries aware of partitions\\nEstablish Clustering Options\\nIn the data warehouse, many of the data access patterns rely on sequential access of large\\nquantities of data. Whenever you have this type of access and processing, you will realizemuch performance improvement from clustering. This technique involves placing andmanaging related units of data in the same physical block of storage. This arrangementcauses the related units of data to be retrieved together in a single input operation.\\nY ou need to establish the proper clustering options before completing the physical\\nmodel. Examine the tables, table by table, and find pairs that are related. This means thatrows from the related tables are usually accessed together for processing in many cases.Then make plans to store the related tables close together in the same file on the medium.For two related tables, you may want to store the records from both files interleaved. Arecord from one table is followed by all the related records in the other table while storingin the same file. \\nPrepare an Indexing Strategy\\nThis is a crucial step in the physical design. Unlike OLTP systems, the data warehouse is\\nquery-centric. As you know, indexing is perhaps the most effective mechanism for im-proving performance. A solid indexing strategy results in enormous benefits. The strategymust lay down the index plan for each table, indicating the columns selected for indexing.The sequence of the attributes in each index also plays a critical role in performance.Scrutinize the attributes in each table to determine which attributes qualify for bit-mappedindexes. \\nPrepare a comprehensive indexing plan. The plan must indicate the indexes for each\\ntable. Further, for each table, present the sequence in which the indexes will be created.Describe the indexes that are expected to be built in the very first instance of the database.Many indexes can wait until you have monitored the data warehouse for some time. Spendenough time on the indexing plan.\\nAssign Storage Structures\\nWhere do you want to place the data on the physical storage medium? What are the phys-\\nical files? What is the plan for assigning each table to specific files? How do you want todivide each physical file into blocks of data? Answers to questions like these go into thedata storage plan. \\nIn an OLTP system, all data resides in the operational database. When you assign the\\nstorage structures in an OLTP system, your effort is confined to the operational tables ac-cessed by the user applications. In a data warehouse, you are not just concerned with thephysical files for the data warehouse tables. Y our storage assignment plan must includeother types of storage such as the temporary data extract files, the staging area, and any432 THE PHYSICAL DESIGN PROCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b6edcae0-fba8-43e9-abee-15326e719e63', embedding=None, metadata={'page_label': '448', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='storage needed for front-end applications. Let the plan include all the types of storage\\nstructures in the various storage areas.\\nComplete Physical Model\\nThis final step reviews and confirms the completion of the prior activities and tasks. By\\nthe time you reach this step, you have the standards for naming the database objects. Y ouhave determined which aggregate tables are necessary and how you are going to partitionthe large tables. Y ou have completed the indexing strategy and have planned for other per-formance options. Y ou also know where to put the physical files.\\nAll the information from the prior steps enables you to complete the physical model.\\nThe result is the creation of the physical schema. Y ou can code the data definition lan-guage statements (DDL) in the chosen RDBMS and create the physical structure in thedata dictionary. \\nPHYSICAL DESIGN CONSIDERATIONS\\nWe have traced the steps for the physical design of the data warehouse. Each step consists\\nof specific activities that finally lead to the physical model. When you look back at thesteps, one step relates to the physical storage structure and several others deal with theperformance of the data warehouse. Physical storage and performance are significant fac-tors. We will cover these two in sufficient depth later in the chapter. \\nIn this section, we will firm up our understanding of the physical model itself. Let us re-\\nview the components and track down what it takes to move from the logical model to thephysical model. First, let us begin with the overall objectives of the physical design process.\\nPhysical Design Objectives\\nWhen you perform the logical design of the database, your goal is to produce a conceptu-\\nal model that reflects the information content of the real-world situation. The logical mod-el represents the overall data components and the relationships. The objectives of thephysical design process do not center on the structure. In physical design, you are gettingcloser to the operating systems, the database software, the hardware, and the platform.Y ou are now more concerned about how the model is going to work than on how the mod-el is going to look.\\nIf you want to summarize, the major objectives of the physical design process are im-\\nproving performance on the one hand, and improving the management of the stored dataon the other. Y ou base your physical design decisions on the usage of data. The frequencyof access, the data volumes, the specific features supported by the chosen RDBMS, andthe configuration of the storage medium influence the physical design decisions. Y ou needto pay special attention to these factors and analyze each to produce an efficient physicalmodel. Now let us present the significant objectives of physical design.\\nImprove Performance. Performance in an OLTP environment differs from that of a\\ndata warehouse in the online response times. Whereas a response time of less than threeseconds is almost mandatory in an OLTP system, the expectation in a data warehouse isless stringent. Depending on the volume of data processed during a query, response timesPHYSICAL DESIGN CONSIDERATIONS 433', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2aa7c98b-a6ac-44dc-a3ab-3ace2cd17060', embedding=None, metadata={'page_label': '449', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='varying from a few seconds to a few minutes are reasonable. Let the users be aware of the\\ndifference in expectations. However, in today’ s data warehouse and OLAP environments,response time beyond a few minutes is not acceptable. Strive to improve performance tokeep the response time at this level. Ensure that performance is monitored regularly andthe data warehouse is kept fine-tuned. \\nMonitoring performance and improving performance must happen at different levels.\\nAt the foundational level, make sure attention is paid by appropriate staff to performanceof the operating system. At the next level lies the performance of the DBMS. Monitoringand performance improvement at this level rests on the data warehouse administrator. Thehigher levels of logical database design, application design, and query formatting alsocontribute to the overall performance.\\nEnsure Scalability. This is a key objective. As we have seen, the usage of the data\\nwarehouse escalates over time with a sharper increase during the initial period. We havediscussed this supergrowth in some detail. During the supergrowth period, it is almost im-possible to keep up with the steep rise in usage. \\nAs you have already observed, the usage increases on two counts. The number of users\\nincreases rapidly and the complexity of the queries intensifies. As the number of users in-creases, the number of concurrent users of the data warehouse also increases proportion-ately. Adopt methods to address the escalation in the usage of the data warehouse on bothcounts.\\nManage Storage. Why is managing storage a major objective of physical design?\\nProper management of stored data will boost performance. Y ou can improve performanceby storing related tables in the same file. Y ou can manage large tables more easily by stor-ing parts of the tables at different places in storage. Y ou can set the space management pa-rameters in the DBMS to optimize the use of file blocks. \\nProvide Ease of Administration. This objective covers the activities that make ad-\\nministration easy. For instance, ease of administration includes methods for properarrangement of table rows in storage so that frequent reorganization is avoided. Anotherarea for ease of administration is in the back up and recovery of database tables. Reviewthe various data warehouse administration tasks. Make it easy for administration whenev-er it comes to working with storage or the DBMS.\\nDesign for Flexibility. In terms of physical design, flexibility implies keeping the de-\\nsign open. As changes to the data model take place, it must be easy to propagate thechanges to the physical model. Y our physical design must have built-in flexibility to satis-fy future requirements.\\nFrom Logical Model to Physical Model\\nIn the logical model you have the tables, attributes, primary keys, and relationships. The\\nphysical model contains the structures and relationships represented in the databaseschema coded with the data definition language (DDL) of the DBMS. What are the activ-ities that transform a logical model into a physical model? Please refer to Figure 18-2. Inthe figure, you see the activities marked alongside the arrow that follows the transforma-tion process. At the end on the right side, notice the box indicated as the physical model.434 THE PHYSICAL DESIGN PROCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8a822582-caff-4711-9456-b89605d618bc', embedding=None, metadata={'page_label': '450', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This is the result of carrying out the activities mentioned alongside the arrow. Review this\\nset of activities and adapt it for your data warehouse environment.\\nPhysical Model Components\\nHaving talked about the physical model in general terms and how to get to it through the\\nphysical design steps, let us now explore its details. The physical model represents the in-formation content at a level closer to the hardware. That means you should have detailssuch as file sizes, field lengths, data types, primary keys, and foreign keys all reflected inthe model. First, please look at Figure 18-3 indicating the major components of the physi-PHYSICAL DESIGN CONSIDERATIONS 435\\nLOGICAL \\nMODEL\\nPHYSICAL \\nMODELConform \\nobject \\nnames to \\nstandardsAlter \\ndata \\ntypes for \\nattributesAssign \\nproper \\ndata type \\nto keysSpecify \\nconstraints \\nfor \\nattributes Define \\nforeign key \\nrelationships\\nInclude \\nconsiderations \\nfor selected \\nDBMSDATA     MODELING     TOOL\\nBe liberal with \\ncomments and \\ninclude them \\neverywhere as \\nnecessary.\\nFigure 18-2 From logical model to physical model.\\nTables ColumnsPrimary \\nKeysForeign \\nKeys\\nIndexesCons-\\ntraintsViewsSyno-\\nnyms\\nCom-\\nmentsUser \\nRolesSecurity \\nPrivilegesFiles/table \\nspacesSCHEMA\\nSUB-\\nSCHEMA\\nDefinitions\\nFigure 18-3 Data warehouse: physical model components.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='04213db5-7f83-4d8a-9100-cb6387fc874e', embedding=None, metadata={'page_label': '451', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='cal model. Make note of the components, one by one. As you know, the components are\\ndescribed to the data dictionary of the DBMS through schemas and subschemas. Y ou usethe data definition language of the DBMS to write the schema definitions. Figure 18-4gives you an example of schema definitions. Please note the different types of schemastatements. Notice the statements defining the database, the tables, and the columns with-in each table. Observe how data types and field sizes are defined. Those of you who haveworked in database administration are quite familiar with the schema statements.\\nLet us tie it all together. Let us relate the components of the logical model to those of\\nthe physical model. Figure 18-5 presents such a combined view. Notice how these relateto the schema definition shown in Figure 18-4.\\nSignificance of Standards\\nStandards in a data warehouse environment cover a wide range of objects, processes, and\\nprocedures. With regard to the physical model, the standard for naming the objects take onspecial significance. Standards provide a consistent means for communication. Effectivecommunication must take place among the members of the project. In a data warehouseproject, the presence of user representatives is more pronounced. As you know, users aremore directly involved in accessing information from a data warehouse than they are in anOLTP environment. Clear communication with the users becomes more significant. \\nA few tips on standards follow.\\nNaming of Database Objects\\nComponents of Object Names. Have a clear method for composing names of the ob-\\njects. The name itself must be able to convey the meaning and description of the object. For436 THE PHYSICAL DESIGN PROCESS\\nFigure 18-4 Sample schema definitions in SQL.CREATE SCHEMA ORDER_ANALYSIS       \\nAUTHORIZATION SAMUEL_JOHNSON…………………...\\nCREATE TABLE PRODUCT  (                    \\nPRODUCT_KEY     CHARACTER ( 8 )    \\nPRIMARY KEY ,\\nPRODUCT_NAME  CHARACTER ( 25 ) ,PRODUCT_SKU           CHARACTER ( 20 ),PRODUCT_ BRAND    CHARACTER ( 25 ) )\\nCREATE TABLE SALESPERSON  (                    \\nSALPERS_KEY            CHARACTER ( 8 )    \\nPRIMARY KEY ,\\nSALPERS_NAME        CHARACTER ( 30 ) ,TERRITORY CHARACTER ( 20 ),REGION CHARACTER ( 20 ) )CREATE TABLE ORDER_FACT  (                    \\nPRODUCT_REF CHARACTER (8 )    \\nPRIMARY KEY ,\\nSALPERS_REF              CHARACTER (8 )    \\nPRIMARY KEY ,\\nORDER_AMOUNT       NUMERIC ( 8,2 ) ,ORDER_COST NUMERIC ( 8,2 ) ,FOREIGN KEY PRODUCT_REF \\nREFERENCES PRODUCT,\\nFOREIGN KEY SALPERS_REF \\nREFERENCES SALESPERSON )    ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6003c764-4cd6-4f68-8836-48acf0a949ff', embedding=None, metadata={'page_label': '452', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='example, look at the name of a column: customer_loan_balance. This naming convention\\nimmediately identifies the column as containing values of balance amounts. What type ofbalance amounts? Loan balances. Is it a total loan balance amount? Whose loan balanceamount? The first word denotes that it is the customer balance and not total balance. Objectnames made up of multiple words generally convey the meanings better. Y ou can standard-ize the function of each word in the name. In our example, the first word denotes the pri-mary subject, the third word the general class of the object, and the second word qualifiesthe class. Many companies adopt this type of naming standard. Y ou may take the standardalready in use in your company and enhance it for clarity and conciseness. \\nWord Separators. Standardize the separators that are also called the delineators. Dash-\\nes ( - ) or underscores ( _ ) are commonly used. If your DBMS has specific conventions orrequirements, follow those conventions. \\nNames in Logical and Physical Models. Names for objects such as tables and attrib-\\nutes may include both the logical model versions and the physical model versions. Y ouneed naming standards for both versions. More than the user community, IT professionalsuse the logical model names. Analysts and logical model designers communicate witheach other through the logical model names. When the users need to refer to the tables andcolumns for data retrieval, they are communicating at the level of the physical model.Therefore, you need to adapt the standards for the physical model for the users. A betterapproach is to keep the logical and physical model versions of the name of a given objectPHYSICAL DESIGN CONSIDERATIONS 437\\nFigure 18-5 Logical model and physical model.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c2ad92a2-bbba-42b7-a2e4-dd0e07063940', embedding=None, metadata={'page_label': '453', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='the same. If you need more qualifiers to further clarify the definition of the object, add\\nthem. Do not hesitate to have the definitions declared in business terminology. \\nNaming of Files and Tables in the Staging Area. As you know, the staging area\\nis a busy place in the data warehouse environment. A lot of data movement happens there.Y ou create many intermediary files from the data extracted from the source systems. Y outransform and consolidate data in this area. Y ou prepare the load files in the staging area.Because of the number of files in the staging area, it is easy to lose track of them. In orderto avoid confusion, you have to be clear about which files serve what purposes. It is nec-essary to adopt effective standards for naming the data structures in the staging area. Con-sider the following suggestions.\\nIndicate the Process. Identify the process to which the file relates. If the file is the\\noutput from a transformation step, let the name of the file denote that. If the file is part ofthe daily incremental update, let that be clear from the name of the file.\\nExpress the Purpose. Suppose you are setting up the scheduling of the weekly update\\nto the product dimension table. Y ou need to know the input load file for this purpose. Ifthe name of the file indicates the purpose for which it was created, that will be a big helpwhen you are setting up the update schedule. Develop standards for the staging area filesto include the purpose of the file in the name.\\nExamples. Given below are the names of a few files in the staging area. See if the fol-\\nlowing names are meaningful and the standards are adequate:\\nsale_units_daily_stage\\ncustomer_daily_updateproduct_full_refreshorder_entry_initial_extractall_sources_sales_extractcustomer_nameaddr_daily_update\\nStandards for Physical Files. Y our standards must include naming conventions for\\nall types of files. These files are not restricted to data and index files for the data ware-house database. There are other files as well. Establish standards for the following:\\n/L50539Files holding source codes and scripts\\n/L50539Database files\\n/L50539Application documents\\nPHYSICAL STORAGE\\nConsider the processing of a query. After the query is verified for syntax and checked\\nagainst the data dictionary for authorization, the DBMS translates the query statements todetermine what data is requested. From the data entries about the tables, rows, andcolumns desired, the DBMS maps the requests to the physical storage where the data ac-438 THE PHYSICAL DESIGN PROCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f91eb2c1-b412-4e52-9852-a4ec263a6f24', embedding=None, metadata={'page_label': '454', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='cess take place. The query gets filtered down to physical storage and this is where the in-\\nput operations begin. The efficiency of the data retrieval is closely tied to where the data isstored in physical storage and how it is stored there. \\nWhat are the various physical data structures in the storage area? What is the storage\\nmedium and what are its characteristics? Do the features of the medium support any effi-cient storage or retrieval techniques? We will explore answers to questions such as these.From the answers you will derive methods for improving performance. First, let us under-stand the types of data structures in the data warehouse. \\nStorage Area Data Structures\\nTake an overall look at all the data related to the data warehouse. First, you have the data\\nin the staging area. Though you may look for efficiency in storage and loading, arrange-ment of the data in the staging area does not contribute to the performance of the datawarehouse from the point of view of the users. Looking further, the other sets of data re-late to the data content of the warehouse. These are the data and index tables in the datawarehouse. How you arrange and store these tables definitely has an impact on the perfor-mance. Next you have the multidimensional data in the OLAP system. In most cases, thesupporting proprietary software dictates the storage and the retrieval of data in the OLAPsystem. \\nPlease see Figure 18-6 showing the physical data structures in the data warehouse. Ob-\\nserve the different levels of data. Notice the detail and summary data structures. Thinkfurther how the data structures are implemented in physical storage as files, blocks, andrecords.PHYSICAL STORAGE 439\\nData Extract Flat\\nFiles\\nRelational Database\\nindex  filesRelational Database\\ndata  files\\n(transformed data)\\nLoad Image Flat\\nFilesData Staging Area\\nRelational Database\\ndata  files\\n(warehouse data)\\nRelational Database\\nindex  filesPartitioned\\nPhysical\\nFilesData Warehouse Repository\\nDetailed\\ndata and\\nlight\\nsummaries\\nOLAP System\\nPhysical files in\\nproprietary matrix format\\nstoring multidimensional\\ncubes of data\\nFigure 18-6 Data structures in the warehouse.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5a427dd4-f5f0-46ac-aa1c-9e06b69ed824', embedding=None, metadata={'page_label': '455', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Optimizing Storage\\nY ou have reviewed the physical storage structures. When you break each data structure\\ndown to the physical storage level, you find that the structure is stored as files in the phys-ical storage medium. Take the example of the customer dimension and the salesperson di-mension tables. Y ou have basically two choices for storing the data of these two dimensiontables. Store records from each table in one physical file. Or, if the records from these ta-bles are retrieved together most of the time, then store records from both the tables in asingle physical file. In either case, records are stored in a file. A collection of records in afile forms a block. In other words, a file comprises blocks and each block containsrecords. \\nIn this subsection, let us examine a few techniques for optimizing storage. Remember\\nany optimizing at the physical level is tied to the features and functions available in theDBMS. Y ou have to relate the techniques discussed here with the workings of yourDBMS. Please study the following optimizing techniques.\\nSet the Correct Block Size. As you understand, a set of records is stored in a block.\\nWhat is special about the block? A data block in a file is the fundamental unit ofinput/output transfer from the database to memory where the data gets manipulated. Eachblock contains a block header that holds control information. The block header is not opento keeping data. Too many block headers means too much wasted space.\\nAssume that the block size for the customer file is 2 KB and that an average of ten cus-\\ntomer records can fit in a single block. Each DBMS has its own default block size. 2 KBand 4 KB are common default block sizes. If the data records requested by a query residein block number 10, then the operating system reads that entire block into memory to getthe required data records.\\nWhat is the effect of increasing the block size in a file? More records or rows will fit\\ninto a single block. Because more records may be fetched in one read, larger block sizesdecrease the number of reads. Another advantage relates to space utilization by the blockheaders. As a percentage of the space in a block, the block header occupies less space in alarger block. Therefore, overall, all the block headers put together occupy less space. Buthere is the downside of larger block sizes. Even when a smaller number of records areneeded, the operating system reads too much extra information into memory, thereby im-pacting memory management.\\nHowever, because most data warehouse queries request large numbers of rows, memo-\\nry management as indicated rarely poses a problem. There is another aspect of data ware-house tables that could cause some concern. Data warehouse tables are denormalized andtherefore the records tend to be large. Sometimes a record may be too large to fit in a sin-gle block. Then the record has to be split across more than one block. The broken partshave to be connected with pointers or physical addresses. Such pointer chains affect per-formance to a large extent.\\nConsider all the factors and set the block size at the appropriate size. Generally, in-\\ncreased block size gives better performance but you have to find the proper size. \\nSet the Proper Block Usage Parameters. Most of the leading DBMSs allow you\\nto set block usage parameters at appropriate values and derive performance improvement.Y ou will find that these usage parameters themselves and the methods for setting them aredependent on the database software. Generally, two parameters govern the usage of the440 THE PHYSICAL DESIGN PROCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='df10b6c9-01b5-4de5-ae3c-dc1de81c13ae', embedding=None, metadata={'page_label': '456', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='blocks, and proper usage of the blocks enhances performance. Let us start with a generic\\nexample of these two parameters and then try to understand the implications. \\nExample:\\nBlock Percent Free 20Block Percent Used 40\\nBlock Percent Free. The DBMS leaves a percentage of each block empty so that the\\nrecords in the block can expand into it. The records can use this reserved area in the blockonly when they expand on update . When a record is modified and expands, then the re-\\nserved area may be used. In the example, this parameter is set at 20. That means 20% ofeach block is reserved for expansion of records while being updated. In a data warehouse,there are hardly any updates. The initial load is all inserts of data records. The incrementalloads are also mostly inserts. A few updates may take place while processing slowlychanging dimension tables. Therefore, setting this parameter at a high value results in toomuch wasted space. The general rule is to set this parameter as low as possible.\\nBlock Percent Used. This parameter sets a watermark level below which the amount of\\nspace used in a block must fall before new records are accepted in that block. Take the ex-ample where this parameter is set at 40. As rows are deleted from a block, the freed space isnot reused until at least 60% of the block is empty. Only when the amount of storage usedfalls below 40% is the freed space reused. What is the situation in a data warehouse?Mostly, addition of new records. There are hardly any deletes except when archiving out ofthe data warehouse. The general rule, therefore, is to set this parameter as high as possible.\\nManage Data Migration. When a record in a block is updated and there is not\\nenough space in the same block for storing the expanded record, then most DBMSs movethe entire updated record to another block and create a pointer to the migrated record.Such migration affects the performance, requiring multiple blocks to be read. This prob-lem may be resolved by adjusting the block percent free parameter. However, migration isnot a major problem in data warehouses because of the negligible number of updates.\\nManage Block Utilization. Performance degenerates when data blocks contain ex-\\ncessive amounts of free space. Whenever a query calls for a full table scan, performancesuffers because of the need to read too many blocks. Manage block underutilization by ad-justing the block percent free parameter downward and the block percent used parameterupward. \\nResolve Dynamic Extension. When the current extent on disk storage for a file is\\nfull, the DBMS finds a new extent and allows an insert of a new record. This task of find-ing a new extension on the fly is referred to as dynamic extension. However, dynamic ex-tension comes with significant overhead. Reduce dynamic extension by allocation oflarge initial extents.\\nEmploy File Striping Techniques. Y ou perform file striping when you split the data\\ninto multiple physical parts and store these individual parts on separate physical devices.File striping enables concurrent input/output operations and improves file access perfor-mance substantially. PHYSICAL STORAGE 441', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='454f5cb6-a7dc-4324-82f6-dca1755dbe43', embedding=None, metadata={'page_label': '457', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Using RAID Technology\\nRedundant array of inexpensive disks (RAID) technology has become common to the ex-\\ntent that almost all of today’ s data warehouses make good use of this technology. Thesedisks are found on large servers. The arrays enable the server to continue operation evenwhile they are recovering from the failure of any single disk. The underlying techniquethat gives the primary benefit of RAID breaks the data into parts and writes the parts tomultiple disks in a striping fashion. The technology can recover data when a disk fails andreconstruct the data. RAID is very fault-tolerant. Here are the basic features of the tech-nology: \\nDisk mirroring— writing the same data to two disk drives connected to the same con-\\ntroller\\nDisk duplexing— similar to mirroring, except here each drive has its own distinct con-\\ntroller\\nParity checking— addition of a parity bit to the data to ensure correct data transmission\\nDisk striping— data spread across multiple disks by sectors or bytes \\nRAID is implemented at six different levels: RAID 0 through RAID 5 .\\nPlease turn to Figure 18-7, which gives you a brief description of RAID. Note the ad-\\nvantages and disadvantages. The lowest level configuration RAID 0, will provide datastriping. At the other end of the range, RAID 5 is a very valuable arrangement.\\nEstimating Storage Sizes\\nNo discussion of physical storage is complete without a reference to estimation of storage\\nsizes. Every action in the physical model takes place in physical storage. Y ou need to442 THE PHYSICAL DESIGN PROCESS\\nRAID 0\\nData records striped across\\nmultiple disks without\\nredundancy\\nHigh performance, less\\nexpensive --entire array out\\neven with single disk failureData interleaved across\\ndisks by bit or block, extra\\ndrives store correction code\\nHigh performance, corrects\\n1-bit errors on the fly, de -\\ntects 2-bit errors --costlyDisk mirroring with data\\nwritten redundantly to pairs\\nof drives\\nHigh read performance and\\navailability --expensive\\nbecause of data duplicationRAID 1 RAID 2\\nRAID 3\\nData interleaved across\\ndisks by bit or block, one\\ndrive stores parity data\\nHigh performance for large\\nblocks of data --on the fly\\nrecovery not guaranteedData records sector -\\ninterleaved across groups of\\ndrives, most popular\\nDedicated parity drive un-\\nnecessary, works with 2 or\\nmore drives --poor writeData records interleaved\\nacross disks by sectors, one\\ndrive stores parity data\\nCan handle multiple I/Os\\nfrom sophisticated OS --\\nused with only two drivesRAID 4 RAID 5\\nFigure 18-7 RAID technology.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d2ed2f1b-c1ba-4ab2-a75a-4e589da160e1', embedding=None, metadata={'page_label': '458', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='know how much of storage space must be made available initially and on an ongoing basis\\nas the data warehouse grows.\\nHere are a few tips on estimating storage sizes:\\nFor each database table, determine\\n/L50539Initial estimate of the number of rows\\n/L50539Average length of the row\\n/L50539Anticipated monthly increase in the number of rows\\n/L50539Initial size of the table in megabytes (MB)\\n/L50539Calculated table sizes in 6 months and in 12 months\\nFor all tables, determine\\n/L50539The total number of indexes\\n/L50539Space needed for indexes, initially, in 6 months, and in 12 months\\nEstimate\\n/L50539Temporary work space for sorting, merging\\n/L50539Temporary files in the staging area\\n/L50539Permanent files in the staging area\\nINDEXING THE DATA WAREHOUSE\\nIn a query-centric system like the data warehouse environment, the need to process\\nqueries faster dominates. There is no surer way of turning your users away from the datawarehouse than by unreasonably slow queries. For the user in an analysis session goingthrough a rapid succession of complex queries, you have to match the pace of the queryresults with the speed of thought. Among the various methods to improve performance,indexing ranks very high. \\nWhat types of indexes must you build in your data warehouse? The DBMS vendors of-\\nfer a variety of choices. The choice is no longer confined to sequential index files. Allvendors support B-Tree indexes for efficient data retrieval. Another option is the bit-mapped index. As we will see later in this section, this indexing technique is very appro-priate for the data warehouse environment. Some vendors are extending the power of in-dexing to specific requirements. These include indexes on partitioned tables andindex-organized tables. \\nIndexing Overview\\nLet us consider the technique of indexing from the perspective of the data warehouse. The\\ndata tables are “read-only.” This feature implies that you almost never update the recordsor delete records. And records are not inserted into the tables after the loads. When you doadds, updates, or deletes, you incur additional overhead for manipulating the index files.But in a data warehouse this is not the case. So you can create a number of indexes foreach table.\\nHow many indexes can you create per table? Most of the indexing is done on the di-\\nmension tables. Generally, you will see a lot more indexes in a data warehouse than inINDEXING THE DATA WAREHOUSE 443', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='38fe221c-499b-451d-a928-a580b8795e17', embedding=None, metadata={'page_label': '459', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='an OLTP system. When a table grows in volume, the indexes also increase in size, re-\\nquiring more storage. The general rule is that the maximum number of indexes varies in-versely with the size of the table. Large numbers of indexes affect the loading processbecause indexes are created for new records at that time. Y ou have to balance the vari-ous factors and decide on the number of indexes per table. Review the tables, one byone.\\nIn the rest of this section, we will study specific indexing techniques in greater depth.\\nBefore doing so, please note the following general principles.\\nIndexes and Loading. When you have a large number of indexes, the loading of data\\ninto the warehouse slows down considerably. This is because when each record is added toa data table, every corresponding index entry must be created. The problem is more acuteduring initial loads. Y ou can address this problem by dropping the indexes before runningthe load jobs. By doing so, the load jobs will not create the index entries during the loadprocess. After the loading process completes, you can run separate jobs to construct theindex files. Construction of the index files takes substantial time, but not as much as cre-ating index entries during the load process.\\nIndexing for Large Tables. Large tables with millions of rows cannot support many\\nindexes. When a table is too large, having more than just one index itself could cause dif-ficulties. If you must have many indexes for the table, consider splitting the table beforedefining more indexes.\\nIndex-Only Reads. As you know, in the data retrieval process the index record is first\\nread and then the corresponding data read takes place. The DBMS selects the best indexfrom among the many indexes. Let us say the DBMS uses an index based on four columnsin a table and many users in your environment request data from these four columns andone more column in the table. How does the data retrieval take place? The DBMS usesthis index record to retrieve the corresponding data record. Y ou need at least twoinput–output (I/O) operations. In this instance, the DBMS has to retrieve the data recordjust for one additional column. In such cases, consider adding that extra column to the in-dex. The DBMS will read the index and find that all the information needed is containedin the index record itself, so it will not read the data record unnecessarily.\\nSelecting Columns for Indexing. How do you select the columns in a table as\\nmost suitable for indexing? Which columns will produce the best performance if in-dexed? Examine the common queries and note the columns that are frequently used toconstrain the queries. Such columns are candidates for indexing. If many queries arebased on product line, then add product line to your list of potential columns for index-ing.\\nA Staged Approach. Many data warehouse administrators are puzzled about how to\\nget started with indexing. How many indexes are needed for each table and whichcolumns must be selected for indexing for the initial deployment of the data warehouse?They do a preliminary review of the tables, but have no experience with real-worldqueries yet. This is precisely the point. Experience cannot be a guideline. Y ou need to waitfor the users to exercise the data warehouse for some time. A staged approach to indexingseems to be prudent. Start with indexes on just the primary and foreign keys of each table.444 THE PHYSICAL DESIGN PROCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='954603d5-f8f4-4c63-878b-e6863e869685', embedding=None, metadata={'page_label': '460', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Keep monitoring the performance carefully. Make a special note of any queries that run\\nfor an inordinately long time. Add indexes as more and more users come on board. \\nB-Tree Index\\nMost database management systems have the B-Tree Index technique as the default in-\\ndexing method. When you code statements using the data definition language of the data-base software to create an index, the system creates a B-Tree index. RDBMSs also createB-Tree indexes automatically on primary key values. The B-Tree index technique su-percedes other techniques because of its data retrieval speed, ease of maintenance, andsimplicity. Now please refer to Figure 18-8 showing an example of a B-Tree Index. Noticethe tree structure with the root at the top. The index consists of a B-Tree (a balanced bi-nary tree) structure based on the values of the indexed column. In the example, the in-dexed column is Name. This B-Tree is created using all the existing names that are thevalues of the indexed column. Observe the upper blocks that contain index data pointingto the next lower block. Think of a B-Tree index as containing hierarchical levels ofblocks. The lowest-level blocks or the leaf blocks point to the rows in the data table. Notethe data addresses in the leaf blocks. \\nIf a column in a table has many unique values, then the selectivity of the column is said\\nto be high. In a territory dimension table, the column for City contains many unique val-ues. This column is therefore highly selective. B-Tree indexes are most suitable for highlyselective columns. Because the values at the leaf nodes will be unique they will lead todistinct data rows and not to a chain of rows. What if a single column is not highly selec-INDEXING THE DATA WAREHOUSE 445\\nA-K\\nL-Z\\nALLEN \\nBUSH \\nCLYNE \\nDUNNEA-D\\nE-G\\nH-K\\nENGEL \\nFARIS  \\nGOREHAIG    \\nIGNAR \\nJONES  \\nKUMARLOEWE \\nMAHER \\nNIXON  \\nOTTOL-O\\nP-R\\nS-Z\\nPAINE \\nQUINN \\nRAJSEGEL \\nTOTO \\nVETRI \\nWILLS\\nENGEL -- address              FARIS   -- address                  GORE   -- address Pointers \\nto data \\nrows\\nFigure 18-8 B-Tree index example.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d51e3523-1831-467f-bc04-aacfc0fce673', embedding=None, metadata={'page_label': '461', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='tive? How can you make use of B-Tree indexing in such cases? For example, the first\\nname column in an employee table is not highly selective. Many common first names ex-ist. But you can improve the selectivity by concatenating the first name with the lastname. The combination is much more selective. Create a concatenated B-Tree index onboth the columns together.\\nIndexes grow in direct proportion to the growth of the indexed data table. Wherever in-\\ndexes contain concatenation of multiple columns, they tend to sharply increase in size. Asthe data warehouse deals with large volumes of data, the size of the index files can because for concern. What can we say about the selectivity of the data in the warehouse? Aremost of the columns highly selective? Not really. If you inspect the columns in the dimen-sion tables, you will notice a number of columns that contain low-selectivity data. B-Treeindexes do not work well with data whose selectivity is low. What is the alternative? Thatleads us to another type of indexing technique.\\nBitmapped Index\\nBitmapped indexes are ideally suitable for low-selectivity data. A bitmap is an ordered se-\\nries of bits, one for each distinct value of the indexed column. Assume that the column forcolor has three distinct colors, namely, white, almond, and black. Construct a bitmap us-ing these three distinct values. Each entry in the bitmap contains three bits. Let us say thefirst bit refers to white, the second to almond, and the third to black. If a product is whitein color, the bitmap entry for that product consists of three bits, where the first bit is set to1, the second bit is set to 0, and the third bit is set to 0. If a product is almond in color, thebitmap entry for that product consists of three bits, where the first bit is set to 0, the sec-ond bit is set to 1, and the third bit is set to 0. Y ou get the picture. Now please study thebitmapped index example shown in Figure 18-9. The figure presents an extract of thesales table and bitmapped indexes for the three different columns. Notice how each entryin an index contains the ordered bits to represent the distinct values in the column. An en-try is created for each row in the base table. Each entry carries the address of the basetable row.\\nHow do the bitmapped indexes work to retrieve the requested rows? Consider a query\\nagainst the sales table in the above example:\\nSelect the rows from Sales table \\nWhere Product is “Washer” and\\nColor is “Almond” and\\nDivision is “East” or “South”\\nFigure 18-10 illustrates how Boolean logic is applied to find the result set based on the\\nbitmapped indexes shown in Figure 18-9.\\nAs you may observe, bitmapped indexes support queries using low-selectivity\\ncolumns. The strength of this technique rests on its effectiveness when using predicates onlow-selectivity columns in queries. Bitmapped indexes take significantly less space thanB-Tree indexes for low-selectivity columns. In a data warehouse, many data accesses arebased on low-selectivity columns. Also, analysis using “what-if ” scenarios requirequeries involving several predicates. Y ou will find that bitmapped indexes are more suit-able for a data warehouse environment than for an OLTP system. 446 THE PHYSICAL DESIGN PROCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4f704fd2-7dbf-4500-97b7-1d537d981a17', embedding=None, metadata={'page_label': '462', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='447Address or Rowid Date Product Color Region Sale ($)\\n00001BFE.0012.0111 15-Nov-00 Dishwasher White East 300\\n00001BFE.0013.0114 15-Nov-00 Dryer Almond West 450\\n00001BFF.0012.0115 16-Nov-00 Dishwasher Almond West 350\\n00001BFF.0012.0138 16-Nov-00 Washer Black North 550\\n00001BFF.0012.0145 17-Nov-00 Washer White South 50000001BFF.0012.0157 17-Nov-00 Dryer White East 400\\n00001BFF.0014.0165 17-Nov-00 Washer Almond South 575\\nAddress or Rowid Bitmap\\n00001BFE.0012.0111 00100001BFE.0013.0114 010\\n00001BFF.0012.0115 001\\n00001BFF.0012.0138 10000001BFF.0012.0145 100\\n00001BFF.0012.0157 010\\n00001BFF.0014.0165 100Address or Rowid Bitmap\\n00001BFE.0012.0111 10000001BFE.0013.0114 010\\n00001BFF.0012.0115 010\\n00001BFF.0012.0138 00100001BFF.0012.0145 100\\n00001BFF.0012.0157 100\\n00001BFF.0014.0165 010\\nAddress or Rowid Bitmap\\n00001BFE.0012.0111 100000001BFE.0013.0114 010000001BFF.0012.0115 0100\\n00001BFF.0012.0138 0010\\n00001BFF.0012.0145 000100001BFF.0012.0157 100000001BFF.0014.0165 0001Extract of Sales Data\\nBitmapped Index for Product Column\\nOrdered bits: Washer, Dryer, DishwasherBitmapped Index for Color Column\\nOrdered bits: White, Almond, Black\\nBitmapped Index for Region Column\\nOrdered bits: East, West, North, South\\nFigure 18-9 Bitmapped index example.\\nProduct\\n001\\n010\\n001\\n100\\n100\\n010\\n100Color\\n100\\n010\\n010\\n001\\n100\\n100\\n010Region\\n1000\\n0100\\n0100\\n0010\\n0001\\n1000\\n0001Select the rows from Sales\\nWhere Product is Washer, and\\nColor is Almond, and\\nDivision is East or South.\\nFirst Bit = 1 Second Bit = 1 First or      \\nFourth Bit = 1 \\nand andAddress or Rowid\\n00001BFE.0012.0111\\n00001BFE.0013.0114\\n00001BFF.0012.0115\\n00001BFF.0012.0138\\n00001BFF.0012.0145\\n00001BFF.0012.0157\\n00001BFF.0014.0165\\nThis row satisfies the query\\nFigure 18-10 Bitmapped indexes: data retrieval.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='122b2908-d5ef-4286-b09e-a5c45a082990', embedding=None, metadata={'page_label': '463', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='On the other hand, if new values are introduced for the indexed columns, the\\nbitmapped indexes have to be reconstructed. Another disadvantage relates to the necessityto access the data tables all the time after the bitmapped indexes are accessed. B-Tree in-dexes do not require table access if the requested information is already contained in theindex.\\nClustered Indexes\\nSome RDBMSs offer a new type of indexing technique. In the B-Tree, bitmapped, or any\\nsequential indexing method, you have a data segment where the values of all columns arestored and an index segment where index entries are kept. The index segment repeats thecolumn values for the indexed columns and also holds the addresses for the entries in thedata segment. Clustered tables combine the data segment and the index segments; the twosegments are one. Data is the index and index is the data.\\nClustered tables improve performance considerably because in one read you get the in-\\ndex and the data segments. Using the traditional indexing techniques, you need one readto get the index segment and a second read to get the data segment. Queries run fasterwith clustered tables when you are looking for exact matches or searching for a range ofvalues. If your RDBMS supports this type of indexing, make use of this technique wher-ever you can in your environment.\\nIndexing the Fact Table\\nWhat do you normally have inside a fact table? What is the nature of the columns? Revis-\\nit the STAR schema. The primary key of the fact table consists of the primary keys of allthe connected dimensions. If you have four dimension tables of store, product, time, andpromotion, then the full primary key of the fact table is the concatenation of the primarykeys of the store, product, time, and promotion tables. What are the other columns? Theother columns are metrics such as sale units, sale dollars, cost dollars, and so on. Theseare the types of columns to be considered for indexing the fact tables.\\nPlease study the following tips and use them when planning to create indexes for the\\nfact tables:\\n/L50539If the DBMS does not create an index on the primary key, deliberately create a B-\\nTree index on the full primary key.\\n/L50539Carefully design the order of individual key elements in the full concatenated key\\nfor indexing. In the high order of the concatenated key, place the keys of the dimen-sion tables frequently referred to while querying.\\n/L50539Review the individual components of concatenated key. Create indexes on combina-\\ntions based on query processing requirements.\\n/L50539If the DBMS supports intelligent combinations of indexes for access, then you may\\ncreate indexes on each individual component of the concatenated key.\\n/L50539Do not overlook the possibilities of indexing the columns containing the metrics.\\nFor example, if many queries look for dollar sales within given ranges, then the col-umn “dollar sales” is a candidate for indexing.\\n/L50539Bitmapped indexing does not apply to fact tables. There are hardly any low-selectiv-\\nity columns.448 THE PHYSICAL DESIGN PROCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='66472b5d-d2a2-493f-b954-ad50cbbc9723', embedding=None, metadata={'page_label': '464', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Indexing the Dimension Tables\\nColumns in the dimension tables are used in the predicates of queries. A query may run\\nlike this: How much are the sales of Product A in the month of March for the Northern Di-vision? Here the columns product, month, and division from three different dimension ta-bles are candidates for indexing. Inspect the columns of each dimension table carefullyand plan the indexes for these tables. Y ou may be not be able to achieve performance im-provement by indexing the columns in the fact tables but the columns in the dimension ta-bles offer tremendous possibilities to improve performance through indexing.\\nHere are a few tips on indexing the dimension tables:\\n/L50539Create a unique B-Tree index on the single-column primary key.\\n/L50539Examine the columns that are commonly used to constrain the queries. These are\\ncandidates for bitmapped indexes.\\n/L50539Look for columns that are frequently accessed together in large dimension tables.\\nDetermine how these columns may be arranged and used to create multicolumn in-dexes. Remember that the columns that are more frequently accessed or thecolumns that are at the higher hierarchical levels in the dimension table are placedat the high order of the multicolumn indexes.\\n/L50539Individually index every column likely to be used frequently in join conditions.\\nPERFORMANCE ENHANCEMENT TECHNIQUES\\nApart from the indexing techniques we have discussed in the previous section, a few other\\nmethods also improve performance in a data warehouse. For example, physically com-pacting the data when writing to storage enables more data to be loaded into a singleblock. That also means that more data may be retrieved in one read. Another method forimproving performance is the merging of tables. Again, this method enables more data tobe retrieved in one read. If you purge unwanted and unnecessary data from the warehousein a regular manner, you can improve the overall performance. \\nIn the remainder of this section, let us review a few other effective performance en-\\nhancement techniques. Many techniques are available through the DBMS, and most ofthese techniques are especially suitable for the data warehouse environment. \\nData Partitioning\\nTypically, the data warehouse holds some very large database tables. The fact tables run\\ninto millions of rows. Dimension tables like the product and customer tables may alsocontain a huge number of rows. When you have tables of such vast sizes, you face certainspecific problems. First, loading of large tables takes excessive time. Then, building in-dexes for large tables also runs into several hours. What about processing of queriesagainst large tables? Queries also run longer when attempting to sort through large vol-umes of data to obtain the result sets. Backing up and recovery of huge tables takes an in-ordinately long time. Again, when you want to selectively purge and archive records froma large table, wading through all the rows takes a long time.\\nWhat if you are able to divide large tables into manageable chunks? Will you not see per-\\nformance improvements? Performing maintenance operations on smaller pieces is easierPERFORMANCE ENHANCEMENT TECHNIQUES 449', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9768df13-9a06-4f10-bac7-8fe464e6181c', embedding=None, metadata={'page_label': '465', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='and faster. Partitioning is a crucial decision and must be planned up front. Doing this after\\nthe data warehouse is deployed and goes into production is time-consuming and difficult. \\nPartitioning means deliberate splitting of a table and its index data into manageable\\nparts. The DBMS supports and provides the mechanism for partitioning. When you definethe table, you can define the partitions as well. Each partition of a table is treated as a sep-arate object. As the volume increases in one partition, you can split that partition further.The partitions are spread across multiple disks to gain optimum performance. Each parti-tion in a table may have distinct physical attributes, but all partitions of the table have thesame logical attributes.\\nWhat are the criteria for splitting a table into partitions? Y ou can split a large table verti-\\ncally or horizontally. In vertical partitioning, you separate out the partitions by grouping se-lected columns together. Each partitioned table contains the same number of rows as theoriginal table. Usually, wide dimension tables are candidates for vertical partitioning.Horizontal partitioning is the opposite. Here you divide the table by grouping selected rowstogether. In a data warehouse, horizontal partitioning based on calendar dates works well.Y ou can split a table into partitions of recent events and past history. This gives you the op-tion to keep the recent events up and running while taking the historical component off-linefor maintenance. Horizontal partitioning of the fact tables produces great benefits.\\nAs you observe, partitioning is an effective technique for storage management and im-\\nproving performance. Let us summarize the benefits:\\n/L50539A query needs to access only the necessary partitions. Applications can be given the\\nchoice to have partition transparency or they may explicitly request an individualpartition. Queries run faster when accessing smaller amounts of data. \\n/L50539An entire partition may be taken off-line for maintenance. Y ou can separately sched-\\nule maintenance of partitions. Partitions promote concurrent maintenance operations.\\n/L50539Index building is faster.\\n/L50539Loading data into the data warehouse is easy and manageable.\\n/L50539Data corruption affects only a single partition. Backup and recovery on a single par-\\ntition reduces downtime. \\n/L50539The input–output load gets balanced by mapping different partitions to the various\\ndisk drives. \\nData Clustering\\nIn the data warehouse, many queries require sequential access of huge volumes of data.\\nThe technique of data clustering facilitates such sequential access. Clustering fosters se-quential prefetch of related data.\\nY ou achieve data clustering by physically placing related tables close to each other in\\nstorage. When you declare a cluster of tables to the DBMS, the tables are placed in neigh-boring areas on disk. How you exercise data clustering depends on the features of theDBMS. Review the features and take advantage of data clustering.\\nParallel Processing\\nConsider a query that accesses large quantities of data, performs summations, and then\\nmakes a selection based on multiple constraints. It is immediately obvious that you willachieve major performance improvement if you can split the processing into components450 THE PHYSICAL DESIGN PROCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2b5a8798-8da5-4600-a7c2-f61934e06f78', embedding=None, metadata={'page_label': '466', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='and execute the components in parallel. The simultaneous concurrent executions will pro-\\nduce the result faster. Several DBMS vendors offer parallel processing features that aretransparent to the users. As a designer of the query, the user need not know how a specificquery must be broken down for parallel processing. The DBMS will do that for the user.\\nParallel processing techniques may be applied to data loading and data reorganization.\\nParallel processing techniques work in conjunction with data partitioning schemes. Theparallel architecture of the server hardware also affects the way parallel processing op-tions may be invoked. Some physical options are critical for effective parallel processing.Y ou have to assess propositions like placing two partitions on the same storage device ifyou need to process them in parallel. Parallel processing and partitioning together providegreat potential for improved performance. But the designer must decide how to use themeffectively.\\nSummary Levels\\nAs we have discussed several times, the data warehouse needs to contain both detailed and\\nsummary data. Select the levels of granularity for the purpose of optimizing theinput–output operations. Let us say you keep the sales data at the levels of daily detail andmonthly summaries. If the users frequently request weekly sales information, then consid-er keeping another summary at the weekly level. On the other hand, if you only keepweekly and monthly summaries and no daily details, every query for daily details cannotbe satisfied from the data warehouse. Choose your summary and detail levels carefullybased on user requirements.\\nAlso, rolling summary structures are especially useful in a data warehouse. Suppose in\\nyour data warehouse you need to keep hourly data, daily data, weekly data, and monthlysummaries. Create mechanisms to roll the data into the next higher levels automaticallywith the passage of time. Hourly data automatically gets summarized into the daily data,daily data into the weekly data, and so on. \\nReferential Integrity Checks\\nAs you know, referential integrity constraints ensure the validity between two related ta-\\nbles. The referential integrity rules in the relational model govern the values of the foreignkey in the child table and the primary key in the parent table. Every time a row is added ordeleted, the DBMS verifies that the referential integrity is preserved. This verification en-sures that parent rows are not deleted while child rows exist and that child rows are notadded without parent rows. Referential integrity verification is critical in the OLTP sys-tems, but it reduces performance. \\nNow consider the loading of data into the data warehouse. By the time the load images\\nare created in the staging area, the data structures have already gone through the phases ofextraction, cleansing, and transformation. The data ready to be loaded has already beenverified for correctness as far as parent and child rows are concerned. Therefore, there isno further need for referential integrity verification while loading the data. Turning offreferential integrity verification produces significant performance gains.\\nInitialization Parameters\\nDBMS installation signals the start of performance improvement. At the start of the in-\\nstallation of the database system, you can carefully plan how to set the initialization para-PERFORMANCE ENHANCEMENT TECHNIQUES 451', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='38e5e6d6-d9ca-4f01-a9a8-cec59602e028', embedding=None, metadata={'page_label': '467', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='meters. Many times you will realize that performance degradation is to a substantial ex-\\ntent the result of inappropriate parameters. The data warehouse administrator has a specialresponsibility to choose the right parameters. \\nFor example, if you set the maximum number of concurrent users too low, the users will\\nrun into bottlenecks. Some users may have to wait to get into the database even though re-sources are available, simply because the parameter was set too low. On the other hand, set-ting this parameter too high results in unnecessary consumption of resources. Next, con-sider the checkpoint frequency. How often must the DBMS write checkpoint records? If therange between two consecutive checkpoints is too narrow, too much system resources willbe used up. Setting the range too wide may affect recovery. These are just a couple of ex-amples. Review all the initialization parameters and set each appropriately.\\nData Arrays\\nWhat are data arrays? Suppose in a financial data mart you need to keep monthly balances\\nof individual line accounts. In a normalized structure, the monthly balances for a year willbe found in twelve separate table rows. Assume that in many queries the users request forthe balances for all the months together. How can you improve the performance? Y ou cancreate a data array or repeating group with twelve slots, each to contain the balance forone month. \\nAlthough creating arrays is a clear violation of normalization principles, this technique\\nyields tremendous performance improvement. In the data warehouse, the time element isinterwoven into all data. Frequently, users look for data in a time series. Another exampleis the request for monthly sales figures for 24 months for each salesperson. If you analyzethe common queries, you will be surprised to see how many need data that can be readilystored in arrays.\\nCHAPTER SUMMARY\\n/L50539Physical design takes the data warehouse implementation closer to the hardware.\\nPhysical design activities may be grouped into seven distinct steps.\\n/L50539The importance of standards cannot be overemphasized. Adopt sound standards\\nduring the physical design process.\\n/L50539Optimizing storage allocation ranks high in the physical design activities. Make use\\nof RAID technology. \\n/L50539Data warehouse performance is heavily dependent on proper indexing strategy. B-\\nTree indexes and bitmapped indexes are suitable.\\n/L50539Other performance improvement schemes that are part of the physical design in-\\nclude the following: data partitioning, data clustering, parallel processing, creationof summaries, adjusting referential integrity checks, proper setting of DBMS ini-tialization parameters, and use of data arrays. \\nREVIEW QUESTIONS\\n1. List any four of the physical design steps. For two of the selected four, describe the\\nactivities.452 THE PHYSICAL DESIGN PROCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3a4d0963-e026-46cf-81c6-b324d7092920', embedding=None, metadata={'page_label': '468', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2. Name the physical design objectives. Which objective do you think ranks as the\\nmost important?\\n3. What are the components that make up the physical model? How are these related\\nto components of the logical model?\\n4. Give two reasons why naming standards are important in a data warehouse envi-\\nronment.\\n5. List any three techniques for optimizing storage. Describe these briefly.6. What is index-only read? How does it improve performance?7. Give two reasons why B-Tree indexing is superior to other indexing methods.8. What is meant by the selectivity for a column in a physical table? What type of in-\\ndexing technique is suitable for low-selectivity data? Why?\\n9. What is data partitioning? Give two reasons why data partitioning is helpful in a\\ndata warehousing environment.\\n10. What is data clustering? Give an example.\\nEXERCISES\\n1. Match the columns:\\n1. fact table A. set at high level\\n2. dynamic extension B. data address in each entry3. file striping C. repeating data group4. block percent used D. combined data and index segments5. B-Tree index E. DBMS finds new extent6. referential integrity check F . set at low level7. clustered index G. candidate for partitioning 8. block percent free H. data addresses in leaf nodes 9. bitmapped index I. store on separate devices\\n10. data array J. suspend for loading\\n2. Prepare an outline for a standards manual for your data warehouse. Consider all\\ntypes of objects and their naming conventions. Indicate why standards are impor-tant. Produce a detailed table of contents.\\n3. Refer back to the STAR schema for orders analysis shown in Figure 10-7. Trans-\\nform it into a physical model. Show all the components of the physical model. Re-late the physical model to the logical model. \\n4. What are the two common block usage parameters? Using appropriate examples,\\ndescribe how storage utilization may be improved in the data warehouse by settingthese parameters properly. How are the settings different from OLTP systems?Why?\\n5. As the data warehouse administrator, performance enhancement is high on your\\nlist. Highlight the techniques you plan to adopt. For each technique, indicate tasksnecessary to implement the technique.EXERCISES 453', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b578d4d7-d3fd-4d50-bc5f-7dd0927371c4', embedding=None, metadata={'page_label': '469', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 19\\nDATA WAREHOUSE DEPLOYMENT \\nCHAPTER OBJECTIVES\\n/L50539Study the role of the deployment phase in the data warehouse development life cycle\\n/L50539Review the major deployment activities and learn how to get them done\\n/L50539Examine the need for a pilot system and classify the types of pilots\\n/L50539Consider data security in the data warehouse environment\\n/L50539Survey the data backup and recovery requirements\\nY ou have now arrived at a point where you are ready to roll out the initial version of the\\ndata warehouse. Deployment is the next phase after construction. In the deploymentphase, you attend to the last few details, turn the data warehouse on, and let the users reapthe benefits. By the time you reach the deployment phase, the majority of the functionsare completed. The main concerns in the deployment phase relate to the users getting thetraining, support, and the hardware and tools they need to get into the warehouse. \\nTo find our place in the whole life cycle of data warehouse development, let us sum-\\nmarize the functions and operations that have been completed up to this point. Here is thelist of major activities already completed in the construction phase:\\n/L50539The infrastructure is in place with the components fully tested.\\n/L50539The validity of the architecture is already verified.\\n/L50539The database is defined. Space allocation for the various tables is completed.\\n/L50539The staging area is fully set up with file allocations.\\n/L50539The extract, transformation, and all other staging area jobs are tested.\\n/L50539The creation of the load images is tested in the development environment. Testing of\\ninitial loads and incremental loads is done. \\n/L50539Query and reporting tools are tested in the development environment. \\n455Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b12aa29a-f4e0-4aff-bba7-6bafbfe22a06', embedding=None, metadata={'page_label': '470', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539The OLAP system is installed and tested.\\n/L50539Web-enabling the data warehouse is completed.\\nMAJOR DEPLOYMENT ACTIVITIES\\nLet us continue from the end of the construction phase. As you observe, a large number of\\ncritical activities have been completed. All the components have been tested. The piecesare in place. Now please look at Figure 19-1 showing the activities in the deploymentphase. Observe the primary tasks in each box representing the activities in this phase. Inthe deployment phase, establish a feedback mechanism for the users to let the projectteam know how the deployment is going. If this is the initial rollout, most of your userswill be new to the processes. Although the users must have received training, substantialhandholding is essential in this phase. Be prepared to provide the support. Let us inspecteach major activity in the deployment phase. As we move along, please pick up tips andadapt them to your environment.\\nComplete User Acceptance\\nProper acceptance of the system by the users is not just a formality in the deployment\\nphase, it is an absolute necessity. Do not proceed to force the deployment before the keyuser representatives express their satisfaction about the data warehouse. Some organiza-tions have procedures for a formal sign-off. Others conduct a series of user acceptancetests in which each function is accepted. It does not matter how the user acceptance activ-ity is completed but do complete it in the manner in which such acceptances are usuallydone in your environment. \\nWho should do the acceptance testing from the side of the users? Remember the users\\nwho are already on the project team? Start with these people. If you have a user liaisonmanager in your project team, then this person must be made responsible. Get the end-456 DATA WAREHOUSE DEPLOYMENT\\nComplete User\\nAcceptancePerform Initial\\nLoadsGet User\\nDesktops Ready\\nComplete Initial\\nUser TrainingInstitute Initial\\nUser SupportDeploy in\\nStagesFinish final testing\\nof all aspects of\\nuser interface\\nincluding system\\nperformance.Load dimension\\ntables followed by\\nthe fact tables.\\nCreate the\\naggregate tables.Install all the\\nneeded desktop\\nuser tools. Test\\neach client\\nmachine.\\nTrain the users on\\ndata warehouse\\nconcepts, relevant\\ncontents, and data\\naccess tools.Set up support to\\nassist the users in\\nbasic usage,\\nanswer questions,\\nand hold hands.Divide the\\ndeployment into\\nmanageable stages\\nin agreement with\\nusers.\\nFigure 19-1 Data warehouse deployment phase.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ade3dd9a-7092-47ce-913a-03f2a560ac96', embedding=None, metadata={'page_label': '471', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='user applications specialists to perform the acceptance testing for their own areas. In addi-\\ntion to the user representatives on the project team, include a few other users for a few fi-nal test sessions. \\nHow should the user acceptance testing be conducted? Which particular users should\\nbe testing at this final phase of deployment? Here are a few tips:\\n/L50539In each subject area or department, let the users select a small number of typical\\nqueries and reports, ones for which they can verify the results without too muchwork or difficulty, but substantial ones involving combinations of dimension tableconstraints. Let the users run the queries and produce the reports. Then produce re-ports from the operational systems for verification. Compare the reports from theoperational systems to the results from the data warehouse. Resolve and account forall seeming discrepancies. Verify and make sure that the reports from the opera-tional systems are free of errors before matching them up with the results from thewarehouse.\\n/L50539This is a good time to test some of the predefined queries and reports. Have each\\nuser group pick a small number of such queries and reports and test the executions.\\n/L50539Have the users test the OLAP system. Create the multidimensional cubes needed for\\nthe test and store the cubes in the OLAP multidimensional database if you areadopting the MOLAP approach. Let the users select about five typical analysis ses-sions to try out. Again, verify the results with reports from the operational systems. \\n/L50539As you know, in almost every warehouse, users need to learn about and be comfort-\\nable with the functioning of the new front-end tools. The majority of the users mustbe able to use the tools with comparative ease. Design acceptance tests for the userrepresentatives to sign-off on the usability of the tools. Of course, most of this typeof testing would have been done at the time of tool selection. But at that time, thetesting would have been done at vendor sites or in the system development environ-ment. Now the verification is being done in the production environment. This is abig difference.\\n/L50539If your data warehouse is Web-enabled, have the users test the Web features. If Web\\ntechnology is used for information delivery, let the users test this aspect. \\n/L50539No user acceptance testing is complete without acceptance of the system perfor-\\nmance. The project must have set the expectation of the users at an agreed level ofperformance. Query response time expectations are usually at the level of about 3 to5 seconds. In fact, individual queries may deviate from the average, and that is un-derstandable. The users will be able to accept such variations provided that these areexceptional and not the norm.\\n/L50539Remember, the acceptance test is useful if conducted in the production environ-\\nment. Y ou may conduct all the previous individual module testing and the overallsystem testing in the development environment. When all the acceptance testing iscompleted successfully, get the sign-off formally or by any other acceptablemethod. This is a signal that the project is ready for full deployment.\\nPerform Initial Loads\\nIn Chapter 12, we discussed the loading of the data warehouse in sufficient depth. We re-\\nviewed how initial loads are done and went over the methods for incremental loads. WeMAJOR DEPLOYMENT ACTIVITIES 457', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ec4a7b49-adea-4b54-b150-84b4524a2188', embedding=None, metadata={'page_label': '472', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='also covered the four different modes for applying data to the warehouse repository. By\\nthe time the project arrives at the deployment phase, the team must have tested sample ini-tial loads and mock incremental loads. Now is the time to do the complete initial load.Also, now the time is also close for doing the first incremental load, which normally takesplace in 24 fours after the deployment. Recalling what we studied in Chapter 12 as back-ground information, let us review the steps of the complete initial load. If you need to goback to Chapter 12 for a brief refresher, please do so now. Especially review how load im-ages are created for dimension and fact tables. The initial load process picks up these loadimages that are already in the format of the table records themselves. \\nHere are the major steps of the complete initial load:\\n/L50539Drop the indexes on the data warehouse relational tables. As you know, index build-\\ning during the loading consumes an enormous chunk of time. Remember that theinitial load deals with very large volumes of data, hundreds of thousands and evenmillions of rows. Y ou cannot afford to have anything slowing down the load process.\\n/L50539As you know, each dimension table record is in a one-to-many relationship with the\\ncorresponding fact table records. That means referential integrity is enforceable bythe DBMS on this relationship. But we assume that the load images have been creat-ed carefully and that we can suspend this restriction to speed up the load process. Thisis up to each team, based on the confidence level for the creation of the load images. \\n/L50539In some cases, the initial loads may run for a number of days. If your initial load\\naborts after a few days of processing because of some system failure, then you havea disaster on your hands. What is the solution? Should you go back to the beginningand start all over again? No. Make sure you have proper checkpointing so that youcan pick up from the latest checkpoint and continue.\\n/L50539Load the dimension tables first for the reasons given in Chapter 12. Remember how\\nthe keys are built for the dimension table records. Recall how the keys for the facttable records are formed from those of the dimension table records. That is why youneed to load the dimension tables first and then the fact tables. Some data ware-house teams opt to load the smaller dimension tables first and verify the loadprocess before starting the loads of the larger tables.\\n/L50539Load the fact tables next. The keys for the fact table records would already have\\nbeen resolved before creating load images in the staging area.\\n/L50539Based on your already established plan for aggregate or summary tables, create the\\naggregate tables based on the records in the dimension and fact tables. Sometimes,load images are created for the aggregate tables beforehand in the staging area. Ifso, apply these load images at this time to create the aggregate tables.\\n/L50539Y ou had suspended index creation during the loads; now build the indexes.\\n/L50539If you had opted not to suspend the enforcement of referential integrity, all the refer-\\nential violations would have recorded on the system load during the load process.Examine the log files and resolve the load exceptions.\\nGet User Desktops Ready\\nGetting the user machines ready for the data warehouse is a comparatively small part in\\nterms of the overall effort from beginning to end. Although the effort may be much lessthan 10% of the total activities, what the users see and experience at their desktops is what458 DATA WAREHOUSE DEPLOYMENT', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9335780d-2027-40e2-aab2-afbaa8f9afce', embedding=None, metadata={'page_label': '473', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='counts for them. The set of desktop tools isthe data warehouse for the users. Therefore,\\npay special attention to the installation of the data access tools, the network connectionsthat link up the user machines to the servers, and the configuration of the middle tiers.Depending upon the method of deployment, consider allocating enough time for gettingthe desktops ready. \\nBefore starting this activity, make a list of configuration needs for the client machines,\\nall the information delivery software to be installed, the hardware setup for the desktopmachines, and the entire spectrum of requirements for network connections. Let us item-ize a few practical suggestions:\\n/L50539Remote deployment of the data access tools for the client machines is a faster\\nmethod. The data warehouse administrators are able to install the software on thevarious machines from a central location, thus avoiding individual visits to the userworkstations. On the other hand, if you plan to install and test the access tools on theclient machines one by one, plan for longer lead time.\\n/L50539Irrespective of whether the deployment method is by remote installation or by indi-\\nvidual visits to user areas, this is a unique opportunity to upgrade the workstationswith other relevant types of software that may be lacking at the user sites. \\n/L50539Desktop tools cannot function without the appropriate server and middle-tier com-\\nponents. Plan for proper timing, installation, and testing of these other components. \\n/L50539Test each client machine to ensure that all components are properly installed and\\nwork well together.\\n/L50539Completion of the desktop readiness activity means that the users can get to their\\nmachines and start accessing the data warehouse information. This activity neces-sarily includes establishing and acquiring the user passwords and logon user-id’ s.Ensure this is done and tested.\\nComplete Initial User Training\\nThe importance of training and orientation for the users cannot be overemphasized. IT\\nprofessionals may think of the separate components of data, applications, and tools. Froman IT department’ s point of view, training is thought of as training about these three com-ponents. But to the users, it is all one. They do not distinguish between applications andtools. The training program must be designed from the users’ point of view. An essentialdifference exists between training of users in operational system implementations anddata warehouse implementations. The capabilities offered in the data warehouse have amuch wider potential. Users are not aware of how much they can really do with the toolsin the data warehouse. \\nTo get started, plan to train the users in the following areas:\\n/L50539Basic database and data storage concepts\\n/L50539Fundamental features of a data warehouse\\n/L50539Contents of the data warehouse as applicable to each user group\\n/L50539Browsing through the warehouse contents\\n/L50539Use of data access and retrieval tools\\n/L50539Applicability of Web technology for information delivery\\n/L50539Set of predefined queries and reportsMAJOR DEPLOYMENT ACTIVITIES 459', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ba4228b7-e072-4d45-bc3c-ad0f20cbac10', embedding=None, metadata={'page_label': '474', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Types of analysis that can be performed\\n/L50539Query templates and how to use them\\n/L50539Report scheduling and delivery\\n/L50539Data load schedule and currency of the data\\n/L50539The support structure, including first line of contact\\nInstitute Initial User Support\\nIn the early days of postdeployment, every member of the data warehouse support staff is\\nusually very busy. Users may have a wide range of questions from basic sign-on to per-forming complex drill-down analysis. Many questions may just relate to the hardware is-sues. The users need an extensive degree of handholding, at least during the initial stages.\\nFigure 19-2 depicts a set-up for the initial user support. Note the basic support centers.\\nThe user representative in each department is the first point of contact. This person musthave been trained well enough to answer most of the questions on the applications anddata content. The user representative is also knowledgeable about the end-user tools onthe desktops. The hotline support comes after the user representative is unable to provideanswers. At least in the initial deployment, this procedure seems to work well. Also, notethe type of support provided by the technical support group.\\nDeploy in Stages\\nBuilding and deploying a data warehouse is a major undertaking for any organization. This\\nis a project that calls for many different types of skills. The data warehouse encompasses460 DATA WAREHOUSE DEPLOYMENT\\nUSER USER \\nREPRESENTATIVE\\nTECHNICAL \\nSUPPORTHOTLINE \\nSUPPORT\\nFirst point of \\ncontact within  \\nthe department\\nProvide remote and onsite \\nsupport on hardware, \\nsystem software, and toolsProvide support on \\nall issues not \\nresolved by User \\nRepresentative \\nFigure 19-2 Initial user support.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d6465be1-283e-4b00-995f-3540ef5b206a', embedding=None, metadata={'page_label': '475', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='several different technologies. Y ou are faced with new techniques of design. Dimensional\\nmodeling is a very different approach not previously used by designers in any operationalsystems. The process of data extraction, transformation, and loading is tedious and labor-intensive. Users receive information in vastly new ways. The effort is big, possibly biggerthan most of the previous information projects any enterprise has ever launched before. \\nUnder these circumstances, what is a reasonable method for deploying your data ware-\\nhouse? Most decidedly, if you parcel the deployment into manageable parts, you can bringthe data warehouse in at a comfortable pace. Make the deployment happen in stages.Clearly chalk out the stages. Plan a schedule that is most effective from the point of viewof the users as well as the project team. \\nIn Chapter 2, we discussed three approaches for building a data warehouse. The top-\\ndown approach started out with a corporate normalized data warehouse feeding severaldepartmental data marts. The bottom-up approach relates to building a group of datamarts without the idea of melding the data marts together. Then the practical approachleads to a set of supermarts that form a conglomerate of conformed data marts.\\nIrrespective of the approach your project team is adopting for whatever reasons most\\nplausible for your environment, divide and stage your deployment. For all the approaches,the overall requirements definition, the architecture, and the infrastructure take the corpo-rate view. Y ou plan for the entire enterprise but you deploy the pieces in well-definedstages. Please refer to Figure 19-3 showing the staging of the deployment. Notice the sug-gested stages under the different approaches. Note how you build the overall enterprise-wide data warehouse first in the top-down approach. Then the dependent data marts aredeployed in a suitable sequence. The bottom-up approach is less structured and less re-fined. In the practical approach, you deploy one data mart at a time. MAJOR DEPLOYMENT ACTIVITIES 461\\nFigure 19-3 Staged data warehouse deployment.Enterprise -\\nwide\\nrequirements\\nand planningEnterprise\\nData\\nWarehouseFirst\\ndepartmental\\ndata martNext\\ndepartmental\\ndata mart-\\nDeploy the overall enterprise data warehouse (E -R model) followed by the dependent\\ndata marts, one by one.\\n-First\\ndepartmental\\ndata martNext\\ndepartmental\\ndata martNext\\ndepartmental\\ndata martNext\\ndepartmental\\ndata mart\\nGather departmental requirements, plan, and deploy the independe nt departmental data\\nmarts, one by one.\\nEnterprise -\\nwide\\nrequirements\\nand planningFirst\\nsubject\\ndata martNext\\nsubject\\ndata mart\\nDeploy the subject data marts (dimensional model), one by one, w ith fully conformed\\ndimensions and facts, according to the pre -planned sequence.Next\\nsubject\\ndata martTOP-DOWN\\nAPPROACHBOTTOM-UP\\nAPPROACHPRACTICAL\\nAPPROACHDeploy the overall enterprise data warehouse (E-R model) followed by the dependent\\ndata marts, one by one.\\nGather departmental requirements, plan, and deploy the independent data marts, oneby one.\\nDeploy the subject data marts (dimensional model), one by one, with fully conformeddimensions and facts, according to the preplanned sequence.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5c104049-fb49-4d82-9b7d-dbbed39761e3', embedding=None, metadata={'page_label': '476', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Y ou remember how conforming the dimensions is key to the success of the practical\\napproach. Let us reconsider this important concept. The reason for this requirement is tobe able to preserve the cohesion of all the data marts that comprise the enterprise datawarehouse. At the fundamental level, conforming of dimensions simply means this: whenwe say “product” in two different data marts, it means the same thing. In other words, theproduct dimension table in every subsequently deployed data mart is the same as the prod-uct dimension table in the first data mart. The tables must be identical in terms of all theattributes and keys.\\nCONSIDERATIONS FOR A PILOT\\nMost companies consider deploying a pilot system before the full deployment of the en-\\ntire warehouse. This is not a matter of substituting the first full-fledged data mart as thepilot. The pilot is separate and distinct, with specific purposes. There are several good rea-sons for deploying a pilot first. The pilot enables your project team to gain broad experi-ence, gain specific experience with the new technologies, and demonstrate proof-of-con-cept to your users. \\nIf your project team opts for a pilot, concentrate on the basics from the beginning.\\nClarify the purpose and goal of the pilot and choose a proper subject area for it. Remem-ber that hardly any pilot deployments are throw-away projects. Treat the pilot with all duerespect as a regular project. \\nWhen is a Pilot Data Mart Useful?\\nEmbarking on a data warehouse deployment is fraught with potential risks for failure. If\\nyou do not strike it right the first time, you may not have a second chance to convinceyour users about the merits of the new paradigm. Success is a primary goal; you cannotrisk a failure. Y ou must be able to demonstrate the potential positive results within a rea-sonably short time and you must be able to manage this activity quite easily. A pilot de-ployment to a small group of users in a limited and closed environment is quite appealing. \\nThis does not, however, imply that a pilot deployment is always necessary. Y our envi-\\nronment may be different. The group of your users may just consists of highly sophisticat-ed analysts, and your IT team may be made up of seasoned veterans for whom any systemis easy. If this is the case in your deployment of the data warehouse, then a pilot may beunnecessary. But most companies are different. Please go over the following list that indi-cates the conditions where pilot deployments are useful:\\n/L50539The user community is totally new to the data warehousing concept.\\n/L50539The users must be shown and convinced of the ease with which they themselves can\\nretrieve information.\\n/L50539The users need to gain experience with new tools and technologies.\\n/L50539The analysts need to perceive the strengths of the analytical features in the data\\nwarehouse.\\n/L50539The sponsors and upper management must observe the benefits of the data ware-\\nhousing concept before getting into it further in a big way.\\n/L50539The IT designers and architects need to gain experience in dimensional modeling\\ntechniques and in the working of the database on this model.462 DATA WAREHOUSE DEPLOYMENT', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='39bf3787-bdc6-4d5a-8404-9193069c3857', embedding=None, metadata={'page_label': '477', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539The project team needs to ensure that all the ETL functions work well.\\n/L50539The project team wants to confirm the working together of all the new infrastructure\\ncomponents such as parallel processing, replication, middleware connections, Web-based technologies, and OLAP elements. \\nTypes of Pilot Projects\\nFrom the list of conditions that warrant consideration for a pilot, you must have inferred\\nthat there could be different types of pilot deployments. When a project team considers apilot, some set of the reasons dominate and, therefore, the pilot would be slanted towardsthose needs. Frequently, pilots are not built for just one set of reasons, but many differentrequirements are meshed together to form the pilot deployment. In this subsection, let usbriefly review six types of pilots. Each pilot stands on a dominating set of reasons buteach may also be serving other purposes as well in small measures. First, please look atFigure 19-4 illustrating the six types of pilots and indicating the major purpose of eachtype.\\nProof-of-Concept Pilot. The data warehouse is the viable solution for decision sup-\\nport. Establishing this proposition is the primary goal of the proof-of-concept pilot. Y oumay have to prove the concept to a broad range of users, including top management. Or,you may have to justify the concept to the sponsors and senior executives for them to ap-CONSIDERATIONS FOR A PILOT 463\\nFigure 19-4 Proof-of-\\nTechnology\\nProve some new \\ntechnologies to the \\nIT professionals on \\nproject teamINITIAL \\nDEPLOYMENT \\nOF THE DATA \\nWAREHOUSEComprehensive \\nTest\\nVerify all \\ninfrastructure and \\narchitectural \\ncomponents\\nBroad      \\nBusiness\\nEarly deliverable \\nbased on a set of user \\nrequirements for a \\nreal business needExpandable \\nSeed\\nManageable and \\ntechnically simple to \\nbe able to integrate \\nwith the deploymentProof-of-\\nConcept\\nJustify data \\nwarehousing concept \\nfor the company and \\nget fundingUser Tool \\nAppreciation\\nProve and appreciate \\nthe features and \\nusage of end-user \\ntool-sets\\nPILOT TYPES\\nFigure 19-4 Types of pilot deployment.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='47c71a52-9185-4b4c-a40d-45217b61f397', embedding=None, metadata={'page_label': '478', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='prove funding for the full data warehouse. Y ou define the scope of the pilot based on the\\naudience. Regardless of the scope, this type of pilot must provide a sampling of all themajor features to show the utility of the warehouse and how easy it is to get information.Y ou focus on the interaction of the information delivery system with the users. Proof-of-concept pilots work with limited amounts of data. \\nThe project team thinks of this type of pilot much earlier in the development scheme.\\nDo not take an inordinately long time. The main goal here is to impress on the minds ofthe users that the data warehouse is a very effective means for information delivery. Gen-erally, no proof-of-concept pilot should take longer than six months to build and explore.Y ou need to keep the focus on effectively presenting the concepts and quickly gaining theapproval. \\nProof-of-Technology Pilot. This is perhaps the simplest and easy to build, but as it\\nis mainly built for IT to prove one or two technologies at a time, this type of pilot is of lit-tle interest to the users. Y ou may just want to test and prove a dimensional modeling toolor a data replication tool. Or, you may want to prove the validity and usefulness of theETL tools. In this type of pilot, you want to get beyond the product demonstrations andclaims of the vendors and look for yourself.\\nThe utility of the proof-of-technology pilots lies in your ability to concentrate and fo-\\ncus on one or two technologies and prove them to your satisfaction. Y ou can check the ap-plicability of a particular type of replication tool to your data warehouse environment.However, as the pilot is confined to proving a small part of the collection of all the tech-nologies, it does not indicate anything about how all the pieces will work together. Thisbrings us to the next type. \\nComprehensive Test Pilot. This is developed and deployed to verify that all the in-\\nfrastructure and architectural components work together and well. It is not as complete inscope as a full-fledged data warehouse and works with a smaller database, but you verifythe data flow throughout the data warehouse from all the source operational systemsthrough the staging area to the information delivery component. \\nThis pilot enables the IT professionals and the users on the project team to appreciate\\nthe complexities of the data warehouse. The team gains experience with the new technolo-gies and tools. This pilot cannot be put together and deployed within a short time. Thescope of the pilot encompasses the entire spectrum of data warehouse functions. It is alsodeployed to benefit the project team more than the users. \\nUser Tool Appreciation Pilot. The thrust of this type of pilot is to provide the users\\nwith tools they will see and use. Y ou place the emphasis on the end-user information de-livery tools. In this type of pilot, you keep the data content and the data accuracy in thebackground. The focus is just on the usability of the tools. The users are able to observeall the features of the end-user tools for themselves, work with them, and appreciate theirfeatures and utility. If different tool sets are provided to different groups of users, you haveto deploy a few versions of this type of pilot. \\nNote that there is little regard for the integrity of the data, nor does this type of pilot\\nwork with the entire data content of the data warehouse. User tool appreciation pilots haverather limited applications. One area where this type is more useful is in the OLAP sys-tem. 464 DATA WAREHOUSE DEPLOYMENT', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='47285e47-1b9e-4c93-abfe-00f25019416d', embedding=None, metadata={'page_label': '479', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Broad Business Pilot. In contrast to the previous type, this type of pilot has a broad-\\ner business scope. Try to understand how this type of pilot gets started. Management iden-tifies some pressing need for decision support in some special business. They are able todefine the requirements fairly well. If something is put together to meet the requirements,the potential for success is great. Management wants to take advantage of the data ware-housing initiatives in the organization. The responsibility rests on the project team tocome up with this highly visible early deliverable business pilot. \\nThis type of pilot based on a specific set of requirements has a few problems. First, you\\nare under time pressure. Depending on the requirements, the scope of the pilot could betoo narrow to get integrated with the rest of the data warehouse later on. Or, the pilotcould turn out to be too complex. A complex project cannot be considered as a pilot. \\nExpandable Seed Pilot. First, note the motivations for this type of pilot. Y ou want to\\ncome up with something with business value. The scope must be manageable. Y ou want tokeep it as technically simple as possible for the users. Nevertheless, you have a choice ofsuitable simple subjects. Simple does not mean useless. Choose a simple, useful, and fair-ly visible business area but plan to go through the length and breadth of the data ware-house features with the pilot. This is like planting a good seed and watching it germinateand then grow. \\nThe project team benefits from such a pilot because they will observe and test the\\nworking of the various parts. Users gain an appreciation of the tools and understand howthey interact with the data warehouse. The data warehouse administration function mayalso be tested.\\nChoosing the Pilot\\nPlease understand that there is no industry-standard naming convention for the pilot types.\\nOne data warehouse practitioner may call a specific type an infrastructure test pilot andanother an architectural planning pilot. The actual names do not matter. The scope, con-tent, and motivations count. Also note that these groupings or types are arbitrary. Y ou mayvery well come up with another four types. However, the major thrust of any pilot comesfrom the same motivations as one of the types described above. Remember that no actualpilot falls exclusively within one specific type. Y ou will see traces of many types in the pi-lot you want to adopt. As the project team is building the data warehouse, it is introducingthe new decision support system in a particular technical and business environment. Thetechnical and business environment of the organization influences the choice of the pilot.Again, the choice also depends on whether the data warehouse project is primarily IT -driven, user-driven, or driven by a truly joint team. \\nLet us examine the conditions in the organization and determine if we can match them\\nwith the type of pilot that is suitable. Please study the guidelines described below.\\nIf your organization is totally new to the concept of data warehousing and your senior\\nmanagement needs convincing and first-hand proof, adopt a proof-of concept pilot. Butmost companies are not in this condition. With so much literature, seminars, and vendorpresentations about data warehousing, practically everyone is at least partially sold on theconcept. The only question may be the applicability of the concept to your organization.\\nProof-of-technology and comprehensive test pilots serve the needs of IT. Users do not\\ngain directly from these two types. If you are expanding your current infrastructure exten-CONSIDERATIONS FOR A PILOT 465', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b49976f3-162a-4191-9ccc-63e1bbd80685', embedding=None, metadata={'page_label': '480', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='sively to accommodate the data warehouse, and if you are adopting new parallel process-\\ning hardware and MOLAP techniques, then these two types merit your consideration. \\nThe importance of user involvement and user training in a data warehouse cannot be\\noverstated. The more the users gain an appreciation of the data warehouse and its benefits,the better it is for the success of the project. Therefore, the user tool appreciation pilot andthe broad business pilot pose substantial advantages. Although the user tool appreciationpilot is very limited in scope and application, it has its place. Usually it is a throw-awaypilot. It cannot be integrated into the main warehouse deployment but it can continue to beused as a training tool. A word about the broad business pilot: this type has the potentialfor great success and can elevate the data warehouse project in the eyes of the top man-agement, but be careful not to bite off more than you can chew. If the scope is too com-plex and large, you could risk failure. \\nAt first blush, the expandable seed pilot appears to be the best choice. Although the\\nusers and the project team can both benefit from this type of pilot because of its con-trolled and limited scope, this pilot may not traverse all the functions and features. But apilot is not really meant to be elaborate. It serves its purpose well if it touches all the im-portant functions. \\nExpanding and Integrating the Pilot\\nThe question arises about what you do with a pilot after it has served its intended primary\\npurpose. What exactly is the purpose and shelf-life of a pilot? Do you have to throw the pi-lot away? Is all the effort expended on a pilot completely wasted? Not so. Every pilot hasspecific purposes. Y ou build and deploy a pilot to achieve certain defined results. Theproof-of-concept pilot has one primarily goal, and one goal only—prove the validity of the466 DATA WAREHOUSE DEPLOYMENT\\nINITIAL \\nDEPLOYMENT \\nOF THE DATA \\nWAREHOUSEProof-of-\\nConcept\\nProof-of-\\nTechnology\\nComprehensive \\nTest\\nUser Tool \\nAppreciation\\nBroad        \\nBusiness\\nExpandable \\nSeedSmall-scale, works with limited \\ndata, not suitable for integration.PILOT          TYPESIntended only to prove new \\ntechnologies for IT.\\nManageable and simple, but \\ndesigned for integration. Early deliverable with broader \\nscope, may  be integrated.Only intended for users to test \\nand become familiar with tools.Only intended for IT to test all \\ninfrastructure/architecture. \\nFigure 19-5 Integrating the pilot into the warehouse.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8542af6a-a16c-4fd1-8d21-dcfda39eb676', embedding=None, metadata={'page_label': '481', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='data warehousing concept to the users and top management. If you are able to prove this\\nproposition with the aid of the pilot, then the pilot is successful and serves its purpose. \\nUnderstand the place of a pilot in the whole data warehouse development effort. A pi-\\nlot is not the initial deployment. It may be a prelude to the initial deployment. Without toomuch modification, a pilot may be expanded and integrated into the overall data ware-house. Please see Figure 19-5 examining each type of pilot and illustrating how some maybe integrated into the data warehouse. Note that the expandable seed pilot stands out asthe best candidate for integration. In each case, observe what needs to be done for integra-tion.\\nSECURITY\\nA data warehouse is a veritable gold mine of information. All of the organization’ s critical\\ninformation is readily available in a format easy to retrieve and use. In a single operationalsystem, security provisions govern a smaller segment of the corporate data but data ware-house security extends to a very large portion of the enterprise data. In addition, the secu-rity provisions must cover all the information that is extracted from the data warehouseand stored in other data areas such as the OLAP system. \\nIn an operational system, security is ensured by authorizations to access the database.\\nY ou may grant access to users by individual tables or through database views. Access re-strictions are difficult to set up in a data warehouse. The analyst at a data warehouse maystart an analysis by getting information from one or two tables. As the analysis continues,more and more tables are involved. The entire query process is mainly ad hoc. Which ta-bles must you restrict and which ones must you keep open to the analyst? \\nSecurity Policy\\nThe project team must establish a security policy for the data warehouse. If you have a se-\\ncurity policy for the organization to govern the enterprise information assets, then makethe security policy for the data warehouse an add-on to the corporate security policy. Firstand foremost, the security policy must recognize the immense value of the informationcontained in the data warehouse. The policy must provide guidelines for granting privi-leges and for instituting user roles. \\nHere are the usual provisions found in the security policy for data warehouses:\\n/L50539The scope of the information covered by the policy\\n/L50539Physical security \\n/L50539Security at the workstation\\n/L50539Network and connections\\n/L50539Database access privileges\\n/L50539Security clearance for data loading\\n/L50539User roles and privileges\\n/L50539Security at levels of summarization\\n/L50539Metadata security\\n/L50539OLAP securitySECURITY 467', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eed5a64b-7c65-4685-9ada-a334ff99b4b3', embedding=None, metadata={'page_label': '482', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Web security\\n/L50539Resolution of security violations\\nManaging User Privileges\\nAs you know, users are granted privileges for accessing the databases of OLTP systems.\\nThe access privilege relates to individuals or groups of users with rights to perform theoperations to create, read, update, or delete data. The access restrictions for these opera-tions may be set at the level of entire tables or at the level of one or more columns of an in-dividual table.\\nMost RDBMSs offer role-based security. As you know, a role is just a grouping of\\nusers with some common requirements for accessing the database. Y ou can create roles byexecuting the appropriate statements using the language component of the database man-agement system. After creating the roles, you can set up the users in the appropriate roles.Access privileges may be granted at the level of a role. When this is done, all the users as-signed to that role receive the same access privileges granted at the role level. Access priv-ileges may also be granted at the individual user level. \\nHow do you handle exceptions? For example, let us say user JANE is part of the role\\nORDERS. Y ou have granted a certain set of access privileges to the role ORDERS. Al-most all these access privileges apply to JANE with one exception. JANE is allowed to ac-cess one more table, namely, the promotion dimension table. How do you work out thisexception? Y ou separately grant privilege to JANE to access the promotion table. Fromthis granting of the additional privilege, JANE can access the promotion table. For every-thing else, JANE derives the privileges from the role ORDERS.\\nFigure 19-6 presents a sample set of roles, responsibilities, and privileges. Please ob-468 DATA WAREHOUSE DEPLOYMENT\\nRun queries and reports \\nagainst data warehouse tables\\nQuery Tool \\nSpecialists\\nData Warehouse \\nAdministrationEnd-Users\\nPower Users / \\nAnalysts\\nHelpdesk / \\nSupport Center\\nSecurity \\nAdministrationROLES RESPONSIBILITIES ACCESS PRIVILEGES\\nSystem : none; Database Admin : none;          \\nTables and Views : selected\\nInstall and maintain DBMS; \\nprovide backup and recovery System : yes; Database Admin : yes;              \\nTables and Views : allRun ad hoc complex queries, \\ndesign and run reportsSystem : none; Database Admin : none;          \\nTables and Views : all\\nHelp user with queries and \\nreports; analyze and explainSystem : none; Database Admin : none;          \\nTables and Views : all\\nInstall and trouble-shoot end-\\nuser and OLAP toolsSystem : none; Database Admin : none;          \\nTables and Views : all\\nGrant and revoke access \\nprivileges; monitor usageSystem : yes; Database Admin : yes;              \\nTables and Views : all\\nSystems / \\nNetwork AdminInstall and maintain Operating \\nSystems and networks System : yes; Database Admin : no;              \\nTables and Views : none\\nFigure 19-6 Roles, responsibilities, and privileges.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6d0c273c-a050-4fc9-8a5e-2413fc2cec9d', embedding=None, metadata={'page_label': '483', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='serve the responsibilities as they relate to the data warehousing environment. Also, please\\nnotice how the privileges match up with the responsibilities.\\nPassword Considerations\\nSecurity protection in a data warehouse through passwords is similar to how it is done in\\noperational systems. Updates to the data warehouse happen only through the data loadjobs. User passwords are less relevant to the batch load jobs. Deletes of the data ware-house records happen infrequently. Only when you want to archive old historical recordsdo the batch programs delete records. The main issue with passwords is to authorize usersfor read-only data access. Users need passwords to get into the data warehouse environ-ment. \\nSecurity administrators can set up acceptable patterns for passwords and also the ex-\\npiry period for each password. The security system will automatically expire the passwordon the date of expiry. A user may change to a new password when he or she receives theinitial password from the administrator. The same must be done just before the expiry ofthe current password. These are additional security procedures.\\nFollow the standards for password patterns in your company. Passwords must be cryp-\\ntic and arbitrary, not easily recognizable. Do not let your users have passwords with theirown names or the names of their loved ones. Do not let users apply their own exotic pat-terns. Have a standard for passwords. Include text and numeric data within a password. \\nThe security mechanism must also record and control the number of unauthorized at-\\ntempts by users to gain access with invalid passwords. After a prescribed number of unau-thorized attempts, the user must be suspended from the data warehouse until the adminis-trator reinstates the user. Following a successful sign-on, the numbers of illegal attemptsmust be displayed. If the number is fairly high, this must be reported. It could mean thatsomeone is trying to work at a user workstation while that user is not there. \\nSecurity Tools\\nIn the data warehouse environment, the security component of the database system itself\\nis the primary security tool. We have discussed role-based security provided by theDBMSs. Security protection goes down to the level of columns in most commercial data-base management systems. \\nSome organizations have third-party security and management systems installed to\\ngovern the security of all systems. If this is the case in your organization, take advantageof the installed security system and bring the data warehouse under the larger securityumbrella. Such overall security systems provide the users with a single sign-on feature. Auser then needs only one sign-on user-id and password for all the computer systems in theorganization. Users need not memorize multiple sign-ons for individual systems. \\nSome of the end-user tools come with their own security system. Most of the OLAP\\ntools have a security feature within the toolset. Tool-based security is usually not as flexi-ble as the security provided in the DBMS. Nevertheless, tool-based security can formsome part of the security solution. Once you set the users up on the security systems in thetoolset, you need not repeat it at the DBMS level, but some data warehouse teams go fordouble protection by invoking the security features of the DBMS also. \\nThe tool-based security, being an integral part of the toolset, cannot be suspended. Just\\nto get into the toolset for accessing the data, you need to get security clearance from theSECURITY 469', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='44655f8d-8fb8-46d5-a488-14dcb173447d', embedding=None, metadata={'page_label': '484', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='toolset software. If you are already planning to use the DBMS itself for security protec-\\ntion, then tool-based security may be considered redundant. Each set of tools from a cer-tain vendor has its own way of indicating information interfaces. Information is organizedinto catalogs, folders, and items as the hierarchy. Y ou may provide security verification atany of the three levels. \\nBACKUP AND RECOVERY\\nY ou are aware of the backup and recovery procedures in OLTP systems. Some of you, as\\ndatabase administrators, must have been responsible for setting up the backups and proba-bly been involved in one or two disaster recoveries. \\nIn an OLTP mission-critical system, loss of data and downtime cannot be tolerated.\\nLoss of data can produce serious consequences. In a system such as airlines reservationsor online order-taking, downtime even for a short duration can cause losses in the millionsof dollars. \\nHow critical are these factors in a data warehouse environment? When an online order-\\ntaking system is down for recovery, you probably can survive for a few hours using manu-al fall-back procedures. If an airlines reservation system is down, there can be no suchmanual fall-back. How do these compare with the situation in the data warehouse? Isdowntime critical? Can the users tolerate a small loss of data?\\nWhy Back Up the Data Warehouse?\\nA data warehouse houses huge amounts of data that has taken years to gather and accu-\\nmulate. The historical data may go back 10 or even up to 20 years. Before the data arrivesat the data warehouse, you know that it has gone through an elaborate process of cleans-ing and transformation. Data in the warehouse represents an integrated, rich history of theenterprise. The users cannot afford to lose even a small part of the data that was sopainstakingly put together. It is critical that you are able to recreate the data if and whenany disaster happens to strike.\\nWhen a data warehouse is down for any length of time, the potential losses are not as\\napparent as in an operational system. Order-taking staff is not waiting for the system tocome back up. Nevertheless, if the analysts are in the middle of a crucial sales season orracing against time to conduct some critical analytical studies, the impact could be morepronounced. \\nObserve the usage of a data warehouse. Within a short time after deployment, the num-\\nber of users increases rapidly. The complexity of the types of queries and analysis sessionsintensifies. Users begin to request more and more reports. Access through Web technolo-gy expands. Very quickly, the data warehouse gains almost mission-critical status. With alarge number of users intimately dependent on the information from the warehouse, back-ing up the data content and ability to recover quickly from malfunctions reaches newheights of importance.\\nIn an OLTP system, recovery requires the availability of backed up versions of the\\ndata. Y ou proceed from the last backup and recover to the point where the system stoppedworking. But you might think that the situation in a data warehouse differs from that in anOLTP system. The data warehouse does not represent an accumulation of data directlythrough data entry. Did not the source operational systems produce the data feeds in the470 DATA WAREHOUSE DEPLOYMENT', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f6f20ad0-26e4-401f-9f21-6f1a3668ec3b', embedding=None, metadata={'page_label': '485', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='first place? Why must you bother to create backups of the data warehouse contents? Can\\nyou not reextract and reload the data from the source systems? Although this appears to bea natural solution, it is almost always impractical. Recreation of the data from the sourcesystems takes enormous lengths of time and your data warehouse users cannot toleratesuch long periods of downtime. \\nBackup Strategy\\nNow that you have perceived the necessity to back up the data warehouse, several ques-\\ntions and issues arise. What parts of the data must be backed up? When to back up? Howto back up? Formulate a clear and well-defined backup and recovery strategy. Althoughthe strategy for the data warehouse may have similarities to that for an OLTP system, stillyou need a separate strategy. Y ou may build on the one that is available in your organiza-tion for OLTP systems, but do take into account the special needs for this new environ-ment.\\nA sound backup strategy comprises several crucial factors. Let us go over some of\\nthem. Here is a collection of useful tips on what to include in your backup strategy:\\n/L50539Determine what you need to back up. Make a list of the user databases, system data-\\nbases, and database logs.\\n/L50539The enormous size of the data warehouse stands out as a dominant factor. Let the\\nfactor of the size govern all decisions in backup and recovery. The need for goodperformance plays a key role.\\n/L50539Strive for a simple administrative setup. \\n/L50539Be able to separate the current from the historical data and have separate procedures\\nfor each segment. The current segment of live data grows with the feeds from thesource operational systems. The historical or static data is the content from the pastyears. Y ou may decide to back up historical data less frequently. \\n/L50539Apart from full backups, also think of doing log file backups and differential\\nbackups. As you know, a log file backup stores the transactions from the last fullbackup or picks up from the previous log file backup. A variation of this is a fulldifferential backup. A differential backup contains all the changes since the lastfull backup.\\n/L50539Do not overlook backing up system databases.\\n/L50539Choosing the medium for backing up is critical. Here, size of the data warehouse\\ndictates the proper choice.\\n/L50539The commercial RDBMSs adopt a “container” concept to hold individual files. A\\ncontainer is a larger storage area that holds many physical files. The containers areknown as table spaces, file groups, and the like. RDBMSs have special methods toback up the entire container more efficiently. Make use of such RDBMS features.\\n/L50539Although the backup functions of the RDBMSs serve the OLTP systems, data ware-\\nhouse backups need higher speeds. Look into backup and recovery tools from third-party vendors. \\n/L50539Plan for periodic archiving of very old data from the data warehouse. A good\\narchival plan pays off by reducing the time for backup and restore and also con-tributes to improvement in query performance.BACKUP AND RECOVERY 471', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b3ee5985-4c2d-4cea-8311-ac8ffefdb9a7', embedding=None, metadata={'page_label': '486', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Setting up a Practical Schedule\\nWithout question, you need to back up the data warehouse properly. Many users will\\neventually depend on the data warehouse for constant flow of information. But the enor-mous size is a serious factor in all decisions about backup and recovery. It takes an inordi-nately long time to back up the full data warehouse. In the event of disasters, reextractingdata from the source operational systems and reloading the data warehouse does not seemto be an option. So, how can you set up a practical schedule for backups? Consider thefollowing issues for making the decisions:\\n/L50539As you know, backups for OLTP systems usually run at night. But in the data ware-\\nhouse environment, the night slots get allocated for the daily incremental loads. Thebackups will have to contend with the loads for system time.\\n/L50539If your user community is distributed in different time zones, finding a time slot be-\\ncomes even more difficult. \\n/L50539Mission-critical OLTP systems need frequent backups. In forward recovery, if you\\ndo not have regular full backups and frequent log file backups, the users must reen-ter the portion of the data that cannot be recovered. Compare this with the datawarehouse. Reentering of data by the users does not apply here. Whatever portioncannot be recovered will have to be reloaded from the source systems if that is pos-sible. The data extraction and load systems do not support this type of recovery.\\n/L50539Setting up a practical backup schedule comes down to these questions. How much\\ndowntime can the users tolerate before the recovery process is completed? How muchdata are the users willing to lose in the worst case scenario? Can the data warehousecontinue to be effective for a long period until the lost data is somehow recovered?\\nA practical backup schedule for your data warehouse certainly depends on the condi-\\ntions and circumstances in your organization. Generally, a practical approach includes thefollowing elements:\\n/L50539Division of the data warehouse into active and static data\\n/L50539Establishing different schedules for active and static data\\n/L50539Having more frequent periodic backups for active data in addition to less frequent\\nbackups for static data\\n/L50539Inclusion of differential backups and log file backups as part the backup scheme\\n/L50539Synchronization of the backups with the daily incremental loads\\n/L50539Saving of the incremental load files to be included as part of recovery if applicable\\nRecovery\\nLet us conclude with a few pointers on the recovery process. Before that, please refer to\\nFigure 19-7 illustrating the recovery process in the data warehouse environment. Noticethe backup files and how they are used in the recovery process. Also, note how the possi-bility of some loss of data exists. Here are a few practical tips:\\n/L50539Have a clear recovery plan. List the various disaster scenarios and indicate how re-\\ncovery will be done in each case.472 DATA WAREHOUSE DEPLOYMENT', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7eff7797-4f9d-417f-ac36-d78e940bf040', embedding=None, metadata={'page_label': '487', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Test the recovery procedure carefully. Conduct regular recovery drills.\\n/L50539Considering the conditions in your organization and the established recovery proce-\\ndure, estimate an average downtime to be expected for recovery. Get a generalagreement from the users about the downtime. Do not surprise the users when thefirst disaster strikes. Let them know that this is part of the whole scheme and thatthey need to be prepared if it should ever happen.\\n/L50539In the case of each outage, determine how long it will take to recover. Keep the\\nusers properly and promptly informed.\\n/L50539Generally, your backup strategy determines how recovery will be done. If you plan\\nto include the possibility of recovering from the daily incremental load files, keepthe backups of these files handy.\\n/L50539If you have to go to the source systems to complete the recovery process, ensure that\\nthe sources will still be available.\\nCHAPTER SUMMARY\\n/L50539Deployment of the first version of the data warehouse follows the construction phase.\\n/L50539Major activities in the deployment phase relate to user acceptance, initial loads,\\ndesktop readiness, initial training, and initial user support.\\n/L50539Pilot systems are appropriate in several sets of circumstances. Common types of pi-\\nlots are proof-of-concept, proof-of-technology, comprehensive test, user tool appre-ciation, broad business, and expandable seed. \\n/L50539Although data security in a data warehouse environment is similar to security in\\nOLTP systems, providing access privileges is more involved because of the natureof data access in the warehouse.CHAPTER SUMMARY 473\\nBackup of \\nhistorical dataBackup of \\ncurrent data Full refresh a few tablesLog File backupIncremental Load\\nSystem \\nCrashTIMELINEFile 1 File 2 File 3\\nA recovery option :\\nUse these backup files\\nFile 1 File 2 File 3Possible loss of \\ndata from the last \\nincremental load\\nFigure 19-7 Data warehouse: recovery.File 1 File 2 File 3\\nFile 1 File 2 File 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='de56e573-6257-4069-944d-f566f68d33eb', embedding=None, metadata={'page_label': '488', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Why back up the data warehouse? Even though there are hardly any direct data up-\\ndates in the data warehouse, there are several reasons for backing up. Schedulingbackups is more difficult and recovery procedures are also more difficult because ofthe data volume in the warehouse.\\nREVIEW QUESTIONS\\n1. List four major activities during data warehouse deployment. For two of these four\\nactivities, describe the key tasks.\\n2. Describe briefly the user acceptance procedure. Why is this important?3. What are the significant considerations for the initial data load?4. Why is it a good practice to load the dimension tables before the fact tables?5. What are the two common methods of getting the desktops ready? Which method\\ndo you prefer? Why?\\n6. What topics should the users be trained on initially?7. Give four common reasons for a pilot system.8. What is a proof-of-concept pilot? Under what circumstances is this type of pilot\\nsuitable?\\n9. List five common provisions to be found in a good security policy.\\n10. Give reasons why the data warehouse must be backed up. How is this different\\nfrom an OLTP system?\\nEXERCISES\\n1. Indicate if true or false:\\nA. It is a good practice to drop the indexes before the initial load.\\nB. The key of the fact table is independent of the keys of the dimension tables.C. Remote deployment of desktop tools is usually faster.D. A pilot data mart is necessary when the users are already very familiar with data\\nwarehousing.\\nE. Backing up the data warehouse is not necessary under any conditions because\\nyou can recover data from the source systems.\\nF . Passwords must be cryptic and arbitrary.G. Always checkpoint the load jobs.H. It is a good practice to load the fact tables before loading the dimension tables.I. Initial training of the users must include basic database and data storage con-\\ncepts.\\nJ. Role-based security provision is not suitable for the data warehouse.\\n2. Prepare a plan for getting the user desktops ready for the initial deployment of your\\ndata warehouse. The potential users are spread across the country in thirty majorcenters. Overseas users from four centers will also be tapping into the data ware-house. Analysts at five major regional offices will be using the OLAP system. Y our474 DATA WAREHOUSE DEPLOYMENT', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='68dc236c-b664-48e2-89b3-e14c3b010bad', embedding=None, metadata={'page_label': '489', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='data warehouse is Web-enabled. Make suitable assumptions, considering all as-\\npects, and work out a plan.\\n3. What are the considerations for deploying the data warehouse in stages? Under\\nwhat circumstances is staged deployment recommended? Describe how you willplan to determine the stages. \\n4. What are the characteristics of the type of pilot system described as a broad busi-\\nness pilot? What are its advantages and disadvantages? Should this type of pilot beconsidered at all? Explain the conditions under which this type of pilot is advisable.\\n5. As the data warehouse administrator, prepare a backup and recovery plan. Indicate\\nthe backup methods and schedules. Explore the recovery options. Describe thescope of the backup function. How will you ensure the readiness to recover fromdisasters?EXERCISES 475', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='daf1106c-bc14-4fcb-bafb-88f0326d8455', embedding=None, metadata={'page_label': '490', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER 20\\nGROWTH AND MAINTENANCE \\nCHAPTER OBJECTIVES\\n/L50539Clearly grasp the need for ongoing maintenance and administration \\n/L50539Understand the collection of statistics for monitoring the data warehouse\\n/L50539Perceive how statistics are used to manage growth and continue to improve perfor-\\nmance\\n/L50539Discuss user training and support functions in detail\\n/L50539Consider other management and administration issues\\nWhere are you at this point? Assume the following plausible scenario. All the user ac-\\nceptance tests were successful. There were two pilots; one was completed to test the spe-cialized end-user toolset and the other was an expandable seed pilot that led to the deploy-ment. Y our project team has successfully deployed the initial version of the datawarehouse. The users are ecstatic. The first week after deployment there were just a fewteething problems. Almost all the initial users appear to be fully trained. With very littleassistance from IT, the users seem to take care of themselves. The first set of OLAP cubesproved their worth and the analysts are already happy. Users are receiving reports over theWeb. All the hard work has paid off. Now what?\\nThis is just the beginning. More data marts and more deployment versions have to fol-\\nlow. The team needs to ensure that it is well poised for growth. Y ou need to make sure thatthe monitoring functions are all in place to constantly keep the team informed of the sta-tus. The training and support functions must be consolidated and streamlined. The teammust confirm that all the administrative functions are ready and working. Database tuningmust continue at a regular pace. \\nImmediately following the initial deployment, the project team must conduct review\\nsessions. Here are the major review tasks:\\n477Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3099a6c0-a5c7-4c79-b1fa-64735565a3af', embedding=None, metadata={'page_label': '491', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Review the testing process and suggest recommendations.\\n/L50539Review the goals and accomplishments of the pilots.\\n/L50539Survey the methods used in the initial training sessions.\\n/L50539Document highlights of the development process. \\n/L50539Verify the results of the initial deployment, matching these with user expectations. \\nThe review sessions and their outcomes form the basis for improvement in the further\\nreleases of the data warehouse. As you expand and produce further releases, let the busi-ness needs, modeling considerations, and infrastructure factors remain as the guiding fac-tors for growth. Follow each release close to the previous release. Y ou can make use of thedata modeling done in the earlier release. Build each release as a logical next step. Avoiddisconnected releases. Build on the current infrastructure.\\nMONITORING THE DATA WAREHOUSE\\nWhen you implement an OLTP system, you do not stop with the deployment. The data-\\nbase administrator continues to inspect system performance. The project team continuesto monitor how the new system matches up with the requirements and delivers the results.Monitoring the data warehouse is comparable to what happens in an OLTP system, exceptfor one big difference. Monitoring an OLTP system dwindles in comparison with themonitoring activity in a data warehouse environment. As you can easily perceive, thescope of the monitoring activity in the data warehouse extends over many features andfunctions. Unless data warehouse monitoring takes place in a formalized manner, desiredresults cannot be achieved. The results of the monitoring gives you the data needed to planfor growth and to improve performance. \\nFigure 20-1 presents the data warehousing monitoring activity and its usefulness. As\\nyou can observe, the statistics serve as the life-blood of the monitoring activity. That leadsinto growth planning and fine-tuning of the data warehouse. \\nCollection of Statistics\\nWhat we call monitoring statistics are indicators whose values provide information about\\ndata warehouse functions. These indicators provide information on the utilization of thehardware and software resources. From the indicators, you determine how the data ware-house performs. The indicators present the growth trends. Y ou understand how well theservers function. Y ou gain insights into the utility of the end-user tools. \\nHow do you collect statistics on the working of the data warehouse? Two common\\nmethods apply to the collection process. Sampling methods and event-driven methods aregenerally used. The sampling method measures specific aspects of the system activity atregular intervals. Y ou can set the duration of the interval. If you set the interval as 10 min-utes for monitoring processor utilization, then utilization statistics are recorded every 10minutes. The sampling method has minimal impact on the system overhead. \\nThe event-driven methods work differently. The recording of the statistics does not\\nhappen at intervals, but only when a specified event takes place. For example, if you wantto monitor the index table, you can set the monitoring mechanism to record the event478 GROWTH AND MAINTENANCE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='99149ef1-87f6-4ec8-940a-9c1cb10437cc', embedding=None, metadata={'page_label': '492', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='when an update takes place to the index table. Event-driven methods add to the system\\noverhead but are more thorough than sampling methods. \\nWhich tools collect statistics? The tools that come with the database server and the\\nhost operating system are generally turned on to collect the monitoring statistics. Overand above these, many third-party vendors supply tools especially useful in a data ware-house environment. Most tools gather the values for the indicators and also interpret theresults. The data collector component collects the statistics; the analyzer component doesthe interpretation. Much of the monitoring of the system occurs in real time. \\nLet us now make a note of the types of monitoring statistics that are useful. The fol-\\nlowing is a random list that includes statistics for different uses. Y ou will find most ofthese applicable to your environment. Here is the list:\\n/L50539Physical disk storage space utilization\\n/L50539Number of times the DBMS is looking for space in blocks or causes fragmentation\\n/L50539Memory buffer activity\\n/L50539Buffer cache usage\\n/L50539Input–output performance\\n/L50539Memory management\\n/L50539Profile of the warehouse content, giving number of distinct entity occurrences (ex-\\nample: number of customers, products, etc.)\\n/L50539Size of each database table\\n/L50539Accesses to fact table records\\n/L50539Usage statistics relating to subject areas\\n/L50539Numbers of completed queries by time slots during the dayMONITORING THE DATA WAREHOUSE 479\\nDATA\\nWAREHOUSE\\nADMINISTRATIONEND -\\nUSERSDATA\\nWAREHOUSE\\nWarehouse Data\\nMonitoring Statistics\\nStatistics Collection\\nSampling\\nSample data\\nwarehouse activity\\nat specific intervals\\nand gather statisticsStatistics Collection\\nEvent -driven\\nRecord statistics\\nwhenever\\nspecified events\\ntake place and\\ntrigger statisticsReview statistics\\nfor growth\\nplanning and\\nperformance\\ntuning\\nFigure 20-1 Data warehouse monitoring.Queries / Reports / Analysis', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c7660bfe-b8f5-41cf-b6c4-5e7bab9a059c', embedding=None, metadata={'page_label': '493', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Time each user stays online with the data warehouse\\n/L50539Total number of distinct users per day\\n/L50539Maximum number of users during time slots daily\\n/L50539Duration of daily incremental loads\\n/L50539Count of valid users\\n/L50539Query response times\\n/L50539Number of reports run each day\\n/L50539Number of active tables in the database\\nUsing Statistics for Growth Planning\\nAs you deploy more versions of the data warehouse, the number of users increases and the\\ncomplexity of the queries intensifies, you then have to plan for the obvious growth. Buthow do you know where the expansion is needed? Why have the queries slowed down?Why have the response times degraded? Why was the warehouse down for expanding thetable spaces? The monitoring statistics provide you with clues as to what is happening inthe data warehouse and how you can prepare for the growth. \\nWe indicate below the types of action that are prompted by the monitoring statistics:\\n/L50539Allocate more disk space to existing database tables\\n/L50539Plan for new disk space for additional tables\\n/L50539Modify file block management parameters to minimize fragmentation\\n/L50539Create more summary tables to handle large number of queries looking for summa-\\nry information\\n/L50539Reorganize the staging area files to handle more data volume\\n/L50539Add more memory buffers and enhance buffer management\\n/L50539Upgrade database servers\\n/L50539Offload report generation to another middle tier\\n/L50539Smooth out peak usage during the 24-hour cycle\\n/L50539Partition tables to run loads in parallel and to manage backups\\nUsing Statistics for Fine-Tuning\\nThe next best use of statistics relates to performance. Y ou will find that a large number of\\nmonitoring statistics prove to be useful for fine-tuning the data warehouse. In a later sec-tion, we will discuss this topic in more detail. For now, let us indicate below the data ware-house functions that are normally improved based on the information derived from thestatistics:\\n/L50539Query performance\\n/L50539Query formulation\\n/L50539Incremental loads\\n/L50539Frequency of OLAP loads\\n/L50539OLAP system480 GROWTH AND MAINTENANCE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2d0d3fb4-1eb4-4002-9d71-bc64af00b286', embedding=None, metadata={'page_label': '494', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Data warehouse content browsing\\n/L50539Report formatting\\n/L50539Report generation\\nPublishing Trends for Users\\nThis is a new concept not usually found in OLTP systems. In a data warehouse, the users\\nmust find their way into the system and retrieve the information by themselves. They mustknow about the contents. Users must know about the currency of the data in the ware-house. When was the last incremental load? What are the subject areas? What is the countof distinct entities? The OLTP systems are quite different. These systems readily presentthe users with routine and standardized information. Users of OLTP systems do not needthe inside view. Please look at Figure 20-2 listing the types of statistics that must be pub-lished for the users.\\nIn your data warehouse is Web-enabled, use the company’ s intranet to publish the sta-\\ntistics for the users. Otherwise, provide the ability to inquire into the dataset where the sta-tistics are kept. \\nUSER TRAINING AND SUPPORT\\nY our project team has constructed the best data warehouse possible. Data extraction from\\nthe source systems was carefully planned and designed. The transformation functionscover all the requirements. The staging area has been laid out well and it supports everyfunction carried out there. Loading of the data warehouse takes place without a flaw. Y ourUSER TRAINING AND SUPPORT 481\\nINTERNAL\\nEND -USERSWEB -ENABLED\\nDATA WAREHOUSE\\nMetadata\\nMonitoring StatisticsWarehouse Data\\nWEB PAGE\\nSTATISTICS AND\\nINFORMATION\\nWarehouse Subjects\\nWarehouse Tables\\nSummary Data\\nWarehouse Navigation\\nWarehouse Statistics\\nPredefined Queries\\nPredefined Reports\\nLast Full Load\\nLast Incremental Load\\nScheduled Downtime\\nContacts for Support\\nUser Tool UpgradesINTRANET\\nFigure 20-2 Statistics for the users.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='705d9cd1-5ed7-4b28-8865-6dd30bd46696', embedding=None, metadata={'page_label': '495', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='end-users have the most effective tools for information retrieval and the tools fit their re-\\nquirements as closely as possible. Every component of the data warehouse works correct-ly and well. With everything in place and working, if the users do not have the right train-ing and support, none of the team’ s efforts matters. It could be one big failure. Y ou cannotoverstate the significance of user training and support, both initially and on an ongoingbasis.\\nTrue, when the project team selected the vendor tools, perhaps some of the users re-\\nceived initial training on the tools. This can never be a substitute for proper training. Y ouhave to set up a training program taking into consideration all of the areas where the usersmust be trained. In the initial period, and continuing after deployment of the first versionof the data warehouse, the users need the support to carry on. Do not underestimate theestablishment of a meaningful and useful support system. Y ou know about the technicaland application support function in OLTP system implementations. For a data warehouse,because the workings are different and new, proper support becomes even more essential.\\nUser Training Content\\nWhat should the users be trained on? What is important and necessary? Try to match the\\ncontent of the training to the anticipated usage. How does each group of users need to in-teract with the data warehouse? If one group of users always uses predefined queries andpreformatted reports, then training these users is easier. If, however, another group of ana-lysts needs to formulate their own ad hoc queries and perform analysis, the content of thetraining program for the analysts becomes more intense.\\nWhile designing the content of user education, you have to make it broad and deep.\\nRemember, the users to be trained in your organization come with different skills andknowledge levels. Generally, users preparing to use the data warehouse possess basiccomputer skills and know how computer systems work. But to almost all of the users, datawarehousing must be novel and different. \\nLet us repeat what was mentioned in the previous chapter. Among other things, three\\nsignificant components must be present in the training program. First, the users must geta good grasp of what is available for them in the warehouse. They must clearly under-stand the data content and how to get to the data. Second, you must tell the users aboutthe applications. What are the preconstructed applications? Can they use the predefinedqueries and reports? If so, how? Next, you must train the users on the tools they need toemploy to access the information. Having said this, please note that users do not com-prehend such divisions of the training program as data content, applications, and tools.Do not plan to divide the training program into these distinct, arbitrary compartments,but keep these as the underlying themes throughout the training program. Please turn toFigure 20-3 showing the important topics to be contained in the training program.Again, a word of caution. The figure groups the topics under the three subjects of datacontent, applications, and tools, just to ensure that no topics are overlooked. Whilepreparing the course syllabus for the training sessions, let the three subjects run throughall the items covered in the course. \\nPreparing the Training Program\\nOnce you have decided on the course contents, you are ready to prepare the training pro-\\ngram itself. Consider what preparation entails. First, the team must decide on the types of482 GROWTH AND MAINTENANCE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='58e0bc1e-e8f1-4848-8876-849e08a74e0c', embedding=None, metadata={'page_label': '496', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='training programs, then establish the course content for each type. Next, determine who\\nhas the responsibility of preparing the course materials. Organize the actual preparation ofthe course materials. Training the trainers comes next. A lot of effort goes into putting to-gether a training program. Do not underestimate what it takes to prepare a good trainingprogram. \\nLet us go over the various tasks needed to prepare a training program. The training pro-\\ngram varies with the requirements of each organization. Here are a few general tips forputting together a solid user training program:\\n/L50539A successful training program depends on the joint participation of user representa-\\ntives and IT. The user representatives on the project team and the subject area ex-perts in the user departments are suitable candidates to work with IT.\\n/L50539Let both IT and users work together in preparing the course materials.\\n/L50539Remember to include topics on data content, applications, and tool usage.\\n/L50539Make a list of all the current users to be trained. Place these users into logical\\ngroups based on knowledge and skill levels. Determine what each group needs to betrained on. By doing this exercise, you will be able to tailor the training program toexactly match the requirements of your organization.\\n/L50539Determine how many different training courses would actually help the users. A\\ngood set of courses consists of an introductory course, an in-depth course, and aspecialized course on tool usage.\\n/L50539The introductory course usually runs for one day. Every user must go through this\\nbasic course.USER TRAINING AND SUPPORT 483\\nDATA CONTENT APPLICATIONS TOOLS\\nSubjects available in \\nthe warehouse.\\nData warehouse \\ndimension and fact \\ntables.\\nData warehouse \\nnavigation.\\nData granularity and \\naggregate tables.\\nSource systems and \\ndata extractions.\\nData transformation \\nand cleansing rules.\\nBusiness terms and \\nmeanings.    Predefined queries.\\nQuery templates.\\nPreformatted reports.\\nReport writer options.\\nData feeds to \\ndownstream \\napplications\\nPre-developed \\napplications.\\nOLAP summaries and \\nmultidimensional \\nanalysis.\\nExecutive Information \\nSystem.Features and functions \\nof the end-user tools.\\nTool interface with the \\nwarehouse metadata.\\nProcedures to sign-on \\nand get into the tool \\nsoftware.\\nUse of tools to \\nnavigate and browse  \\nwarehouse content.\\nUse of tools to \\nformulate queries and \\nobtain results.\\nUse of tools to run \\nreports.\\nFigure 20-3 User training content.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='52d13f37-638d-4eb5-8e03-626952146d71', embedding=None, metadata={'page_label': '497', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Have several tracks in the in-depth course. Each track caters to a specific user group\\nand concentrates on one or two subject areas.\\n/L50539The specialized course on tool usage also has a few variations, depending on the\\ndifferent tool sets. OLAP users must have their own course.\\n/L50539Keep the course documentation simple and direct and include enough graphics. If the\\ncourse covers dimensional modeling, a sample STAR schema helps the users to visu-alize the relationships. Do not conduct a training session without course materials.\\n/L50539As you already know, hands-on sessions are more effective. The introductory course\\nmay just have a demo,. but the other two types of courses go well with hands-on ex-ercises. \\nHow are the courses organized? What are the major contents of each type of course?\\nLet us review some sample course outlines. Figure 20-4 presents three sample outlines,one for each type of course. Use these outlines as guides. Modify the outlines accordingto the requirements of your organization.\\nDelivering the Training Program\\nTraining programs must be ready before the deployment of the first version of the data\\nwarehouse. Schedule the training sessions for the first set of users closer to the deploy-ment date. What the users learned at the training sessions will be fresh in their minds.How the first set of users perceive the usefulness of the data warehouse goes a long wayto ensure a successful implementation, so pay special attention to the first group ofusers.484 GROWTH AND MAINTENANCE\\nIntroductory Course In-depth Course End-User Tool Usage\\nIntroduction to Data \\nWarehousing.\\nIntroduction to the \\nData Warehouse and \\nhow data is stored.\\nData Warehouse \\nnavigation.\\nDimension and fact \\ntables.\\nPredefined queries and \\npreformatted reports.\\nEnd-user applications.\\nHands-on session to \\nview warehouse \\ncontents.Refresher on Data \\nWarehousing.\\nReview of all subjects.\\nDetailed review of \\nselected subject -- fact \\ntables, dimension \\ntables, data \\ngranularity, and \\nsummaries.\\nReview of source \\nsystems and data \\nextractions.\\nReview of \\ntransformations.\\nHands-on session. Tool overview.\\nDetailed review of \\ntool functions.\\nTool feature \\nhighlights.\\nUsage of tool to \\nnavigate and browse \\ndata warehouse \\ncontent. \\nHands-on usage of \\ntool for queries, \\nreports, and analysis.\\nExtra tool features \\nsuch as drill-down, \\nexport of data. \\nFigure 20-4 Sample training course outlines.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f0233725-2dbe-4b41-a051-c3b89fdb901e', embedding=None, metadata={'page_label': '498', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Ongoing training continues for additional sets of users. As you implement the next\\nversions of the data warehouse, modify the training materials and continue offering thecourses. Y ou will notice that in the beginning you need to have a full schedule of cours-es available. Some users may need refresher courses. Remember that users have theirown responsibilities to run the business. They need to find the time to fit into the train-ing slots.\\nBecause of the hectic activity relating to training, especially if you have a large user\\ncommunity, the services of a training administrator become necessary. The administratorschedules the courses, matches the courses with the trainers, makes sure the training ma-terials are ready, arranges for training locations, and takes care of the computing resourcesnecessary for hands-on exercises. \\nWhat must you do about training the executive sponsors and senior management staff?\\nIn OLTP systems, the senior management and executive staff rarely have a need to sitdown at their desktop machines and get into the systems. That changes in the data ware-house environment. This new environment supports all decision makers, especially theones at the higher levels. Of course, the senior officials need not know how to run everyquery and produce every report. But they need to know how to look for the informationthey are interested in. Most of these interested senior managers do not wish to take part incourses with other staff. Y ou need to arrange separate training sessions for these execu-tives, sometimes one-on-one sessions. Y ou may modify the introductory course and offeranother specialized course for the executives.\\nAs the training sessions take place, you will find that some users who need to use the\\ndata warehouse are still not trained. Some users are too busy to be able to get away fromtheir responsibilities. Some analysts and power users may feel that they need not attendany formal course and can learn by themselves. Y our organization must have a definitepolicy regarding this issue. When you allow a user into the data warehouse without evenminimal training, two things usually happen. First, they will disrupt the support structureby asking for too much attention. Second, when they are unable to perform a function orinterpret the results, they will not attribute it to the lack of training but will blame thesystem. Generally, a “no training, no data warehouse access” policy works effectively.\\nUser Support\\nUser support must commence the minute the first user clicks on his mouse to get into the\\ndata warehouse. This is not meant to be dramatic, but to emphasize the significance ofproper support to the users. As you know, user frustration mounts in the absence of a goodsupport system. Support structure must be in place before the deployment of the first ver-sion of the data warehouse. If you have a pilot planned or an early deliverable scheduled,make sure the users will have recourse to getting support.\\nAs an IT professional having worked on other implementations and ongoing OLTP\\nsystems, you are aware of how the support function operates. So let us not try to cover thesame ground you are already familiar with. Let us just go over two aspects of support.First, let us present a tiered approach to user support in a data warehouse environment.Please see Figure 20-5. The figure illustrates the organization of the user support function.Notice the different tiers. Note how the user representatives within each user group act asthe first point of contact.\\nNow let us survey a few points especially pertinent to supporting the data warehouse\\nenvironment. Please note the following points:USER TRAINING AND SUPPORT 485', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f8b9e05b-f710-4bfa-92dd-9cfc1970740e', embedding=None, metadata={'page_label': '499', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Make clear to every user the support path to be taken. Every user must know whom\\nto contact first, whom to call for hardware-type problems, whom to address for theworking of the tools, and so on.\\n/L50539In a multitiered support structure, clarify which tier supports what functions.\\n/L50539If possible, try to align the data warehouse support with the overall user support\\nstructure in the organization.\\n/L50539In a data warehouse environment, you need another type of support. Frequently,\\nusers will try to match the information retrieved from the data warehouse with re-sults obtained from the source operational systems. One segment of your supportstructure must be able to address such data reconciliation issues.\\n/L50539Very often, at least in the beginning, users need handholding for browsing through\\nthe data warehouse contents. Plan for this type of support.\\n/L50539Include support on how to find and execute predefined queries and preformatted re-\\nports.\\n/L50539The user support can serve as an effective conduit to encourage the users based on\\nsuccesses in other departments and to get user feedback on specific issues of theirconcern. Ensure that communications and feedback channels are kept open.\\n/L50539Most enterprises benefit from providing a company Website specially designed for\\ndata warehouse support. Y ou can publish information about the warehouse in gener-486 GROWTH AND MAINTENANCE\\nUSERUSER \\nREPRESENTATIVE\\nTECHNICAL \\nSUPPORTHOTLINE \\nSUPPORT\\nFirst point of \\ncontact within  \\nthe department\\nProvide remote and onsite \\nsupport on hardware, \\nsystem software, and toolsRecord support \\nrequests, render help, \\nand pass on requests \\nas necessary HELPDESK \\nSUPPORTProvide support on all \\nissues not resolved by \\nhotline support \\nThe Multi-\\ntier \\nSupport \\nStructure\\nFigure 20-5 User support structure.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='48c902f5-c938-4150-b7e4-271292513387', embedding=None, metadata={'page_label': '500', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='al, the predefined queries and reports, the user groups, new releases, load schedules,\\nand frequently asked questions (FAQs). \\nMANAGING THE DATA WAREHOUSE\\nAfter the deployment of the initial version of the data warehouse, the management func-\\ntion switches gear. Until now, the emphasis remained on following through the steps ofthe data warehouse development life cycle. Design, construction, testing, user acceptance,and deployment were the watchwords. Now, at this point, data warehouse management isconcerned with two principal functions. The first is maintenance management. The datawarehouse administrative team must keep all the functions going in the best possible man-ner. The second is change management. As new versions of the warehouse are deployed,as new releases of the tools become available, as improvements and automation take placein the ETL functions, the administrative team’ s focus includes enhancements and revi-sions.\\nIn this section, let us consider a few important aspects of data warehouse management.\\nWe will point out the essential factors. Postdeployment administration covers the follow-ing areas:\\n/L50539Performance monitoring and fine-tuning\\n/L50539Data growth management\\n/L50539Storage management\\n/L50539Network management\\n/L50539ETL management\\n/L50539Management of future data mart releases\\n/L50539Enhancements to information delivery\\n/L50539Security administration\\n/L50539Backup and recovery management\\n/L50539Web technology administration\\n/L50539Platform upgrades\\n/L50539Ongoing training\\n/L50539User support\\nPlatform Upgrades\\nY our data warehouse deployment platform includes the infrastructure, the data transport\\ncomponent, end-user information delivery, data storage, metadata, the database compo-nents, and the OLAP system components. More often, a data warehouse is a comprehen-sive cross-platform environment. The components follow a path of dependency, startingwith computer hardware at the bottom, followed by the operating systems, communica-tions systems, the databases, GUIs, and then the application support software. As timegoes on, upgrades to these components are announced by the vendors. \\nAfter the initial rollout, have a proper plan for applying the new releases of the plat-\\nform components. As you have probably experienced with OLTP systems, upgrades causeMANAGING THE DATA WAREHOUSE 487', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='29849641-9228-411d-b3ef-de0957582ece', embedding=None, metadata={'page_label': '501', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='potentially serious interruption to the normal work unless they are properly managed.\\nGood planning minimizes the disruption. Vendors try to force you into upgrades on theirschedule based on their new releases. If the timing is not convenient for you, resist the ini-tiatives from the vendors. Schedule the upgrades at your convenience and based on whenyour users can tolerate interruptions.\\nManaging Data Growth\\nManaging data growth deserves special attention. In a data warehouse, unless you are vig-\\nilant about data growth, it could get out of hand very soon and quite easily. Data ware-houses already contain huge volumes of data. When you start with a large volume of data,even a small percentage increase can result in substantial additional data. \\nIn the first place, a data warehouse may contain too much historical data. Data beyond\\n10 years may not produce meaningful results for many companies because of the changedbusiness conditions. End-users tend to opt for keeping detailed data at the lowest grain. Atleast in the initial stages, the users continue to match results from the data warehouse withthose from the operational systems. Analysts produce many types of summaries in thecourse of their analysis sessions. Quite often, the analysts want to store these intermediarydatasets for use in similar analysis in the future. Unplanned summaries and intermediarydatasets add to the growth of data volumes. \\nHere are just a few practical suggestions to manage data growth:\\n/L50539Dispense with some detail levels of data and replace them with summary tables.\\n/L50539Restrict unnecessary drill-down functions and eliminate the corresponding detail\\nlevel data.\\n/L50539Limit the volume of historical data. Archive old data promptly.\\n/L50539Discourage analysts from holding unplanned summaries. \\n/L50539Where genuinely needed, create additional summary tables.\\nStorage Management\\nAs the volume of data increases, so does the utilization of storage. Because of the huge data\\nvolume in a data warehouse, storage costs rank very high as a percentage of the total cost.Experts estimate that storage costs are almost four or five times software costs, yet you findthat storage management does not receive sufficient attention from data warehouse devel-opers and managers. Here are a few tips on storage management to be used as guidelines:\\n/L50539Additional rollouts of the data warehouse versions require more storage capacity.\\nPlan for the increase. \\n/L50539Ensure that the storage configuration is flexible and scalable. Y ou must be able to\\nadd more storage with minimum interruption to the current users.\\n/L50539Use modular storage systems. If not already in use, consider a switchover.\\n/L50539If yours is a distributed environment with multiple servers having individual storage\\npools, consider connecting the servers to a single storage pool that can be intelli-gently accessed.\\n/L50539As usage increases, plan to spread data over multiple volumes to minimize access\\nbottlenecks.488 GROWTH AND MAINTENANCE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='afd0814a-8dfb-4cbf-9fc1-99f14ee8cdb9', embedding=None, metadata={'page_label': '502', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Ensure ability to shift data from bad storage sectors.\\n/L50539Look for storage systems with diagnostics to prevent outages.\\nETL Management\\nThis is a major ongoing administrative function, so attempt to automate most of it. Install\\nan alert system to call attention to exceptional conditions. The following are useful sug-gestions on ETL (data extraction, transformation, loading) management:\\n/L50539Run daily extraction jobs on schedule. If source systems are not available under ex-\\ntraneous circumstances, reschedule extraction jobs.\\n/L50539If you employ data replication techniques, ensure that the result of the replication\\nprocess checks out.\\n/L50539Ensure that all reconciliation is complete between source system record counts and\\nrecord counts in extracted files.\\n/L50539Make sure all defined paths for data transformation and cleansing are traversed cor-\\nrectly.\\n/L50539Resolve exceptions thrown out by the transformation and cleansing functions. \\n/L50539Verify load image creation processes, including creation of the appropriate key val-\\nues for the dimension and fact table rows.\\n/L50539Check out the proper handling of slowly changing dimensions.\\n/L50539Ensure completion of daily incremental loads on time.\\nData Model Revisions\\nWhen you expand the data warehouse in future releases, the data model changes. If the\\nnext release consists of a new data mart on a new subject, then your model gets expandedto include the new fact table, dimension tables, and also any aggregate tables. The physi-cal model changes. New storage allocations are made. What is the overall implication ofrevisions to the data model? Here is a partial list that may be expanded based on the con-ditions in your data warehouse environment:\\n/L50539Revisions to metadata\\n/L50539Changes to the physical design\\n/L50539Additional storage allocations\\n/L50539Revisions to ETL functions\\n/L50539Additional predefined queries and preformatted reports\\n/L50539Revisions to OLAP system\\n/L50539Additions to security system\\n/L50539Additions to backup and recovery system\\nInformation Delivery Enhancements\\nAs time goes on, you will notice that your users have outgrown the end-user tools they\\nstarted out with. In the course of time, the users become more proficient with locating andMANAGING THE DATA WAREHOUSE 489', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9a7f64ba-a979-4ac5-9f73-23d8c638a0af', embedding=None, metadata={'page_label': '503', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='using the data. They get ready for more and more complex queries. New end-user tools\\nappear in the market all the time. Why deny your users the latest and the best if they reallycan benefit from them?\\nWhat are the implications of enhancing the end-user tools and adopting a different tool\\nset? Unlike a change to ETL, this change relates to the users directly, so plan the changecarefully and proceed with caution. \\nPlease review the following tips:\\n/L50539Ensure compatibility of the new tool set with all data warehouse components.\\n/L50539If the new tool set is installed in addition to the existing one, switch your users over\\nin stages.\\n/L50539Ensure integration of end-user metadata.\\n/L50539Schedule training on the new tool set.\\n/L50539If there are any data-stores attached to the original tool set, plan for the migration of\\nthe data to the new tool set. \\nOngoing Fine-Tuning\\nAs an IT professional, you are familiar with the techniques for fine-tuning OLTP systems.\\nThe same techniques apply to the fine-tuning of the data warehouse. The techniques arevery much the same except for one big difference: the data warehouse contains a lot more,in fact, many times more data than a typical OLTP system. The techniques will have to ap-ply to an environment replete with mountains of data.\\nThere may not be any point in repeating the indexing and other techniques that you al-\\nready know from the OLTP environment. Let us just go over a few practical suggestions: \\n/L50539Have a regular schedule to review the usage of indexes. Drop the indexes that are no\\nlonger used.\\n/L50539Monitor query performance daily. Investigate long-running queries. Work with the\\nuser groups that seem to be executing long-running queries. Create indexes if need-ed.\\n/L50539Analyze the execution of all predefined queries on a regular basis. RDBMSs have\\nquery analyzers for this purpose. \\n/L50539Review the load distribution at different times every day. Determine the reasons for\\nlarge variations. \\n/L50539Although you have instituted a regular schedule for ongoing fine-tuning, from time\\nto time, you will come across some queries that suddenly cause grief. Y ou will hearcomplaints from a specific group of users. Be prepared for such ad hoc fine-tuningneeds. The data administration team must have staff set apart for dealing with thesesituations. \\nCHAPTER SUMMARY\\n/L50539Immediately following the initial deployment, the project team must conduct review\\nsessions.490 GROWTH AND MAINTENANCE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9b401915-99b1-4a3c-96aa-aea0cef998e1', embedding=None, metadata={'page_label': '504', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Ongoing monitoring of the data warehouse requires collection of statistics on a vari-\\nety of indicators. Use the statistics for growth planning and for fine-tuning.\\n/L50539User training function consists of determining the content of the needed training,\\npreparation of the training program, and delivering the training program. \\n/L50539User support function needs to have multiple tiers to deliver the appropriate support\\nrelating to data content, applications, and tools.\\n/L50539Ongoing management and administration includes the following: platform up-\\ngrades, managing data growth, storage management, ETL management, data modelrevisions, information delivery enhancements, and ongoing fine-tuning.\\nREVIEW QUESTIONS\\n1. List the types of statistics collected for monitoring the functioning of the data\\nwarehouse. \\n2. Describe any six different types of action for growth planning prompted by the\\ncollected statistics.\\n3. How do the statistics help in fine-tuning the data warehouse?4. Do you think publishing the statistics and similar data for the users is helpful? If\\nso, why?\\n5. What are the three main subjects in user training content? Why are these essential?6. Describe any four major tasks needed to prepare a training program.7. What are the responsibilities of the training administrator?8. Do you think a multitier user support structure is suitable for the data warehouse\\nenvironment? What are the alternatives?\\n9. What role can the company’ s intranet play in training and user support?\\n10. List any five factors to be part of ETL management. \\nEXERCISES\\n1. Set up a procedure for gathering statistics from the data warehouse following the\\ndeployment. List all the types of statistics to be gathered. Indicate how each type ofstatistics will be used for the ongoing support and improvements.\\n2. Y ou are specifically assigned to improve the query performance in your data ware-\\nhouse, deployed about six months ago. How will you plan for the assignment? Whatare the types of statistics you would need? Create a detailed plan.\\n3. Y ou are hired as the new training administrator for the data warehouse in a na-\\ntionwide automobile dealership. The data warehouse is Web-enabled. The analystsin the headquarters work with the OLAP system based on the MOLAP model.Establish an improved ongoing training program. Describe the highlights of theprogram.\\n4. Review the user support structure illustrated in Figure 20-5. Is this support structure\\nadequate for a Web-enabled data warehouse with about 250 internal users and 150outside business partners accessing the data warehouse? About 20 full-time ana-EXERCISES 491', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bd1e793d-5e44-4120-9140-0eea778f0f62', embedding=None, metadata={'page_label': '505', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='lysts use the OLAP system for complex analysis and studies. What improvements\\nwould you want to make to the support structure? Indicate the highlights.\\n5. As the Manager for the data warehouse project, write a project completion report to\\nyour CIO and the executive project sponsor. List the major activities completed.Mention the plan for staged deployment of future releases. Indicate the plans forongoing maintenance. Briefly highlight each topic on growth and maintenance.492 GROWTH AND MAINTENANCE', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='87699545-cd99-4db0-9aa0-cf644d94600b', embedding=None, metadata={'page_label': '506', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='APPENDIX A\\nPROJECT LIFE CYCLE STEPS \\nAND CHECKLISTS\\nDATA WAREHOUSE PROJECT LIFE CYCLE: \\nMAJOR STEPS AND SUBSTEPS\\nNote: The substeps indicated here are at a high level. Expand these substeps as necessary\\nto suit the requirements of your environment.\\nPROJECT PLANNING\\nDefinition of scope, goals, objectives, and expectationsEstablishment of implementation strategyPreliminary identification of project resourcesAssembling of project teamEstimation of project schedule\\nREQUIREMENTS DEFINITION\\nDefinition of requirements gathering strategyConducting of interviews and group sessions with usersReview of existing documentsStudy of source operational systemsDerivation of business metrics and dimensions needed for analysis\\nDESIGN\\nDesign of the logical data modelDefinition of data extraction, transformation, and loading functionsDesign of the information delivery framework Establishment of storage requirementsDefinitions of the overall architecture and supporting infrastructure\\n493Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bacab7f4-5d7c-4cee-8613-015893d2bd8a', embedding=None, metadata={'page_label': '507', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CONSTRUCTION\\nSelection and installation of infrastructure hardware and softwareSelection and installation of the DBMSsSelection and installation of ETL and information delivery toolsCompletion of the design of the physical data modelCompletion of the metadata component\\nDEPLOYMENT\\nCompletion of user acceptance testsPerformance of initial data loadsMaking user desktops ready for the data warehouseTraining and support for the initial set of usersProvision for data warehouse security, backup, and recovery\\nMAINTENANCE\\nOngoing monitoring of the data warehouseContinuous performance tuningOngoing user trainingProvision of ongoing support to usersOngoing data warehouse management\\nCHECKLISTS FOR MAJOR ACTIVITIES\\nNote: Break the following major activities down into detailed tasks. Prepare your own\\nchecklists for the tasks in each major activity. \\nPROJECT PLANNING\\nComplete definition of project scope and objectivesPrepare project initiation documentConduct project initiation sessions with sponsor and user representativesComplete selection of project team membersPrepare project scheduleComplete project plan document\\nREQUIREMENTS DEFINITION\\nComplete summaries of interviews and group sessionsSummarize available information on data from source systemsDocument information delivery requirements to user groupsDiscuss and agree with users on system performanceComplete information package diagrams showing metrics and dimensionsPrepare and publish requirements definition document\\nDESIGN\\nComplete the E-R and dimensional diagrams for the logical data modelDocument all data extraction and data staging functionsEstimate data storage requirements Classify users and determine information delivery criteria for each class Document desired features needed in all types of tools 494 PROJECT LIFE CYCLE STEPS AND CHECKLISTS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='141051c0-967a-45b0-872f-42c1df5d3404', embedding=None, metadata={'page_label': '508', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Create design specifications for any in-house programs for data staging, loading, and\\ninformation delivery  \\nCONSTRUCTION\\nInstall and test infrastructure hardware and softwareInstall and test selected DBMSsInstall and test selected toolsComplete the physical data model and allocate storageTest initial and ongoing data loadsComplete overall system test\\nDEPLOYMENT\\nComplete user acceptance documentsLoad initial dataInstall and test all the required software on the client machinesTrain initial set of usersGrant authorizations for data warehouse access to various classes of usersEstablish backup and recovery procedures\\nMAINTENANCE \\nEstablish procedures to gather statistics for ongoing monitoringFine-tune the data warehouse as necessaryComplete materials for training of users and establish training scheduleEstablish user support structureEstablish procedures for data warehouse managementUpgrade infrastructure as needed from time to timeCHECKLISTS FOR MAJOR ACTIVITIES 495', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b59b7089-0caf-490e-acd3-0272874f7bd8', embedding=None, metadata={'page_label': '509', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='APPENDIX B\\nCRITICAL FACTORS FOR SUCCESS\\n/L50539Do not launch the data warehouse unless and until your company is ready for it.\\n/L50539Find the best executive sponsor. Ensure continued, long-term, and committed sup-\\nport. \\n/L50539Emphasize the business aspects, not the technological ones, of the project. Choose a\\nbusiness-oriented project manager.\\n/L50539Take an enterprise-wide view for the requirements.\\n/L50539Have a pragmatic, staged implementation plan.\\n/L50539Communicate realistic expectations to the users; deliver on the promises.\\n/L50539Do not overreach to cost-justify and predict ROI.\\n/L50539Institute appropriate and effective communication methods.\\n/L50539Throughout the project life cycle, keep the project as a joint effort between IT and\\nusers. \\n/L50539Adopt proven technologies; avoid bleeding-edge technologies.\\n/L50539Recognize the paramount importance of data quality.\\n/L50539Do not ignore the potential of data from external sources.\\n/L50539Do not underestimate the time and effort for the data extraction, transformation, and\\nloading (ETL) functions.\\n/L50539Select the architecture that is just right for your environment; data warehousing is\\nnot a one-size-fits-all proposition.\\n/L50539Architecture first, technology next, and only then, tools.\\n/L50539Determine a clear training strategy.\\n/L50539Be wary of “analysis paralysis.”\\n497Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='03056890-08c3-4cf3-a4ee-d1ca9901bf90', embedding=None, metadata={'page_label': '510', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Begin deployment with a suitable and visible pilot to deliver early benefits.\\n/L50539Do not neglect the importance of scalability. Plan for growth and evolution.\\n/L50539Focus on queries, not transactions. The data warehouse is query-centric, not transac-\\ntion-oriented.\\n/L50539Clearly define and manage data ownership considerations.498 CRITICAL FACTORS FOR SUCCESS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='39c3c793-5120-4e65-8cd5-c8052bf0cd0b', embedding=None, metadata={'page_label': '511', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='APPENDIX C\\nGUIDELINES FOR EVALUATING \\nVENDOR SOLUTIONS\\nHere are a few practical guidelines. These are general guidelines relating to all types of\\nproducts that perform various functions from data extraction through information deliv-ery. Adapt and apply these to your specific environment.\\n/L50539First and foremost, determine the functions in your data warehouse that absolutely\\nneed vendor tools and solutions.\\n/L50539For each type of product you need, carefully list the features that are expected. Di-\\nvide the features into groups by importance—high, medium, and low. Use thesegroups of features to grade the products you are considering.\\n/L50539Allocate enough time to research available solutions and vendors thoroughly.\\n/L50539If you try to incorporate solutions from too many different vendors, you must be\\nprepared to face serious challenges of incompatibilities and restrictions for integra-tion. Stay with two or three vendors whose products are most appropriate for yourneeds.\\n/L50539Metadata is a key component of a data warehouse. Ensure that the vendor products\\nyou choose can handle metadata satisfactorily.\\n/L50539The standing and stability of the vendor is equally important as the effectiveness of\\nthe products themselves. Even when the products are suitable for your environment,if you are concerned about the staying power of the vendor, have second thoughts onselecting these products.\\n/L50539Never rely solely on vendor demonstrations as the basis for your selection, nor\\nshould you check only the references furnished by the vendors themselves.\\n/L50539Test the tools and products in your environment, with subsets of your own data.\\n/L50539Arrange for both user representatives and IT members of the project team to test the\\nproducts, jointly and independently.\\n499Data Warehousing Fundamentals: A Comprehensive Guide for IT Professionals. Paulraj Ponniah\\nCopyright © 2001 John Wiley & Sons, Inc.\\nISBNs: 0-471-41254-6 (Hardback); 0-471-22162-7 (Electronic)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4fc0aa0f-3fdc-476e-8b11-62c76e9e5862', embedding=None, metadata={'page_label': '512', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='/L50539Establish a definitive method for comparing and scoring competing products. Y ou\\nmay devise a point system to score the various features you are looking for in aproduct type.\\n/L50539The success of your data warehouse rides on the end-user tools. Pay special atten-\\ntion to the choice of the end-user tools. Y ou may compromise a bit on the othertypes, but not on the end-user tools.\\n/L50539Most of your end-users will be new to data warehousing. Good, intuitive and easy-\\nto-use tools go a long way in winning them over. \\n/L50539Users like tools that seamlessly incorporate online queries, batch reporting, and data\\nextraction for analysis. 500 GUIDELINES FOR EVALUATING VENDOR SOLUTIONS', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e02a12df-a492-4fec-b95e-e78d8681aa22', embedding=None, metadata={'page_label': '513', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Business intelligence, 12–13\\nBusiness metrics, 96–97Business requirements, see Requirements \\nCASE tools, 209–210\\nCategories, in dimensions, 95–96Checkpoint, for data loads, 458Cleansing, see Data cleansing\\nClient/server architecture, 156–157Cluster detection, in data mining, 409–411Clustered indexes, 448Clustering, 432Clusters, server hardware, 161–162Codd, E.F., OLAP guidelines, 350–352Computing environment, 149–150Conformed dimensions, 253Costs, see Project costs\\nCRM (Customer Relationship Management), 54–56 Cubes, multidimensional data, 354, 357, 365–366.\\nSee also OLAP \\nDDL (Data Definition Language), 434, 436\\nData\\narchived, 30external, 30–31internal, 29–30spatial, 46unstructured, 45 \\nData acquisition\\ndata flow, 136–137functions and services, 137–138\\nDatabase management system, see DBMS\\nDatabase software, 164–167INDEX\\n511Additive measures, 215Agent technology, 51–52Aggregates\\nfact tables, 243–247need for, 242–243\\nAggregation\\ngoals, 248options, 247–248suggestions for, 248–249\\nAlert systems, 170Analysis, 333–334Analytical processing, see OLAP\\nApplications, 334Architecture, 127–142\\ncharacteristics of, 129–132  definitions for, 127–128framework, 132–134in major areas, 128–129technical, 134–142  \\nArchived data, 30\\nBackup, see also Recovery\\nreasons for, 470–471schedule, 472strategy, 471\\nBitmapped index, 446–448. See also Indexing\\nBottom-up approach, 27Browser, Web, 387–388B-Tree index, 445–446. See also Indexing\\nBusiness data, dimensional nature of, 90–92Business dimensions\\nexamples of, 92–93hierarchies in, 95–96 ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a5c53c84-306d-4876-82d4-936b01de01a3', embedding=None, metadata={'page_label': '514', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Data cleansing\\ndecisions, 305–306pollution discovery for, 306–307practical tips, 311purification process, 309–311\\nData design\\nCASE tools for, use of, 209–210decisions, 204dimensional modeling in, 204–209\\nData dictionary, 436Data extraction\\noverview, 31techniques\\ndeferred extraction, 268–270evaluation of, 270–271immediate extraction, 266–268\\ntools, 169 \\nData granularity, 23–24, 215, 217–218Data integration, 275–277Data loading\\napplying data in, 280– 281dimension tables, procedure for, 283–284fact tables, history and incremental, 284–285full refresh in, 282–283incremental loads, 282initial load, 281overview, 33\\nData mart, 24–26Data model, 111–113Data movement, considerations for, 155–156Data mining, 399–426\\napplications, 422–423\\nbanking and finance, 426business areas, 422–423retail, 424–425telecommunications, 425\\nbenefits of, 423–424and data warehouse, 406–408 defined, 401–402knowledge discovery, 402–404\\nphases, 403–404\\nOLAP versus, 404–406techniques\\ncomparison of, 421cluster detection, 409–411decision trees, 411–413genetic algorithms, 418– 419link analysis, 415–417memory-based reasoning, 413–414neural networks, 417–418\\ntools, evaluation of, 421–422\\nData pollution, 297–301. See also Data quality\\ncryptic values, 298data aging, 300fields, multipurpose, 299identifiers, nonunique, 298sources, 117, 299–301512 INDEX\\nData quality\\nbenefits of, 295–296data accuracy, compared with, 292–293dimensions, 294–295explained, 291–292framework, 308–309initiative, 304–308names and addresses, validation of, 301–302problem types, 117, 297–299tools, features of, 169, 303–304\\nData refresh, 282–283. See also Data loading\\nData replication, 266–268Data sources, 28–31Data staging, 31–33Data storage\\narchitecture, technical, 138–140component, 33–34sizing, 120–121, 442–443 \\nData transformation\\nimplementation, 277– 279integration in, 275–276overview, 32problems\\nentity identification, 276–277multiple sources, 277\\ntasks, 272 - 273tools, 169types, 273–275\\nData visualization, 46–47Data warehouse\\ncomponents, 28–35data content, 130–131data granularity, 23–24definitions for, 13–14, 19–20development approaches for, 25–28integrated data in, 21–22nonvolatile data in, 23subject-oriented data in, 20–21time-variant data in, 22–23\\nData warehousing\\nenvironment, a new type of, 12market, 42–43products, 43–44solution, only viable, 12uses, 2, 41–42\\nData Webhouse, 391. See also World Wide Web. \\nDBMS, selection of, 120, 166–167Decision-support systems\\nfailures of, past, 7–8history of, 8progression of, 400–401scope and purpose, 11\\nDecision trees, in data mining, 411–413Degenerate dimensions, 216Deployment, 455–473\\nactivities\\ndesktop readiness, 458–459', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e48a0acc-e180-4069-b6ba-e6f0612a32d3', embedding=None, metadata={'page_label': '515', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='initial loads, 457–458\\ninitial user training, 460user acceptance, 456–457\\nof backup, 470–472of pilot system, 462–466of recovery, 472–473of security, 467–470in stages, 460–461\\nDimensions\\nconforming of, 253degenerate, 216junk, 235large, 231–233rapidly changing, 233–234slowly changing, 226–231\\ntype 1 changes (error corrections), 227–228type 2 changes (preserving history), 228–230type 3 changes (soft changes), 230–231\\nDimensional modeling\\ndesign\\ndimension table, 206–208fact table, 205–206\\nentity-relationship modeling, compared with, \\n209\\nDimension table, 206–208, 212–214DOLAP (desktop ROLAP), 363Drill-down analysis, 211–213, 360–362DSS, see Decision-support systems\\nEnd-users, see Users\\nEntity-relationship modeling, versus dimensional\\nmodeling, 209\\nEIS (Executive Information Systems), 8, 34, 334ETL (Data Extraction, Transformation, and \\nLoading)\\nconsiderations for, in requirements, 116management, 489metadata, 286–287overview, 31–33, 258–260steps, 260–261tool options, 285–286\\nExternal data, 30\\nFacts, 205–206\\nFact table, 205–206, 214–216, 241–242\\nfactless, 216–217\\nFat client, in OLAP architecture, 369Fault tolerance, in RAID technology, 442Fine-tuning, ongoing, 490Foreign keys, in STAR schema, 219–220\\nGenetic algorithms, in data mining, 418–419\\nGranularity, see Data granularity\\nHardware \\nselection, guidelines for, 148–149server optionsINDEX 513\\nclusters, 161–162MPP, 162–163NUMA, 163–164SMP, 160–161\\nHelpdesk, in support structure, 486Hierarchies, in dimension table, 214, 232–233Hypercubes, 357–360. See also OLAP\\nIndexing, 443–449\\nB-Tree index, 445–446bitmapped index, 446–448for dimension table, 449for fact table, 448\\nInformation\\ncrisis, 3–4technology growth, 5\\nInformation delivery\\narchitecture, technical\\ndata flow, 140–141functions and services, 141–142\\nin business areas, 319–320components, 34–35enhancement, 489–490methods, 34\\nanalysis, 333–334applications, 334queries, 331–332reports, 332–333\\noperational systems, differences, 316–317tools, selection of, 335–340user-information interface, 321–322users, to broad class of, 330 \\nInformation packages\\ndimensional model, basis for, 204–208examples, 98purpose and contents, 94–95requirements document, essential part of, 106\\nInformation potential, data warehouse\\nbusiness areas, for, 319–320plan-assess-execute loop, for, 318–319\\nInfrastructure, 145–170\\ncomputing environment, 149–150database software, 164–167hardware, 148–149operational, 147physical, 147–148platforms, computing,150–158. See also Platform\\noptions\\nserver options, 160–164tools, features of, 169–170\\nInmon, Bill\\ndata warehouse, definition of, 19data warehouse or data mart, 24\\nInternal data, 29–30Internet, 379, 392, 395Interviews, 99–100. See also JAD\\nIntranet, 379, 380, 392', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1ece4ae8-3b27-4cf6-99ec-868bccfd1ac0', embedding=None, metadata={'page_label': '516', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='JAD (Joint Application Development), 102–103. See\\nalso Interviews\\nJunk dimensions, 235\\nKelly, Sean\\ndata warehouse, definition of, 19–20\\nKeys, STAR schema, 218–220Kimball, Ralph\\ndata Webhouse, 378, 391\\nKM (Knowledge Management), 53–54Knowledge discovery systems, see Data mining\\nLoading, see Data loading\\nLogging, 266–267, 472–473Logical model, and physical model, 429, 434–435\\nMainframes, 149. See also Hardware\\nManaged query system, 331–332Managed report system, 332–333Management\\nof data warehouse, 487–490tools, features of, 170  \\nMDDB (Multidimensional Database), 364–365MDS (Multidimensional Domain Structure),\\n357–359. See also OLAP\\nMemory-based reasoning, in data mining, 413–414Metadata, 173–200\\nbusiness, 188–190challenges, 196data acquisition, 184–185data storage, 186end-user, 177–179ETL, 286–287implementation options, 199information delivery, 186–187IT, 179–181repository, 196–198requirements, 193–194sources, 194–196standards, initiatives for, 56–58, 198tasks, driven by, 181–183 technical, 190–192\\nMetadata Coalition, 57, 198Middleware, 170MOLAP (Multidimensional OLAP), 365MPP (Massively Parallel Processing), 162–163Multidimensional analysis, 344–345, 354–355. See\\nalso OLAP\\nNaming standards, 436–438\\nNeural networks, in data mining, 417–418NUMA (Nonuniform Memory Architecture),\\n163–164\\nOLAP (Online Analytical Processing), 343–374\\ncalculations, powerful, 345–346definitions of, 350514 INDEX\\ndrill-down and roll-up, 360–362guidelines, Codd, E. F, 351–352hypercubes, 357–360models\\nMOLAP, 365ROLAP, 366–367ROLAP versus MOLAP, 367–368\\nmultidimensional analysis, 344–345options\\narchitectural, 365–367platform, 372–373\\nresults, online display of, 354–355slice-and-dice, 362standards, initiatives for, 57–58tools, features of, 170, 373\\nOLAP Council, 57–58, 350OLTP (Online Transaction Processing) systems, 10OMG (Object Management Group), 57, 198Operational systems, 9–11Operating system, 149\\nParallel hardware, 160–164\\nParallel processing\\ndatabase software in, 164–166implementation of, 48–49performance improvement by, 450–451of queries, 165–166\\nPartitioning, data, 449–450Performance, improvement of\\ndata arrays, use of, 452data partitioning, 449–450DBMS, initialization of, 451–452indexing, 443–449parallel processing, 450–451referential integrity checks, suspension of, 451summarization, 451\\nPhysical design, 429–452\\nblock usage, 440–441data partitioning, 449–450indexing, 443–449objectives, 433–434RAID, use of, 442scalability, provision for, 434steps, 430–433storage, allocation of, 438–443 \\nPhysical model, 429, 434–436Pilot system, 462–466. See also Deployment\\nchoices, 465–466types, 463–465usefulness, 462–463\\nPlanning, see Project plan\\nPlatform options\\nclient/server, 156–157for data movement, 155–156as data warehouse matures, 157–158hybrid, 152–153for OLAP, 372–373', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='40d8256b-4bf0-420a-942a-97932aedae25', embedding=None, metadata={'page_label': '517', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='in staging area, 153–154\\nsingle platform, 150–151\\nPost-deployment review, 477–478Primary keys, in STAR schema, 218–219Project, differences from OLTP, 70Project costs, 67–68Project management\\napproach, practical, 84–85failure scenarios, 81principles, guiding, 81–82success factors, 82–84warning signs, 82–83\\nProject plan\\napproach, top-down or bottom-up, 65build or buy decisions, 65plan outline, 69, 73readiness assessment, 71risk assessment, 64survey, preliminary, 66–67values and expectations, 64vendor options, 65–66\\nProject sponsor, 67, 80Project team\\nchallenges, 75job titles, 76responsibilities, 78roles, 77skills and experience levels, 77–78user participation, 78–80\\nQuality, see Data quality\\nQueries, 331–332\\nRAID (Redundant Array of Inexpensive Disks), \\n442\\nRecovery, 472–473. See also Backup\\nReferential integrity, 451Refresh, data, 282–283. See also Data loading\\nReplication, data, 266–268Reports, 332–333Requirements\\ndriving force for\\narchitecture plan, 113–119data design, 110–113data quality, 117DBMS selection, 120information delivery, 121–124storage specifications, 119–121\\nETL, considerations for, 116methods, for gathering\\ndocumentation, existing, 103–104interviews, 99–100JAD sessions, 102–103\\nnature of, 90, 93survey, preliminary, 66–67\\nROLAP (Relational OLAP), 366–367Roll-up analysis, 360–362INDEX 515\\nScalability, 148–149, 434Schemas, see STAR schema. See also Snowflake\\nschema.\\nSDLC (System Development Life Cycle), 71–73Security\\npasswords, 469policy, 467–468privileges, user, 468–469tools, 469–470\\nSemiadditive facts, 215–216Server options, see Hardware\\nSlice-and-dice analysis, 362SMP (Symmetric Multiprocessing), 160–161Snowflake schema\\nadvantages and disadvantages, 238dimension tables, normalization of, 235–237guidelines, for using, 238–239 \\nSource systems, 28–31Source-to-target mapping, 264Sparsity, 246–247Sponsor, see Project sponsor\\nSpreadsheet analysis, 347–348SQL (Structured Query Language), 348–349Standardizing facts, 254Standards\\nlack of, 56metadata, 57, 198OLAP, 57–58, 350\\nSTARS, family of\\nSummary, 254Tables\\ncore and custom, 251snapshot and transaction, 250–251value chain and value circle, supporting,\\n251–253\\nSTAR schema\\nadvantages, 220–223example, 210–211formation of, 208keys, 218–220navigation, optimized with, 221–222query processing, most suitable for, 222–223\\nSteering committee, data quality, 308Storage, see Data storage\\nStrategic information, 2, 3, 6 Striping, 441– 442 Summarization, 451. See also Aggregates. \\nSupergrowth, 383–385Supermart, 27Surrogate keys, in STAR schema, 219Syndicated data, 52\\nTables\\naggregate, 239–241dimension, 206–208, 212–214, 226–231fact, 205–206, 214–216, 250–251', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dd8de7d5-0169-4b9b-8332-a8f8d78296d3', embedding=None, metadata={'page_label': '518', 'file_name': 'dmw.pdf', 'file_path': '/content/docs/dmw.pdf', 'file_type': 'application/pdf', 'file_size': 4192561, 'creation_date': '2024-01-31', 'last_modified_date': '2024-01-31', 'last_accessed_date': '2024-01-31'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Technical architecture, 134–142. See also\\nArchitecture\\nTechnical support, 485–486Thin client, in OLAP architecture, 369Time dimension, criticality of, 345Tools\\navailability, by functions, 118–119data quality, for, 303–304features, 169–170information delivery, for, 335–340OLAP, for, 373options, for ETL, 285–286security, for, 469–470\\nTop-down approach, 26Training, see User training\\nTransformation, see Data transformation\\nTrends, significant data warehousing\\nactive data warehousing, 56agent technology, use of, 51–52browser tools, enhancement of, 50CRM, integration with, 54–56data types, multiple and new, 44–46data visualization, 46–47ERP, integration with, 52–53growth and expansion, 40–42KM, integration with, 53–54multidimensional analysis, provisions for, 51parallel processing, 48–49query tools, enhancement of, 49–50syndicated data, use of, 52vendor solutions, maturity of, 42–43Web-enabling, 58–61\\nUser acceptance, 456–457\\nUsers\\nclassification of516 INDEX\\nusage, based on, 324job functions, based on, 325\\ndivisions, broad\\nexplorers, 328farmers, 327miners, 328operators, 327tourists, 326 \\nUser support, 485–487User training\\ncontent of, 482delivery of, 484–485preparation for, 482–484\\nValue chain, fact tables supporting, 251–252\\nValue circle, fact tables supporting, 252–253Vendor solutions, evaluation guidelines for, \\n499–500\\nWorld Wide Web (WWW)\\nbrowser technology, 387–388data source, for the data warehouse, 382–383data warehouse, adapting to, 59–60, 381information delivery, Web-based, 383–387security, considerations of, 389technology, converging with data warehousing,\\n380–381\\nWeb-enabled data warehouse\\nconfiguration, 60implementation, considerations for, 393–394processing model, 394–395\\nWeb-OLAP\\napproaches, implementation, 390engine design, 390–391', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys_prompt = \"\"\"\n",
        "  You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on\n",
        "  the instructions and context provided.\n",
        "\"\"\"\n",
        "\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ],
      "metadata": {
        "id": "WYYpiZlNjL-2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP_dEJjjj6Vo",
        "outputId": "4af0634d-46f8-4ba5-883a-f6fdf5496112"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|',\n",
              " '    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|',\n",
              " '    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|',\n",
              " '    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|',\n",
              " '    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|',\n",
              " '',\n",
              " '    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .',\n",
              " 'Token: ',\n",
              " 'Add token as git credential? (Y/n) Y',\n",
              " 'Token is valid (permission: read).',\n",
              " '\\x1b[1m\\x1b[31mCannot authenticate through git-credential as no helper is defined on your machine.',\n",
              " 'You might have to re-authenticate when pushing to the Hugging Face Hub.',\n",
              " \"Run the following command in your terminal in case you want to set the 'store' credential helper as default.\",\n",
              " '',\n",
              " 'git config --global credential.helper store',\n",
              " '',\n",
              " 'Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\\x1b[0m',\n",
              " 'Token has not been saved to git credential helper.',\n",
              " 'Your token has been saved to /root/.cache/huggingface/token',\n",
              " 'Login successful']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window = 4096,\n",
        "    max_new_tokens = 256,\n",
        "    generate_kwargs = {\"temperature\" : 0.0, \"do_sample\" : False},\n",
        "    system_prompt = sys_prompt,\n",
        "    query_wrapper_prompt = query_wrapper_prompt,\n",
        "    tokenizer_name = 'meta-llama/Llama-2-7b-chat-hf',\n",
        "    model_name = 'meta-llama/Llama-2-7b-chat-hf',\n",
        "    device_map = 'auto',\n",
        "    model_kwargs = {'torch_dtype' : torch.float16, 'load_in_8bit' : True}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745,
          "referenced_widgets": [
            "3c3b5c53fd684c61ba39f556fc76e2a2",
            "2e5114fd505a446093b29cc2da487dbb",
            "223779e15daf43468f2565f6a437bc86",
            "8c5cd83fa1dd4b37a9493f5a79291000",
            "cea6cff19a934e538e7f01e3d112f48b",
            "b7b51152cffd4fc38cb7466f86629637",
            "0d0683d01aa54dc29aadb8be590b4e3c",
            "a69ab68c759d4bccaf90b332bcfa6326",
            "fea4691323b94a99b42825ce407752ee",
            "f2907e1e62f04fd7b1484b0ddaecabf0",
            "498dae17dfe54703be3207e124270e92",
            "20af554668a3480a8bc06cd48f6b2d88",
            "9536ebc1d9de42d69a7ac80357d29bf0",
            "f86e5e0fc2854796aea2df2c91cb4b72",
            "14a2dbdecbf8477085c949ab39048b8e",
            "4c530a8971a74173be6c6f4f0fa43ded",
            "de99ecd582de4d9a9694598d240da5fe",
            "3f5c53c088ce4590ac2fe054610c878a",
            "fafb889a238b42888616f774137dea51",
            "40a9a877ccb74136b9e0edf3dc1fb2df",
            "d591b7e891b741c2abe4b22fde6f2cdd",
            "f99d6d14f3aa48329fdbebf73e35a461",
            "651bdbc4f878490ebf05c5986c813806",
            "0f74a04c006343a1b336fa528c19b620",
            "cf1becc4277f4da88b09947decb6d71d",
            "0fb0807a9ced4074bc45e6f75db5f344",
            "eb3ab1dc89634c3da2d06bbc4b7eda0b",
            "adfc14d368c24222945d58d076e42887",
            "bbe1f006992d472aaa264b653d725d29",
            "1b271e03fa114f188d2bc61589b85f56",
            "8fee11ae0f3b4e93a787375943980700",
            "24a0996f264a48d9b551c9e4b356eacd",
            "0e74702609254ef39fec984532330eb4",
            "77aa9d4bc5df48b7bb10f685cc04400a",
            "05367981e76f4e158fca2bbb8319a653",
            "87676f1ca62e4e3396fb5a8132ab8feb",
            "a8c3147f905246eaa4cae73ed9528022",
            "88e916a250f344b8a474daf43d96a0c1",
            "46a7517d95b7488d80e3ff53bd672578",
            "60712292e87c40d5a5b6be701ab139ff",
            "34167f00e9e54ea993a3950509c79c2c",
            "c6a80339be2c46f998b568482e76387a",
            "aad022034dce43e499aafb51dbda0384",
            "ed83d296d49947c886e15723f73aa593",
            "522020c17cf64ca2bae443b6236452e8",
            "56ebe9d06ee64b88a0927cd55770a6cb",
            "1785b300424c4920860357aa387606f3",
            "b880a14989e04a77afa062930514ebcc",
            "2d5f9629667a486380422bedb8f2407a",
            "9e66d4b0e18940519cd2582271e1cf75",
            "ceda604f5e064b84887e8c33171f7a70",
            "090c2e48fb774f3786487fcf258e7cba",
            "37a1d632e3fb4e6d99370755c377f593",
            "c5b2a75e229944e08ebc23da96365056",
            "5ba61d84d9254fdda0f8601a1c6b69ce",
            "9eb36cf18504493fa01a468feedb5ee7",
            "96c14035bf6041cda1072aaa535664e3",
            "94f4411599db4c11b3aafd26d07e3756",
            "f75507a0d2574fa4a0831e080ffbe65f",
            "8f42dd5e603e4b18939720702d2492e9",
            "b472be8a949749bc9b08661d2524a193",
            "9a2013182c5442efb94a5da0b504d376",
            "1c0bbe4953764083b63b7fe7d92f3586",
            "1e30f39ef4924e17ac730609179145de",
            "41d6aa85cfaf48d4b85e9d4be5a9d07e",
            "1946fb2bdeab4ac7a39367b4bf3a275f",
            "81fdfcc5067347118d3ea9089a1b4ea2",
            "773d7c33d876490abdc247c6163f7b09",
            "962c10e150f44a3684d3810a139e3dd3",
            "94a2d0e0cae04bae90d1a35ab4634109",
            "4cedf8db6b104dd987db6c640abef5be",
            "f2334cedebe94440804c4c220958f8e8",
            "0f24cfb7d6824b1db9bf52460f354d89",
            "b0dd3382add44fb8ae0f89f2c3c07c7a",
            "5e47287fb8b948799fb0b5f904059171",
            "3cbcac6f7d2e454c8575b9724a1f4b38",
            "99ca3b77d04c480ea9ec460e8a8be0dc",
            "126980d5a9c446bcbc567a8e43fc0463",
            "a120c98d991a42838bb4718932b51042",
            "5c440524344e407da1e1ec9d49ae9075",
            "428df9f4679444f2a5abd474e892ff61",
            "a9ef976919db4388b35e00f4e4fde3f6",
            "8292728186f1448bb99952ea196cee06",
            "5afd015bc8d847ca94b604a254ce3937",
            "5a78e3c991da4494b9fc7809c3e7038a",
            "f8dff4d125e44b728343f779ab02de58",
            "474b7d0483f94d309f1546fc08b7555a",
            "c984c002a1834330ae01f834125ef21f",
            "878ac444553640728bf2a96260da035b",
            "ac470c3113ca4b1fb1b44c71ded862db",
            "d32b4766f9654c8781f4d102f44ed99f",
            "3cdf01bb525449d3b667d3d5525d10f5",
            "8b1141fa13a045bd8a4083a2a1b41f40",
            "51e1a78890284f0a8e68c4bdd0603adb",
            "50590f9910e9437788d4d54b48706d7b",
            "d6e8c292cd9249c8b70da63af3c17863",
            "29ccd06e517546178029bb8dea8d29fa",
            "1b7ede89b07e4b0dbfd54c9dd4b4495c",
            "44ed3168d08a41a78231dde5b5acbaca",
            "98b2200e9cdd4dbfac47448e55a2bd8a",
            "39bf446473014366a299a96decd59191",
            "8b380853b3624d1bb17fb416a2d981e8",
            "d2299f4ee9674df295d130087040ff59",
            "9873099406e14fdbb8e904ba2e1e92ec",
            "c119d83591bb46e99a0c7f879818945a",
            "0df6592e593a49b48fd826353063d7bd",
            "367ade8833354a6e9affb798fda4b736",
            "a878f9e24de94f94b12d9226d2802e7e",
            "0251700590794b1d81973c651da71825",
            "e3050ee556f54a0a953659890d1669a8",
            "ab35fef595364edfa1c5c76176a6fbaf",
            "933f8e848c1d44848ef6288edbf23d38",
            "cf7c9d9789d244b992d8b1bff525b856",
            "03949fd9ffdd4031aa4ad1ab297d6585",
            "a0bffe0ba0454d669fcbb5bfa061a017",
            "498dd232222b4b7797232ab2e51b7561",
            "2528ddca263f42bbad4759b458723ca4",
            "82df6d5ea1be41e2b86814b060bc7f69",
            "93663b6ef22a4e4583d6b71ae762fc56",
            "42b08480705541809f069e6ae72ecb30",
            "5f186060b8b040dca354007f4cabb871"
          ]
        },
        "id": "YzCUjXeYkRhi",
        "outputId": "072d6101-3f28-4c7f-ad8f-be80cfe2a179"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c3b5c53fd684c61ba39f556fc76e2a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20af554668a3480a8bc06cd48f6b2d88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "651bdbc4f878490ebf05c5986c813806"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77aa9d4bc5df48b7bb10f685cc04400a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "522020c17cf64ca2bae443b6236452e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9eb36cf18504493fa01a468feedb5ee7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81fdfcc5067347118d3ea9089a1b4ea2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "126980d5a9c446bcbc567a8e43fc0463"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "878ac444553640728bf2a96260da035b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98b2200e9cdd4dbfac47448e55a2bd8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab35fef595364edfa1c5c76176a6fbaf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index import ServiceContext\n",
        "from llama_index.embeddings import LangchainEmbedding\n",
        "\n",
        "embed_model = LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-mpnet-base-v2')\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537,
          "referenced_widgets": [
            "bbc1e60edd994aad8a6019f294024e56",
            "d65d9fcc5ebf4754883ed25abc78dab7",
            "4f03dd2ec904443ab851476721d1e26a",
            "0a016ee9ea2b4959991a58e7583af32c",
            "85b4e96b05ed4d7c90926dc5257a828c",
            "4abd701554d548e7b2087b221254afd8",
            "5c9b1af13a8b47169a96a1851938930e",
            "ffbc2650c8ba4f439d3f35509b923c2e",
            "1a9a473d4ad64d959b8559723b85a711",
            "ee21df104a18470bad7f339557b3e9d7",
            "57170e6c3dd34d27b270078dd089cef4",
            "eed4ed16576d487cb419a5718a05255d",
            "e6212d43637840cc83ce80eee6848b5d",
            "44afc373b8434829b85a575573044bb4",
            "d87cc676dac14f0fbf44add109130018",
            "74cdd77837f94a9db0e0b612f58c1833",
            "8a63bd15c5684fc5b465b9d373089302",
            "6823dd03dcec4f209d2589bcc1650e1d",
            "ae9f45af81ef4d249053b9c6021f8e19",
            "f47cbde305c0482b934ac865f1633767",
            "58ef2327baa24107a0d31ee6cdb8574b",
            "1ddee2905a3845349742a9bbe2bc04b7",
            "33f24c56b0c243739d9e196b7395a954",
            "bcdd9c45956e425eb6d9e79ae269e0e4",
            "022411744c4a4fba9f93d7ab31d65ffb",
            "89c1f882654f447290bd3222d8c25505",
            "29a0170dd53a465eab477e70a6f1afd3",
            "a6f55b7a70bd46b886c0747d6a1ba89e",
            "8c78209650864062b0ca39a4926784da",
            "8e7f01c0d21b4460a6582262b83bfe42",
            "fddfc9e3949246548dfa488578145227",
            "3df895f8dadb40cd96b34363d80397c1",
            "8a6d3693dc3c45a28f7e75aff47fde7a",
            "7116ca05e2c74d5f8d8e1fa51d709297",
            "2d69e4b87b8f40afa06d62ee54ab4f84",
            "8b966f6f546a4ff2b0a92413933f9e5f",
            "ee6ce8fbf7234291a40925a621223d65",
            "c5215600719641f08a1cd484d7c83747",
            "b3afb911f67147dfbaa30aa4b6c8fef3",
            "7bdbd1fde16e4b4aaa2004e3f7dfc298",
            "243c8f59bf5b4e00b05b961bcdabf85a",
            "b74322955e5d41a6b00012e52762c35d",
            "3300d909f96a43b7a3c1034fdd8ca629",
            "d53e15c8e6d74051aca61366c70ba0c6",
            "367d5a81981a4724ae9b4f0edb6b448b",
            "684e17dfce6a4aeeb797ad1fc252d678",
            "0f81d9cf83164595ab0bb31b5d49f299",
            "134a7f3475e644a5b9716d8394b54dc4",
            "810c01ada3264379a83fadfcc9725e8b",
            "bb247cb6ffaa4428afde0749e0b73f18",
            "8dcc66664fdb43d8b4355811dcda1feb",
            "06382ae2a006456faec98da5d063a0db",
            "209a1c5836d3460790eb0d69ae2cb85f",
            "a9736c323f9742988f620055935cf2ef",
            "7a40808f89e44e05a12ee1587561ad3c",
            "1cba51e764084f4a96032ddd08058396",
            "0be7a6031feb470085aba84d35c2ee5d",
            "64736854a6e84479ae6ac2d9849050fe",
            "a1ae709277834fe1ace32fcc4f96834a",
            "82a89f5a61e74bf1a63f7511d26117ab",
            "e14fee1129d84d15a280688fdff41501",
            "95250e29e1fc4863bd85dcb4bcb85d8b",
            "0f32edf1c8c940d0824e8126a17aec54",
            "bc069484b53a45f09d1d3a97ddfa7810",
            "2911be61532142d587b3025bde5dfb9a",
            "4534bcb912964da9acc1b00d38bc6d77",
            "546b6697800142f5a9151879bd5b703d",
            "5d46151eedf148dca968809cbd5aa299",
            "c52d5dc10d134f6c9c1e710862c8e5aa",
            "73446268d96548ccb65691f5330969bd",
            "97084c7bb59344c69e82e107e99ac7a1",
            "d4a47e2dbfac43b0abf6c0628eda3ee6",
            "418be8ae9efc44cfb8de7f941cf7db56",
            "2d935b661da848a793b8330af533f4d9",
            "9b36c85d0322486199c45d447a96077d",
            "04e4c70015ba4f04953cfa311a138096",
            "267bb6610d0c43cda77f2e40d8cd8545",
            "d9b1ac80d41d4f63afe61ef30249647f",
            "3473893423c64c21ad54ff03fc969fc2",
            "9b5bca86543a46deb554f26722793fc1",
            "2a4c9e9f318e4f9f82dd19d4eafc010a",
            "2a0d7ea1a4c14db0919dfd0f53f66dc9",
            "49213edb77954249b15ee40190fa6723",
            "cc6e4b1d82ef4aaba1423b66f6965785",
            "87b6b972d9d046f7b2f93797b105bf5f",
            "68339754933d4b71ad3cc0ac2e9b2ddb",
            "4546172525ef42539b70d8684179a478",
            "e7dfb984f58549fbafd9c8a1ea4f6b4c",
            "8d1a8cd6f42544bd8aff098c81d64bc2",
            "9aaefc63547943c391f68a6539cfba77",
            "3dd70d93bfbc4cf094bc7a260e60bfb6",
            "d5b7e8d1047b435d8c270a4aa9f6e0b3",
            "7af3d17acae24858b3063f849a7c5a06",
            "6b936cc1c7134f5d9b7b5cf19d0ea572",
            "1fe94a8f5c25493185f5fb2ace6495d3",
            "347066369e7047f1b846a1fede3efab6",
            "5ed29fb9e37c4c0a93483bcdf50b8ad7",
            "c06f69c888e941cbaa3c86ebc2b1ec68",
            "2efa754cbc934207aaf1d96c6c4a47ac",
            "b398f2f636e3439f9def3537c2780981",
            "00770e0d7ebe4f4db64444e221167f88",
            "2df65f850826419cb0f64a9840e18011",
            "d362b59bdc8e4a909d1185d2fb144466",
            "8314d73125d94911be32c2c1860d58b2",
            "1d090856ee2a41209e390b8429afee67",
            "e161172162694385bb982a7f2ba46aae",
            "6cff4dd3349747d4bd05575264fcded2",
            "89482cf4ec8341a1bc05c66c456d8837",
            "c1208affd68144a29f356cb3260d94d5",
            "8f661c97d2f9430a89b3082753097b83",
            "ceef2476a4a24c2ca0b5d376219e5ac2",
            "ba4c43903b5140e4a77330fd6f7ebafd",
            "bb34908a81e54353b20781310a6e979b",
            "da7db5631eaa4315b98f6342aa21b54a",
            "f2ab5483a955444d8c2079f4627a051d",
            "1a96bde1074a43f498f3e92c79d1c10a",
            "7961c2df5ab54d45be2372d7e0f7df9e",
            "d6ffa4923e0448b39ccc69019b21b5d1",
            "e1abbac4c2fc41aca3980a4320d75d3c",
            "a0753ec466f14f6b8694343643c71d6a",
            "cbd3f29428a440e3ae53be40f8f747ac"
          ]
        },
        "id": "CcIUW77klopf",
        "outputId": "f7271069-2e5c-4ea0-e80e-5d69060efb8e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bbc1e60edd994aad8a6019f294024e56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eed4ed16576d487cb419a5718a05255d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33f24c56b0c243739d9e196b7395a954"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7116ca05e2c74d5f8d8e1fa51d709297"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "367d5a81981a4724ae9b4f0edb6b448b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1cba51e764084f4a96032ddd08058396"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "546b6697800142f5a9151879bd5b703d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9b1ac80d41d4f63afe61ef30249647f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d1a8cd6f42544bd8aff098c81d64bc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b398f2f636e3439f9def3537c2780981"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ceef2476a4a24c2ca0b5d376219e5ac2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size = 4096,\n",
        "    llm = llm,\n",
        "    embed_model = embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "weLNg87lloHg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHTHDJnFmZMP",
        "outputId": "ad62a32a-b78f-4391-8cd8-77f90ee870a1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=LangchainEmbedding(model_name='sentence-transformers/all-mpnet-base-v2', embed_batch_size=10, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7a4e30c178b0>), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7a4e30c178b0>, id_func=<function default_id_func at 0x7a4eef766b00>, chunk_size=4096, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.logger.base.LlamaLogger object at 0x7a4eef228370>, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7a4e30c178b0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(docs, service_context = service_context)"
      ],
      "metadata": {
        "id": "yU4n0Ms7mb2G"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dJ0gy1wmkMQ",
        "outputId": "43ce0e6e-0b6c-4f2f-9812-91eb2542eae3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x7a4e325eab00>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "fhTfOossmo-r"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp = query_engine.query('What is KDD process')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo2ochtHmvlK",
        "outputId": "c90b83f8-5a6a-476d-b2ea-49f90bd4b3d5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SIxl9XYnF6O",
        "outputId": "3c5e8003-97db-41a6-fa85-a1e46af0be41"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " KDD process stands for Knowledge Discovery in Databases. It is a process of identifying useful patterns, relationships, or insights from large datasets. The goal of KDD is to extract knowledge from data that can be used to make informed decisions or improve business processes. The KDD process typically involves several steps, including data selection, data cleaning, data transformation, pattern identification, and knowledge representation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resp2 = query_engine.query('what is star schema and other schema types')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWeAfLz4npMj",
        "outputId": "28dd4971-711f-430a-a84b-2981e21b9f6d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(resp2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIjF36Xbn5aZ",
        "outputId": "d98c28c0-2fca-44a0-8046-dc66dbbef75c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The STAR schema is a type of dimensional modeling that is commonly used in data warehousing. It is called a star schema because of its resemblance to a star, with the fact table at the center and dimension tables radiating out from it. The STAR schema is a denormalized version of the traditional relational model, where each dimension table is associated with a single fact table. This allows for faster query performance and easier data analysis, as the dimension tables are pre-joined with the fact table, reducing the amount of data that needs to be processed.\n",
            "Other schema types include:\n",
            "\n",
            "1. Snowflake schema: This is a variation of the STAR schema that uses a hierarchical structure to organize the dimension tables.\n",
            "2. Cube schema: This is a three-dimensional structure that organizes the data into a series of interconnected cubes, each representing a different level of detail.\n",
            "3. Multidimensional schema: This is a type of schema that uses a combination of the STAR and cube schemas to create a multidimensional structure that can handle large amounts of data and complex queries.\n",
            "4. Hybrid schema: This is a combination of the\n"
          ]
        }
      ]
    }
  ]
}